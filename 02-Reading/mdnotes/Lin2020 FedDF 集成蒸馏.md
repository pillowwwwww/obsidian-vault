# 🔤 联邦学习中用于鲁棒模型融合的集成蒸馏 (2020, Lin) ()

**原名：**Ensemble distillation for robust model fusion in federated learning

**译名：** 联邦学习中用于鲁棒模型融合的集成蒸馏

**作者：**Lin et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/ARXIV.2006.07242](https://doi.org/10.48550/ARXIV.2006.07242)

**发表时间：**2020

**本地链接:** [2020-Ensemble distillation for robust model fusion in federated learning.pdf](zotero://open-pdf/0_WAD6NSAL)

`zoteroAlt + ←可以跳转回到刚刚阅读的位置`

该文也是解决了不同客户端的模型异构结构异构（figure7），通过集成蒸馏，对logits进行蒸馏的形式。

**摘要翻译：**

提出了FedDF

我们研究了更强大、更灵活的 FL 聚合方案。具体来说，我们提出了用于模型融合的集成蒸馏，即通过客户端模型输出上的未标记数据来训练中央分类器。这种知识蒸馏技术在与基线 FL 算法相同的程度上减轻了隐私风险和成本，但允许对异构客户端模型进行灵活的聚合，这些模型可以在大小、数值精度或结构上有所不同。

## 💡创新点

FedDF 并不直接合并参数；

客户端依然传递模型参数，在服务器端用 unlabeled data，获取各 local model 的 logits 输出 → 求平均（ensemble） → 蒸馏到一个新模型；

这种方式**提取了知识但避免了结构冲突**，从而得到了清晰、平滑、统一的决策边界。

---

### 为什么平均 logits 表现更好？

- **保留多样性**  
    每个客户端模型在不同子分布上可能学到不同的“模式”（模式 A、模式 B……），直接对 logits 求平均，相当于用“多数投票”或“信心水平”综合决策，通常能得到更稳健的预测。
- **避免参数对齐问题**  
    不同客户端即使结构相同，也可能学到不同的滤波器顺序或通道顺序，参数级别简单相加容易“混乱”内部特征；而 logits 只关心输出结果，不受内部结构差异影响。

平均 logits”本质上就是最简单的 **集成学习（ensemble learning）**——把多个弱模型的预测（probability／logits）平均后再做决策。它通常能显著提升泛化，但是：

`Alt + ←`

---

1. **引入 Ensembling + Distillation 融合机制：FedDF**
    
    - 将客户端模型作为教师，在服务器上通过无标签数据进行 ensemble 蒸馏，生成统一的中心模型（学生）。
    - **突破了传统参数平均（如 FedAvg）必须模型结构一致的限制**。
2. **异构模型支持**：
    
    - FedDF 支持不同架构（如 ResNet-20、ShuffleNet、DistilBERT）、不同精度（如 binarized model）间的融合。
3. **无标签数据融合**：
    
    - 允许使用来自其他领域的无标签数据（甚至是 GAN 生成的合成数据）完成蒸馏。
4. **无需修改客户端训练流程**：
    
    - 所有 distillation 和融合都发生在服务器端，不改变客户端逻辑，便于集成和部署。

## 💧新名词：

1. 使用GAN生成伪造数据，喂给模型生成logits，作为教师模型。

### GAN（生成对抗网络，Generative Adversarial Network）的基本结构：

|模块|功能|
|---|---|
|**生成器（G）**|接收随机噪声 $z$，输出伪造图像 $G(z)$，目标是“骗过判别器”|
|**判别器（D）**|输入图像，判断它是真实数据还是生成的假图|

论文中提到 GAN2，是一种已经预训练好的生成器。你可以：

1. 用它在服务器上生成海量“看起来合理”的图像；
2. 把这些伪图像喂给各个客户端模型（教师）；
3. 收集它们对这些图像的预测（logits），做平均，蒸馏给学生模型。

### 这样做的好处：

- **无隐私风险**：伪图像不属于任何用户；
- **无限生成**：可以动态扩展、重采样；
- **不依赖人工标注**：节省标注成本。

## 🌏研究背景：

## 🌟重点：

### 第三章算法1

## 🔬实验方法：

模型蒸馏过程部分的实验：

---

## 🧪 模型蒸馏过程（FedDF 的关键步骤）

- 蒸馏数据：
    
    - 默认使用无标签数据：CIFAR-100（用于 CIFAR-10 蒸馏），ImageNet（用于 CIFAR-100）
    - 可选使用 BigGAN 生成的图像作为蒸馏输入（图像生成由 GAN 在 FL 外部预先完成）
- 蒸馏优化器：
    
    - Adam（学习率 1e-3）
    - Cosine Annealing
    - 蒸馏早停：验证集准确率超过 1000 步未提升则停止（最多 1e4 步）

蒸馏所使用数据：需要高覆盖率样本

假设：

- 客户端 A 擅长识别“猫”
- 客户端 B 擅长识别“狗”

如果服务器只用几张“鸟”的图片作为蒸馏数据：

- 蒸馏过程无法提取“猫”“狗”这两个类别的信息
- 最终的全局模型效果就会很差

所以我们需要：

> **数量更多、类别覆盖更广的蒸馏数据集**，来激活每个客户端模型在各个类别上的知识

## 📜 总结：

FedDF实际上是从FedAvg上的改进，在每一轮：

1. 各客户端基于当前全局模型 $x_{t-1}$ 做本地训练，上传各自模型参数 $\hat{x}^k_t$；
2. 服务器**先用 FedAvg 方式对这些模型做参数加权平均**，得到一个初始融合模型 $x_{t,0}$， 作为学生模型的初始化；
3. 然后，服务器使用一批**无标签数据**，让所有客户端模型对这些样本做前向计算，得到 logits；
4. 将这些 logits **求平均**（即集成学习），作为**教师模型的输出**；
5. 用教师 logits 与学生模型输出做 KL 散度，反向传播更新学生模型参数 $x_{t,j}$；
6. 多轮蒸馏后得到最终模型 $x_t$，再广播给下一轮客户端。
    

---

实现了解决客户端架构异构：（在服务器上使用不同的模型作为学生模型，然后蒸馏）

1. ….前面流程都一样
2. 教师模型是一样的生成方式，但学生模型：
3. 服务器分别使用多个**不同架构的学生模型（称为原型模型）**，各自蒸馏这个 ensemble logits；
4. 于是，**每种结构都拥有一个融合过的、蒸馏后的模型**；
5. 下一轮训练中，每个客户端只需下载“与自己架构匹配的学生模型”即可。

### 为什么 “对每组模型分别蒸馏”？

- **不同架构的网络不能直接参数平均**，但都能输出同维度的 logits；
- 因此我们先在“输出层”把它们的 logits 聚在一起（ensemble），然后再**各凭本事**去蒸馏到自己那个架构的学生模型上。

---

### 利用未标注/生成数据进行蒸馏

传统的蒸馏方式选择联合损失（软标签+硬标签的方式）进行[[集成学习Ensembling和蒸馏]]，但是FedDF 不依赖带标签的数据，而是用无标签数据（甚至可生成）来做蒸馏，从而实现隐私保护下的模型融合。因为：**蒸馏的本质不是学“真实标签”，而是模仿“教师的行为”。