# 🔤 NeurIPS2025: RobustMerge：具有方向稳健性的参数高效模型合并方法 (2025, Zeng) ()

**原名：**RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness

**译名：** NeurIPS2025: RobustMerge：具有方向稳健性的参数高效模型合并方法

**作者：**Zeng et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/ARXIV.2502.17159](https://doi.org/10.48550/ARXIV.2502.17159)

**发表时间：**2025

**本地链接:** [2025-RobustMerge Parameter-Efficient Model Merging for MLLMs with Direction Robustness.pdf](zotero://open-pdf/0_S7327S7M)

**摘要翻译：** _使用自定义数据微调预训练模型会产生针对特定任务的众多专家模型。将模型合并为一个通用模型以增强多任务能力并避免数据泄露的做法已变得流行。随着数据和模型规模的扩大，参数高效调优已成为高效获取任务特定模型的常用做法。然而，很少有方法专注于高效合并，而现有为全面微调合并设计的方法在高效合并时会失效。为解决这一问题，我们从低秩分解进行分析，并揭示出合并过程中方向稳健性对于合并高效模块至关重要。我们进一步发现，补偿显著奇异值之间的差距有助于方向稳健性。因此，我们提出了一种无需训练的参数高效合并方法RobustMerge，该方法通过互补参数调整来维持方向稳健性。具体而言，我们（1）通过参数间关系修剪参数并调整系数以保持远离任务干扰的方向稳定性，以及（2）执行跨任务归一化以增强对未见过任务的泛化能力。我们建立了一个由多种模态任务组成的基准，通过实验验证了我们方法的出色性能和泛化能力。进一步的研究和详尽的分析进一步展示了其有效性。代码可在https://github.com/AuroraZengfh/RobustMerge 获取。_

## 💡创新点

PEFT+多模态+模型合并

1. 剪枝：使用 **基于幅度的剪枝** 来去除对方向影响较小的参数，这与 DARE 的随机剪枝不同。把剪枝挪到了PEFT场景下
2. **缩放机制不同：** 这是该论文最大的创新。它没有使用固定的标量，而是利用 LoRA 中矩阵 A 和 B 的非对称性及统计特性，构建了一个 **自适应的缩放矩阵 S**（Complementary Parameter Scaling） 。这种缩放是为了补偿因剪枝导致的奇异值（尤其是尾部奇异值）的损失，从而维护低秩空间的方向稳定性 。

## 💧新名词：

## 🌏研究背景：

模型合并 [62, 51] 指的是将不同能力的多个模型合并为一个通用模型以处理多任务学习。

1. 现有的模型合并方法主要集中在全调优（FFT）技术[61, 11] 上，而不是PEFT.  
    所以该文专注于PEFT的merging，通过lora。
2. 当前高性能方法依赖于已见任务的额外信息（例如验证数据[63]、额外存储[18]）来提升性能。因此，这些方法只能处理已见任务，而无法泛化到未见任务，这在实际场景中质疑了它们的鲁棒性和扩展性，

**为什么要多任务？**

- 随着模型规模的扩大[4, 58]，一旦模型专门针对特定数据集进行训练，重新训练模型以获得另一领域的知识将耗费大量时间和资源，甚至会出现灾难性遗忘[66]。
- 此外，数据隐私问题可能阻碍其实用应用。

现有的模型合并方法主要集中在全调优（FFT）技术[61, 11] 上，这些方法在直接应用于参数高效模型合并时会遇到分布偏移的问题，并且性能会下降，如图1所示。此外，另一个问题是当前高性能方法依赖于已见任务的额外信息（例如验证数据[63]、额外存储[18]）来提升性能。因此，这些方法只能处理已见任务，而无法泛化到未见任务，这在实际场景中质疑了它们的鲁棒性和扩展性。

论文参考了LoraHub[17]，是最相关的工作。

**之前针对FFT的模型merging有两大缺点**：

### 1. 缺点一：Stark Singular Values（奇异值差异显著/极端的奇异值分布）

- **含义：** 论文作者对 LoRA 矩阵进行奇异值分解（SVD）后发现，在同一个任务的低秩矩阵中，**头部奇异值（大值）和尾部奇异值（小值）之间存在巨大的数值差距** 。
- **带来的问题：**
    
    - **方向不稳定性（Direction Instability）：** 头部的大奇异值通常代表主要知识，方向比较稳定；但尾部的小奇异值非常脆弱。在融合多个任务的模型时，这些小奇异值对应的**方向（即特征向量的方向）极易受到其他任务的干扰而发生改变** 。
    - 这种方向的改变（Direction Shift）会破坏模型对特定任务的微调知识，导致融合后的模型性能下降 。

### 2. 缺点二：Distinct Wider Distribution in Efficient Parameters（参数分布显著更宽）

- **含义：** 作者对比了全量微调（FFT）和参数高效微调（PEFT/LoRA）的参数数值分布：
    
    - **FFT 模型：** 参数更新量通常集中在 0 附近，分布非常窄。因此，FFT 融合的主要矛盾是不同模型参数的“符号冲突”（Sign Conflict，即一个模型想变大，另一个想变小）
    - **PEFT 模型：** 参数的数值分布范围要**宽得多且分散** 。
- **带来的问题：**
    
    - 由于分布不同，照搬为 FFT 设计的算法（如 Ties-Merging，它依赖于解决符号冲突）会失效。
    - 在 PEFT 中，**“符号冲突”不再是主要矛盾**，保留大幅值的参数（无论符号如何）对于维持低秩空间的方向更为重要 。

## 🌟重点：

### Figure3 a（剪枝) :

- ① 原始 PEFT 矩阵存在 “头尾奇异值差距悬殊”（stark difference）：头部奇异值的幅值远大于尾部（比如头部幅值接近 2.1，尾部接近 0.05），这种极端分布会导致合并时 “头部方向主导，尾部方向易被扭曲”；
- ② 剪枝（Original+Pruning）可有效缩小头尾差距：剪枝后尾部奇异值的幅值被显著 “拉高”（比如尾部幅值从 0.05 提升至 0.5 左右），而头部幅值变化较小 —— 这验证了 3.2 节 “缩放尾部奇异值可提升方向鲁棒性” 的猜想，为后续 RobustMerge 的 “剪枝与互补参数缩放” 组件提供了实验支撑。

奇异值的 “幅值”= 模型对该奇异值对应 “任务知识” 的 “依赖度 / 使用优先级”：

- 幅值越大 → 知识越核心 → 模型处理任务时用得越多 → 利用程度越高；
- 幅值越小 → 知识越次要 → 模型处理任务时用得越少 → 利用程度越低。

#### 那么该论文是如何进行剪枝的？

剪枝直接作用于 PEFT 调优得到的低秩矩阵 A 和 B，具体操作是为 A 和 B 分别生成二进制掩码矩阵$(\mathcal{M}_A(k))、(\mathcal{M}_B(k)$：

- 掩码矩阵的每个元素为 “1” 或 “0”：“1” 代表保留对应位置的参数，“0” 代表剔除（置零）；
- 掩码的生成规则：按参数幅值从大到小排序，保留前 k%（剪枝率 k）的大参数，将剩余（100-k）% 的小参数置零。
- 例如，若剪枝率 k=50%，则对 A 矩阵中所有参数按幅值排序后，保留前 50% 的大参数，后 50% 的小参数设为 0；B 矩阵同理。

#### 与传统剪枝方法的差异

论文明确指出，传统 FFT 合并的剪枝方法（如 Ties-merging 的 “符号筛选”、DARE 的 “随机剪枝”）不适合 PEFT：

- 符号筛选：PEFT 参数分布宽，“符号冲突” 不是核心问题，筛选符号会破坏低秩方向；
- 随机剪枝：可能误删大参数，导致核心知识丢失；
    
    而论文的 “幅值剪枝” 能精准保留关键参数，最小化对方向鲁棒性的影响。
    

Figure 3b（缩放奇异值，放大小奇异值）：

1. 缩放策略的 “针对性”：对尾部小奇异值的缩放幅度，远大于头部大奇异值

从图中趋势可见：

- 对于左侧头部大奇异值（索引 1-4）：缩放系数接近 1（或小幅大于 1），意味着 RobustMerge 对其 “几乎不缩放” 或 “轻微缩放”—— 因为大奇异值的方向本身稳定（3.2 节观察），过度缩放反而可能破坏核心任务知识；
- 对于右侧尾部小奇异值（索引 12-16）：缩放系数显著大于 1（如达到 1.5-2 倍），意味着 RobustMerge 对其 “大幅缩放”—— 因为小奇异值的方向易被干扰（3.2 节核心痛点），通过放大其幅值，可缩小与头部奇异值的差距，增强方向稳定性。

这一趋势直接体现了 “互补” 的含义：不平均缩放所有奇异值，而是 “缺啥补啥”—— 重点强化尾部小奇异值，对头部大奇异值 “少动或不动”。

2. 缩放的目的：缓解 “方向不稳定性”，平衡任务知识利用程度

结合 3.2 节的理论（小奇异值方向易变导致任务干扰），图 3b 的缩放策略本质是：

- 小奇异值幅值被放大后，其在低秩空间中的 “话语权” 提升 —— 原本易被大奇异值 “带偏” 的方向（对应次要但必要的任务知识），现在能稳定保留，减少合并时的知识丢失；
- 大奇异值仅轻微缩放，确保其对应的 “核心任务知识” 不被破坏 —— 避免因过度调整导致核心方向偏移。

例如：某任务的 “细节特征知识”（对应小奇异值）原本幅值小，合并时易被其他任务的 “全局特征知识”（对应大奇异值）覆盖；缩放后，“细节特征知识” 的幅值提升，能与 “全局特征知识” 平衡，从而保留该任务的完整知识。

#### 先剪枝（去掉噪音和垃圾），再缩放（把次要重要的内容放大），前后顺序不能乱，否则会放大噪声。

#### Figure 3c：

论文指出 LoRA 的两个矩阵具有不对称性：矩阵 $B$ 通常服从**高斯分布**，而矩阵 $A$ 近似服从**均匀分**布，整体分布范围比 FFT 宽得多。

Figure 3(c) 展示了全量微调（FFT）参数与参数高效微调（PEFT, 此处指 LoRA）参数在数值分布上的显著不同：

- **FFT 参数（深蓝色区域）：分布集中且幅值小**
    
    - **原因：** 在全量微调的模型合并中（如 Task Arithmetic），合并的对象通常是“任务向量”（Task Vector），即微调后的模型权重减去预训练模型权重（$\tau = \theta_{ft} - \theta_{pre}$）2。由于预训练模型已经非常强大，微调通常只需要对海量参数进行微小的调整即可适应下游任务。因此，绝大多数参数的改变量都非常微小，且集中在 0 附近，形成尖锐的分布 33。
    - **导致的问题：** 在这种分布下，主要的干扰来源是不同模型参数之间的“符号冲突”（即一个模型想让参数变大，另一个想让其变小），因此像 Ties-Merging 这种基于符号选举的方法在 FFT 中很有效 4444。
        
- **PEFT 参数（浅蓝/灰色区域）：分布范围宽**
    
    - **原因：** LoRA 将参数更新分解为两个低秩矩阵 $A$ 和 $B$（$W = W_0 + B \cdot A$）5。由于秩（rank）远小于原始维度（$r \ll d$），为了达到与全量微调相似的效果，这两个小矩阵中的每个参数需要承载更大的数值变化。
    - **分布特性：** 论文指出 LoRA 的两个矩阵具有不对称性：矩阵 $B$ 通常服从高斯分布，而矩阵 $A$ 近似服从均匀分布，整体分布范围比 FFT 宽得多 6666。
    - **导致的问题：** 由于参数分布较宽且包含较大数值，简单的随机剪枝（如 DARE）或仅关注符号（如 Ties）会破坏保留在这些大数值参数中的关键方向信息（Direction Instability）7777。

## 🔬实验方法：

## 📜 总结：