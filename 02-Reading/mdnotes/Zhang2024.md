# 🔤 解锁低秩适配器中的全球协同效应 (2024, Zhang) ()

**原名：**Unlocking the Global Synergies in Low-Rank Adapters

**译名：** 解锁低秩适配器中的全球协同效应

**作者：**Zhang et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/ARXIV.2406.14956](https://doi.org/10.48550/ARXIV.2406.14956)

**发表时间：**2024

**本地链接:** [2024-Unlocking the Global Synergies in Low-Rank Adapters.pdf](zotero://open-pdf/0_ABG5AWSW)


我们提出了一种动态HeteroLoRA框架，该框架自动确定LLM微调中LoRA模块的“开/关”状态。

---
1. 这篇论文提出了：lora模块和LoRA 适配的 Shortcut 模块。该模块可以替换残差链接。 (LoRA-adapted Shortcut)**：
- **位置**：
    - 蓝色： **Residual Shortcut（残差捷径）**：替换每一层内部原本的残差连接（图 3 中的蓝色块）
	    位置：位于 Transformer 层内部。$s_{res1}$ 跨越 Attention 层，$s_{res2}$ 跨越 MLP 层 。
	    实质：这就是上述被“升级”的原地残差连接。
	- 绿色块：同层式跨层捷径 ($s_{in}$)
		位置：连接不同层的输入位置（例如从第 $i$ 层的输入直接连到第 $i+1$ 层的输入）
		初始状态：$W_0 = 0$（因为它原本不存在）。
		作用：建立新的“高速公路”，让信息可以跳过某些处理步骤直接向后传递。
    - 红色块：**Cross-Layer Shortcut（跨层捷径）**：直接跨越层级连接。例如，从第 $i$ 层的输入直接连到第 $i+1$ 层的输出，跳过中间的结构（图 3 中的绿色/红色块）。

2. 这篇论文不做“微调秩的大小”（比如这一层调成3，那一层调成5），而是做**“二选一的开关”**（要么给你高配 $r=8$，要么直接关掉），以此来模拟多秩分配的效果。

3. 这篇论文对于连接方式进行了修改（Figure3）

4. Table2，只应用dynamic去选择怎么分配秩，其实只提升了0.3个点。

5. - **基准（Baseline）**：所有 LoRA 模块都打开，但是秩很低，**$r=2$**。这是“大锅饭”模式。
	- **挑战者（HeteroLoRA）**：只有 **25%** 的模块打开，但是给高配 **$r=8$**。这是“精英模式”。

6. 这篇文章发现**零成本代理** 把所有 LoRA 模块扫描一遍，问谁最重要，效果很差。

7. Figure 4 发现了一个规律：**Value 投影（$W_V$）通常比 Query 投影（$W_Q$）更重要**（被开启的频率更高）。更容易拿到高分，被分配更高的秩.

8. 修改残差链接和动态选择top25%的高秩能提升的都不多，我认为最有价值的是Figure4实验：
	![[image-101.png]]
---

### 0. 翻译摘要原文

LoRA（低秩适配）已成为大型语言模型参数高效微调（PEFT）的事实标准。

1 我们提出了 HeteroLoRA，这是一种轻量级的搜索算法，利用零成本代理（zero-cost proxies）将有限的 LoRA 可训练参数分配给模型中的不同部分，以获得更好的微调性能。

2 除了对标准 LoRA 适配模型进行分配外，我们还通过在一个更具挑战性的搜索空间（包含 LoRA 模块和 LoRA 适配的快捷连接）中执行分配，证明了 HeteroLoRA 的有效性。

3 实验表明，在相同的参数预算下，HeteroLoRA 能够提升模型性能。

4 例如，在 MRPC 任务上，我们在保持相似训练参数预算的情况下，准确率提高了 1.6%。

---

### 1. 方法动机 (Motivation)

#### a) 为什么提出这个方法？

作者希望解决“资源分配效率”的问题。

微调大模型时，通常有两个限制：显存和计算预算。作者认为，简单地把参数均匀撒在每一层（Standard LoRA）是低效的。他们希望像神经架构搜索（NAS）一样，自动找到哪些层更重要，把好钢用在刀刃上，甚至通过增加“快捷路”（Shortcuts）来增强层与层之间的信息流动（即所谓的 Global Synergies）。

#### b) 现有方法的痛点/不足

1. **均匀分配的盲目性**：现有的 LoRA 方法通常在所有层使用相同的秩 $r$（例如 $r=8$）。这忽略了不同层对特定下游任务的贡献差异巨大的事实 7。
    
2. **架构搜索的局限**：目前的 LoRA 实现主要局限于 Transformer 的原有架构（Attention/MLP 层），很少有人探索是否可以通过添加新的连接路径（如跨层 Shortcuts）来提升性能 8。
    
3. **高昂的搜索成本**：传统的 NAS（神经架构搜索）太贵了，不适合微调场景。需要一种极其廉价（Zero-cost）的方法来评估模块重要性。
    

#### c) 核心假设与直觉

- **假设 1**：在参数预算固定的情况下，与其让所有层都“平庸”地开启（低秩），不如让重要的层“高配”（高秩），关掉不重要的层。
    
- **假设 2**：增加跨层的线性快捷连接（Shortcuts）可以充当“高速公路”，不仅能补充 LoRA 的能力，还能促进全局信息的协同 9。
    

---

### 2. 方法设计 (Method Design)

这是一个结合了**动态分配**和**架构扩展**的方法。

#### a) 详细 Pipeline (HeteroLoRA 流程)

核心逻辑是：**初始化过量的模块 → 训练中定期评估重要性 → 动态开启Top-K模块**。

1. **扩展搜索空间初始化 (Initialization)**：
    
    - 在 Transformer 的每一层，除了标准的 LoRA 模块（针对 $W_Q, W_V$ 等），额外插入“LoRA适配的快捷连接”（Shortcuts）。
        
    - 此时，潜在的可训练模块数量远超预算。
        
2. **训练启动与预热**：
    
    - 开始常规的微调训练。
        
3. **周期性配置搜索 (Dynamic Search Step)** 10101010：
    
    - **触发时机**：例如每 $1/5$ 个 Epoch 触发一次。
        
    - **重要性评估 (Saliency Estimation)**：
        
        - 从训练集中取一小批数据（如 32 batches）11。
            
        - 使用 **GRAD-NORM**（梯度范数）作为代理指标计算每个模块 $M$ 的重要性得分 $S(M)$。
            
        - **公式**：$S_{gradnorm}(M) = ||\frac{\partial \mathcal{L}}{\partial A}||_2 + ||\frac{\partial \mathcal{L}}{\partial B}||_2$。即：如果移除或改变这个模块会导致 Loss 剧烈变化（梯度大），说明它很重要 12121212。
            
    - **Top-K 选择**：根据得分对所有模块（标准 LoRA + Shortcuts）进行排序。
        
    - **状态切换**：
        
        - 保留得分最高的 Top $P\%$（例如 25%）的模块，状态设为 **Enabled**。
            
        - 其余模块设为 **Disabled**（冻结或置零，不参与后续几步的更新）。
            
4. **继续训练**：
    
    - 使用更新后的活跃模块配置继续训练，直到下一个搜索周期。
        
5. **输出**：
    
    - 最终得到一个针对特定任务“定制”了秩分配（Rank Allocation）和连接结构的模型。
        

#### b) 模型结构：LoRA-Adapted Shortcuts

为了增强“全局协同”，作者引入了两种 Shortcut 13131313：

- **Residual Shortcut ($s_{res}$)**：替代 Transformer 块内部原有的残差连接。初始权重 $W_0 = I$ (单位矩阵)。
    
- **Cross-Layer Shortcut ($s_{in}, s_{cut}$)**：跨越 Transformer 块的连接（例如从第 $i$ 层直接连到 $i+1$ 层的某个位置）。初始权重 $W_0 = 0$（因为原模型没这条路）。
    
- **数学形式**：$W = W_0 + \frac{\alpha}{r}BA$。这使得 Shortcuts 也可以像 LoRA 一样低成本训练 14。
    

#### c) 关键公式解读

- **Saliency (GRAD-NORM)**: $S(M) = \sum_{\theta \in M} | \nabla_\theta \mathcal{L} |$。
    
    - _直白解释_：**梯度即信号**。如果模型极力想修改某个模块的参数（梯度大），说明该模块当前的配置让模型“很不舒服”，它是瓶颈，必须保留并优化。
        

---

### 3. 与其他方法对比

|**对比维度**|**Standard LoRA (Baseline)**|**Pruning-based PEFT**|**HeteroLoRA (本方法)**|
|---|---|---|---|
|**参数分配**|**均匀** (Uniform)：每层秩相同|**静态剪枝**：训练前定生死|**动态分配** (Dynamic)：训练中根据重要性调整开关|
|**搜索空间**|仅限原模型权重 (Attention/FFN)|仅限原模型权重|**扩展空间**：包含原权重 + 新增的 Shortcuts|
|**搜索成本**|无 (手动指定 $r$)|低 (一次性)|**中低** (周期性计算梯度，但在训练循环内完成)|
|**架构灵活性**|刚性|刚性（做减法）|**弹性**（做加法：不仅分配秩，还增加连接路径）|

**本质不同/创新点**：

1. **动态性**：不是在一开始就决定好谁重要，而是承认在训练不同阶段，模块的重要性会变化（Dynamic HeteroLoRA > Static HeteroLoRA）15。
    
2. **架构突破**：首次在 PEFT 中大规模引入并搜索额外的 Shortcut 连接，证明了增加连接路径比单纯增加 LoRA 秩更有效 16。
    

---

### 4. 实验表现与优势

#### a) 实验设置

- **模型**：OPT-350M, RoBERTa-large 17。
    
- **数据集**：GLUE benchmark (MRPC, RTE, SST-2)。
    
- **对比基准**：在**相同可训练参数量**（Parameter Budget）的前提下，对比 Standard LoRA 和 HeteroLoRA。
    

#### b) 关键结果

1. **HeteroLoRA 更强**：在 MRPC 上，HeteroLoRA (S&L) 达到 **85.0%** 准确率，而基准 LoRA (S&L, $r=8$) 为 84.6%，提升显著 18。
    
2. **Shortcuts 有用**：引入 Shortcuts 后的模型表现普遍优于只用 LoRA 的模型。例如在 2.3M 参数预算下，Shortcuts 组平均分 92.6，LoRA-only 组 92.1 19。
    
3. **GRAD-NORM 最佳**：对比 SNIP, SYNFLOW 等代理指标，基于梯度的 GRAD-NORM 效果最好 20。
    

#### c) 局限性与风险 (Critique)

- **模型规模偏小**：实验主要在 350M 参数模型上进行。**这是最大的风险点**。在 7B、70B 这样的大模型上，残差流（Residual Stream）的动态非常不同，跨层 Shortcut 可能会导致训练不稳定或收益递减，论文未验证大规模场景。
    
- **计算开销被低估**：虽然叫 "Zero-cost proxy"，但计算所有模块的梯度（包括那些最终被 disable 的模块）需要前向和反向传播。尽管不更新权重，但显存占用和计算时间并非“零”。这实际上是用时间换空间效率。
    
- **超参敏感**：引入了新的超参（搜索频率、Enable rate、Shortcut 的 scaling factor $\alpha$），增加了调参的复杂度。
    

---

### 5. 学习与应用

#### a) 实现/复现关键

- **开源**：作者承诺接收后开源 21，目前需自己实现。
    
- **核心逻辑**：
    
    1. 定义一个 `LoRALinear` 层，包含额外的 `active` 布尔标志。
        
    2. 定义 `Shortcut` 模块。
        
    3. 在训练 Loop 中插入 `search_step()`：
        
        - Forward pass on a batch.
            
        - `loss.backward()`
            
        - 收集所有 LoRA 层的 `.grad` 范数。
            
        - `topk_indices = argsort(grads)`
            
        - 设置对应层的 `active = True/False`。
            
        - `optimizer.zero_grad()` (清除这次搜索的梯度)。
            

#### b) 避坑指南

- **Shortcuts 初始化**：Cross-layer shortcut 的 $W_0$ 必须初始化为 **0** 22，否则模型初始状态就被破坏了，导致无法训练。
    
- **搜索频率**：不需要每个 step 都搜。论文建议每 **1/5 个 epoch** 搜一次 23。太频繁浪费时间，太稀疏捕捉不到变化。
    
- **LayerNorm**：Shortcut 合并回主干后，必须再加一个 LayerNorm 24，否则数值会不稳定。
    

#### c) 迁移性

- 该方法完全可以迁移到 LLaMA 3 或 Mistral 上。
    
- **建议**：在显存受限但希望榨干模型性能时使用。特别是当你发现标准的 LoRA 效果在这个任务上已经饱和，加秩 ($r$) 也没用时，试试加 Shortcut。
    

---

### 6. 总结

#### a) 核心思想

利用梯度范数动态筛选重要的LoRA层，并引入跨层捷径以增强全局协同。

#### b) 速记版 Pipeline (The "No-Fluff" Version)

1. **全员进场**：给模型每一层都装上 LoRA 和额外的快捷通道（Shortcuts）。
    
2. **试跑测压**：正常训练一小段时间，然后暂停。
    
3. **末位淘汰**：计算每个模块的梯度（谁反应最剧烈说明谁最重要），保留前 25% 的模块。
    
4. **冻结闲人**：关掉剩下的 75% 模块（不更新也不参与计算）。
    
5. **循环往复**：带着精锐部队继续练，每隔一阵子重新测压、重新选人。
    
