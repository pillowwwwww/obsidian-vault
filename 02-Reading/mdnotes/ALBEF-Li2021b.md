# 🔤 ALBEF:先对齐再融合：采用动量蒸馏的视觉与语言表征学习 (2021, Li) ()

**原名：** Align before fuse Vision and language representation learning with momentum distillation

**译名：** ALBEF：先对齐再融合：采用动量蒸馏的视觉与语言表征学习

**作者：**Li et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/ARXIV.2107.07651](https://doi.org/10.48550/ARXIV.2107.07651)

**发表时间：**2021

**本地链接:** [2021-Align before fuse Vision and language representation learning with momentum distillation.pdf](zotero://open-pdf/0_9JHP2MRS)

**摘要翻译：** _大规模视觉与语言表征学习已在各种视觉-语言任务上展现出显著改进。大多数现有方法采用基于 Transformer 的多模态编码器，以共同建模视觉标记（基于区域的图像特征）和词语标记。由于视觉标记和词语标记不对齐，多模态编码器难以学习图像-文本交互。在本文中，我们引入了一种对比损失，以在通过跨模态注意力融合（ALBEF）图像和文本表征之前对其进行对齐，这使得更扎实的视觉与语言表征学习成为可能。与大多数现有方法不同，我们的方法不需要边界框标注，也不需要高分辨率图像。为了改进从嘈杂网络数据中学习的效果，我们提出了动量蒸馏，这是一种从动量模型生成的伪目标中学习的自训练方法。我们从互信息最大化的角度对 ALBEF 进行了理论分析，表明不同的训练任务可以被解释为为图像-文本对生成不同视图的不同方式。ALBEF 在多个下游视觉-语言任务上取得了最先进的性能。在图像-文本检索任务上，ALBEF 优于那些在数量级更大的数据集上进行预训练的方法。在 VQA 和 NLVR$^2$ 任务上，与最先进水平相比，ALBEF 取得了 2.37% 和 3.84% 的绝对改进，同时享有更快的推理速度。代码和预训练模型可在 https://github.com/salesforce/ALBEF/ 获取。_

## 💡创新点

- 把文本的12层BERT拆开，前六层作为文本encoder，后六层作为多模态融合。图像使用12层ViT
    
- 三个损失：ITC, MLM, ITM
    

MLM: 把一些单词mask掉，BEFT常用。

ITC： image-text的对比学习损失。

ITM: 判断image 和 text 是否是一个文本对。 二分类任务。判断谁是正样本对可能有一点难度，但是判断负样本对就太简单了。所以我们把同一个batch中的最接近正样本的负样本算出来（求一个相似度，选最高的）作为负样本，那么这个就变得非常有挑战性了。

可以看到ITC和ITM都使用了image text input，但是MLM使用的是 image text’ input。也就是说整个过程有两对输入，进行了两次前向。

- 对比学习：使用hard negative
    
- 设计了动量模型，使用pseudo-targets给训练器一个第二个label， 去防止noisy web data的干扰。
    

### 流程

我的理解如下：

1. 分别进入text ,image encoder 。输出embedding
    
2. 计算对比学习损失ITC
    
3. 把第一步的text embedding, image embedding输入多模态融合器，计算ITM
    
4. 再进一遍text encoder，输出 text’ embedding
    
5. text’ embedding,image embedding输入多模态融合器，计算MLM
    

你的 5 步理解已经很接近了，但需要把“**全局向量 vs. 逐 token 序列**”、以及“**两次进入的是多模态编码器而不是只跑文本编码器**”这两点拧正。我按 **ALBEF 论文与官方实现** 来把流程梳理成“每个迭代里实际发生了什么”。

** 一次训练迭代（更正后的流程）**

**0) 准备三个子网**

- **图像编码器**：ViT-B/16，把图像切成 patch，得到 token 序列（含一个 [CLS]）；
    
- **文本编码器**：BERT-base 前 6 层，把句子变成词 token 序列（含 [CLS]）；
    
- **多模态编码器**：BERT-base 后 6 层，每层带**跨模态注意力**，用来“图文互看、融合
    

**1) 各自编码（一次前向，供 ITC & 后续复用）**

- 图像 (I) → **图像 token 序列** $$({v_{\text{cls}}, v_1,\dots,v_N})$$
- 文本 (T) → **文本 token 序列** $$({w_{\text{cls}}, w_1,\dots,w_L})$$
- 取两侧的 **[CLS] 向量** 各过一个线性头并 **L2 归一化** 到 **256 维**，得到一对可做相似度计算的全局向量；这里产生 **ITC** 所需的表示。
    
> 关键更正：这里有**两种“输出”**：
> 
> - 用于 **ITC** 的**全局 [CLS] 向量**（256 维、归一化、只做对比）；
>     
> - 用于 **后续融合** 的**逐 token 序列**（词/patch 序列），这才是要喂给多模态编码器的输入，**不是**那个 256 维全局向量。
>     

**2) 计算 ITC（图文对比对齐）**

- 用上一步的 **[CLS] 全局向量** 计算批内图↔文相似度矩阵，做**双向 InfoNCE / 交叉熵**；
    
- 维护 **动量编码器 + 队列**（MoCo 风格）产生更稳定的负样本；这一步同时为 ITM **在线挖难负**提供依据；（ALBEF 还引入 **Momentum Distillation**，用动量模型的软分布做蒸馏）。
    

**3) ITM：图文匹配真假（多模态前向 #1，喂 (I, T)）**

- **输入给多模态编码器的是“逐 token 序列”**：图像 patch 序列 + 文本词序列（完整的 **T**，不遮盖）；
    
- 在每层里做**跨模态注意力**让两侧互看，最后用 [CLS] 接二分类头做 **match / mismatch**；
    
- 负样本来自上一步 ITC 的**难负挖掘**。
    

> 更正你第 3 步：**不是把 256 维的“text/image embedding”喂进去**；而是把**两侧的 token 序列**喂进**多模态编码器**做交互，再算 ITM。

**4) MLM：掩码词预测（多模态前向 #2，喂 (I, T′)）**

- 把 **T** 按 BERT 规则**随机 15% mask** 得到 **T′**（80% [MASK]/10% 随机/10% 保留）；
    
- 仍然把 **图像 token 序列 + 文本 token 序列（T′）** 喂给**同一套多模态编码器**，只是在被 mask 的位置做词预测的交叉熵（可叠加动量蒸馏的 KL 项）。
    

> 更正你第 4、5 步：**MLM 不是“只再跑一遍文本编码器”**。它与 ITM 一样，**必须**把 (I, T′) **一起**进多模态编码器，让图像帮助补词；只是文本侧换成了 **T′**，所以需要**再前向一次**。

**5) 汇总损失并更新**

- 总损失：(;L = L_{\text{ITC}} + L_{\text{ITM}} + L_{\text{MLM}};)（加上 MoD 版本则再叠 KL 项）；
    
- 对“学生模型”做一次反向与参数更新；“动量老师”用 EMA 更新，用于下一步产生软目标
    

## 用“伪代码”看一眼（突出两次多模态前向）

```
V_tokens = image_encoder(I)        # {v_cls, v1..vN}
W_tokens = text_encoder(T)         # {w_cls, w1..wL}
ITC on [CLS] (带队列/动量蒸馏)
ITC = contrastive( proj(v_cls), proj(w_cls) )
ITM（多模态前向 #1）：输入 (I, T) 的 token 序列
ITM = itm_classifier( multimodal_encoder(V_tokens, W_tokens) )
MLM（多模态前向 #2）：输入 (I, T′) 的 token 序列
W_tokens_masked = text_encoder(T')  # T' 是把 T 做了 15% mask
MLM = mlm_head( multimodal_encoder(V_tokens, W_tokens_masked) )
loss = ITC + ITM + MLM (+ MoD 项)
loss.backward(); optimizer.step(); update_momentum_teacher()

## 你原始 5 步里的两处关键修改

- **第 3 步**：输入多模态融合器的是**token 序列**，不是 ITC 的那种**全局 256 维 embedding**。
    
- **第 4/5 步**：MLM **不是只跑文本编码器**，而是与图像一起再次进入**同一个多模态编码器**做遮盖词预测；这就是“**两次多模态前向**”的由来（一次 (I,T) 做 ITM，一次 (I,T′) 做 MLM）。
```

---

## 🌏研究背景：

## 以前（“先检测再融合”）

- **先跑目标检测器**（如 Faster R-CNN），在图上找出 K 个“物体框”（dog、sofa…）。
    
- 每个框做 **ROI 特征**（向量），这些 **K 个区域特征**当成“视觉 token”，再和文本 token 一起送进多模态 Transformer。
    
- 缺点：**要依赖框标注**、前置计算很重，而且能看到的语义受检测器“词表/粒度”限制。
    

## 现在（ViLT/ALBEF 这类“无检测器”路线）

- **不再跑检测器**，不需要先知道“图里有什么”。
    
- 把图像 **切成固定大小的 patches**（比如 32×32），每个 patch **展平后做一次线性投影**，得到一个向量（就像文本的词向量）。
    
- **注意**：这一步“线性投影”只是**把 patch 变成 token**；**后面还要经过多层 Transformer**（单流或跨模态）去做自注意力、跨模态交互、下游任务学习。
    
    - ViLT：patch→线性投影→（和文本 token 拼接）→**同一个 Transformer** 里做跨模态建模（单流），完全**无检测器**。
        
    - ALBEF：图像用 ViT 式编码（也是 patch→线性投影起步），先做 **对比对齐（ITC）**，再把图文序列送进**跨模态 Transformer**做 **ITM/MLM**，同样**不依赖检测器**。
        

### 一句话对齐

- 你的总结“现在不监测，直接线性变换”**抓住了第一步**：确实**不做目标检测**，图像先**用线性投影把 patch 变成向量**。
    
- 但**不是只做这一层就完了**：**真正的表征与对齐**是在后面的 **Transformer 层**里学出来的；区别只是在于**前端不再需要检测器和框特征**了。
    

这样做的好处：

- **省去框标注与重前端**（更轻更快）；
    
- **端到端训练**，不被检测器词表束缚；
    
- 仍能在 VQA、NLVR2、图文检索等任务上取得与“检测器派”相当甚至更好的效果。
    

---

## 🌟下游任务：

图文检索： recall召回率

视觉蕴含：给定一个假设，看看能否去推理出这个前提。（三分类问题，指标应该是分类准确度）

VQA：视觉问答，能否回答出来这个问题？答案都是固定的，是一个答案set，变成了多分类问题，指标是准确度。（分为闭集VQA和开集VQA文本生成，难度大了很多）

视觉推理：一个文本能不能同时描述一对图片，二分类问题，准确度。

## 🔬实验方法：

## 📜 总结：

有趣的研究发现：从noisy web data中学到有用的表征。