# 🔤 NeuIPS2025:大型语言模型的激活导向共识合并 (2025, Yao) ()

**原名：**Activation-guided consensus merging for large language models

**译名：** NeuIPS2025:大型语言模型的激活导向共识合并

**作者：**Yao et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/ARXIV.2505.14009](https://doi.org/10.48550/ARXIV.2505.14009)

**发表时间：**2025

**本地链接:** [2025-Activation-guided consensus merging for large language models.pdf](zotero://open-pdf/0_9H8J7P5X)

**摘要翻译：**

你好！我是Gemini。作为一名深挖论文细节的AI研究生，我仔细研读了这篇名为《Activation-Guided Consensus Merging for Large Language Models》的论文。

这篇论文针对模型融合（Model Merging）中“层级重要性异质性”的问题，提出了一种基于**互信息（Mutual Information）**的免训练、即插即用方法，特别是在“长思维转短思维”（Long-to-Short, L2S）场景下表现出色。

以下是针对该论文方法部分的深度分析与总结：

0. 摘要原文翻译

最近的研究越来越关注如何协调“系统2”的推理能力与“系统1”的效率。虽然现有的基于训练和基于提示（Prompt）的方法在效率和稳定性方面面临巨大挑战，但模型融合（Model Merging）作为一种将不同大语言模型（LLMs）的多样化能力整合到一个统一模型中的策略，展现出了广阔的前景 1。然而，传统的模型融合方法通常假设各层的重要性是统一的，忽略了神经组件内在的功能异质性 2。为了解决这一局限，我们提出了激活引导的共识融合（Activation-Guided Consensus Merging, ACM），这是一个即插即用的融合框架，它根据预训练模型和微调模型激活值之间的互信息来确定层级特定的融合系数 3。ACM有效地保留了特定任务的能力，且无需梯度计算或额外训练 4。在长转短（Long-to-Short, L2S）和通用融合任务上的广泛实验表明，ACM始终优于所有基线方法 5。例如，在Qwen-7B模型案例中，配备ACM的TIES-Merging方法在响应长度减少55.3%的同时，推理准确率提高了1.3个点 66。

1. 方法动机 (Motivation)

a) 作者为什么提出这个方法？

作者的核心驱动力是解决Long-to-Short (L2S) 框架中的矛盾：如何结合“系统2”模型（推理强但啰嗦、慢）的准确性与“系统1”模型（直觉强但推理弱、快）的效率，同时避免高昂的训练成本 7。模型融合（Model Merging）是一个低成本的解决方案，但需要更精细的控制手段。

b) 现有方法的痛点是什么？

统一缩放的局限性： 现有的任务向量（Task Vector）方法（如Task Arithmetic, TIES-Merging）通常对所有层使用静态且统一的缩放系数（Scaling Coefficient）8。

忽略层级差异： 神经网络的各层功能不同（heterogeneity）。例如在L2S任务中，lm_head 层控制生成长度，其重要性可能与其他层完全不同，统一权重无法处理这种差异 9。

其他动态方法的缺陷：

基于梯度的敏感度方法（如Sens-Merging）计算复杂，需要反向传播 10。

部分基于激活的方法（如AIM）只关注预训练（PT）模型的激活，忽略了微调（FT）模型与PT模型之间的交互关系 11。

c) 研究假设/直觉

直觉： 激活值的分布模式与权重的显著性（Salience）高度相关 12。

核心假设： 如果某一层微调模型（FT）的激活值与预训练模型（PT）的激活值具有很高的互信息（Mutual Information, MI），说明该层变化不大，信息冗余高，融合时应分配较低的权重；反之，如果互信息低，说明该层学到了独特的任务知识，应分配较高的权重 131313。

2. 方法设计 (Method Methodology)

ACM 是一种层级自适应（Layer-wise Adaptive） 的加权策略，它可以无缝集成到现有的任务向量融合方法中。

a) 方法流程 (Pipeline)

准备阶段：校准数据 (Calibration Corpus)

准备一个包含少量样本（约100条即可）的校准数据集 $D$（如s1K数据集）141414。

聚类采样： 为了数据平衡，先对数据进行K-means聚类（如聚成20类），然后均匀采样，避免数据分布偏差 15151515。

第一步：提取激活值 (Activation Extraction)

将校准数据输入到 预训练模型 (Base Model, $\theta_0$) 和 微调模型 (FT Models, $\theta_i$) 中。

收集每一层 $k$ 的激活值输出，记为 $A_0^k$（Base模型）和 $A_i^k$（第 $i$ 个FT模型） 16。

第二步：计算互信息 (Calculate Mutual Information)

对于每一层 $k$，计算 Base 模型激活值与 FT 模型激活值之间的互信息 $I_i^k$：

$$I_i^k = I(A_0^k, A_i^k)$$

物理含义： 互信息量化了两个模型在该层处理数据时的依赖程度。

技术细节： 为了数值稳定性，计算时会进行平移变换（减去最大MI值）17。

第三步：计算层级融合系数 (Compute Layer-specific Coefficients)

利用 Sigmoid 函数的变体将互信息映射到权重系数 $\lambda_i^k$：

$$\lambda_i^k = 1 - \frac{1}{1 + e^{-t \cdot I_i^k}}$$

公式解读：

$t$ 是温度超参数（Hyperparameter），控制函数的陡峭程度（论文推荐 $t=0.7$ 具有鲁棒性）18181818。

反比关系： 注意公式结构。$I_i^k$（互信息）越大 $\rightarrow$ $e^{-t \cdot I_i^k}$ 越小 $\rightarrow$ 分母越小 $\rightarrow$ 减数越大 $\rightarrow$ $\lambda_i^k$（权重）越小。

这符合核心假设：相似度高（MI大）= 冗余 = 权重低；相似度低（MI小）= 特异性强 = 权重高 19。

第四步：执行融合 (Merge)

将计算出的 $\lambda_i^k$ 应用于任务向量（Task Vector, $\delta_i = \theta_i - \theta_0$）。

最终融合参数 $\theta_{merged}$ 的第 $k$ 层计算如下：

$$\theta_{merged}^k = \theta_0^k + \frac{1}{N} \sum_{i=1}^{N} \lambda_i^k \cdot (\theta_i^k - \theta_0^k)$$

该系数可以应用在 Task Arithmetic 或 TIES-Merging 等方法中 20。

b) 关键模块功能

互信息 (MI) 模块： 充当“差异检测器”。它不直接比较权重数值，而是比较模型对数据的“反应”（激活）。相比KL散度（不对称）和余弦相似度（波动大、有负值），MI更稳健 21。

系数映射模块 (Sigmoid-based)： 充当“归一化控制器”。将MI的统计值转化为 0~1 之间的可操作权重。

3. 与其他方法对比

a) 本质不同

Task Arithmetic / TIES / DARE: 这些是静态方法，假设所有层对任务的贡献相同，使用全局统一的标量。

Sens-Merging: 是基于梯度的动态方法，需要计算参数对Loss的敏感度，计算量大。

AIM: 虽然也是基于激活，但仅看 Base 模型的激活幅度，忽略了 FT 模型到底学到了什么新东西（即忽略了 Base 与 FT 的交互）。

ACM (本文): 关注 Base与FT 激活值的相关性 (互信息)，捕捉的是“信息增量”。

b) 创新点

理论关联： 建立了激活空间互信息与权重显著性之间的理论联系 22。

反向加权策略： 创造性地利用“互信息越大权重越小”的策略来去除冗余，保留FT模型的独特能力 23。

  

无梯度高效性： 仅需前向传播（Inference）即可获得系数，速度极快（几分钟内）24。

c) 适用场景

Long-to-Short (L2S): 需要在保留推理能力的同时压缩输出长度。

同构模型融合： 必须是基于同一Base模型微调出来的不同模型（权重形状一致）。

d) 方法对比表

特性Task Arithmetic TIES-Merging Sens-Merging ACM (Ours)权重策略静态 (Static)静态 (Static)动态 (Dynamic)动态 (Dynamic)层级特异性无 (Uniform)无 (Uniform)有 (Layer-wise)有 (Layer-wise)计算依赖无权重幅度 (Pruning)梯度 (Gradients)激活值 (Activations)是否需训练否否否 (但需反向传播)否 (仅前向传播)主要度量标量系数 $\lambda$符号冲突解决Fisher信息/梯度互信息 (Mutual Info)主要优点简单、快速解决参数干扰精度高精度高、去冗余强、快4. 实验表现与优势

a) 实验设置

任务：

  

L2S 推理： 使用 Qwen2.5-Math (1.5B, 7B, 14B, 32B) 和 DeepSeek-R1 (蒸馏版) 系列。数据集包括 GSM8K, MATH500, AIME2024 等 28。

  

代码生成： HumanEval-Pro, LiveCodeBench 29。

  

通用融合： LLaMA系列模型在常识推理任务上的融合 30。

对比基线： Average Merging, Task Arithmetic, TIES, DARE, AIM, Sens-Merging。

b) 关键结果

L2S 效果 (Qwen-7B)：

相比 TIES-Merging，ACM-TIES 将响应长度减少了 55.3%（去冗余能力极强），同时准确率提升了 1.3% 31313131。

相比 Task Arithmetic，ACM-TA 在保持准确率的同时大幅压缩了长度 32。

小模型 vs 大模型：

在小模型（1.5B, 7B）上，ACM能同时实现“长度缩减”和“精度提升” 33。

在大模型（14B, 32B）上，ACM在代码任务上表现更好，数学任务上倾向于用更长的输出来换取更高的准确率 34343434。

层级系数可视化： 实验发现 embed 层和 lm_head 层的互信息较高（权重应低），而中间层互信息较低（权重应高），证明了层级差异的必要性 35。

c) 局限性  

未在超大模型测试： 由于资源限制，未在 LLaMA-3.1-70B 等超大模型上验证 36。

仅限同构模型： 无法用于 MoE 模型或异构模型融合 37。

大模型压缩难： 在14B/32B模型上，要在数学任务上显著压缩长度且不掉点比较困难（大模型更依赖冗长的CoT来推理）38。

5. 学习与应用

a) 复现关键步骤 (开源情况)

开源： 代码已在 ACM 仓库发布（文中提到 39，但PDF中未给出具体URL，通常指GitHub）。

实现步骤：

加载 Base 模型和 FT 模型。

加载校准数据集（如 s1K 或 LIMO），进行 K-means 聚类 (K=20) 并采样 10% 数据 40404040。

Hook 模型的每一层输出，运行 Inference，保存激活张量。

计算 Base 与 FT 激活张量的互信息 (MI)。

应用公式 $\lambda = 1 - \dots$ 计算系数。

加权融合权重并保存新模型。

b) 实施建议

超参数 $t$： 建议设置为 0.7，实验表明在 [0.5, 0.9] 范围内都很鲁棒 41414141。

数据量： 校准数据不需要多，100条 左右即可达到稳定效果，过多数据不会显著提升但会增加计算时间 42。

基线结合： 建议将 ACM 结合 TIES-Merging 使用 (ACM-TIES)，效果通常优于结合 Task Arithmetic (ACM-TA)。

c) 迁移能力

该方法不局限于 L2S 任务，可迁移到任何同构模型融合场景（如将数学模型、代码模型、聊天模型融合）。文中在 LLaMA 系列的通用任务（HellaSwag等）上也验证了有效性 43。

6. 总结

a) 核心思想

利用激活值互信息量化层级冗余度，动态降低高相似层的权重以保留模型特异性能力。

b) 速记版 Pipeline

采样： 准备小规模校准数据（聚类+采样）。

激活： 输入数据，获取 Base 和 FT 的每层激活值。

计算： 算激活值的互信息，互信息越高，融合权重越低。

融合： 用算出的权重对各层参数进行加权合并。