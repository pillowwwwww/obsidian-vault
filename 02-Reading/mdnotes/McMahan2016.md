# 🔤 从分散数据中对深网的沟通有效学习 (2016, McMahan) ()

**原名：**Communication-Efficient Learning of Deep Networks from Decentralized Data

**译名：** 从分散数据中对深网的沟通有效学习

**作者：**McMahan et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/ARXIV.1602.05629](https://doi.org/10.48550/ARXIV.1602.05629)

**发表时间：**2016

**本地链接:** [@Communication-Efficient Learning of Deep Networks from Decentralized Data.md](zotero://open-pdf/0_X7LVARNS)

**摘要翻译：** _现代移动设备可以访问适合学习模型的大量数据，进而可以大大改善设备上的用户体验。例如，语言模型可以改善语音识别和文本输入，并且图像模型可以自动选择好照片。但是，这些丰富的数据通常对隐私敏感，数量较大或两者兼而有之，这可能排除记录到数据中心并使用常规方法训练那里。我们主张一种将培训数据留在移动设备上的替代方案，并通过汇总本地计算的更新来学习共享模型。我们认为这种分散的方法联合学习。我们提出了一种基于迭代模型平均的联邦学习深度网络学习的实用方法，并考虑了五个不同的模型架构和四个数据集，并进行了广泛的经验评估。这些实验证明了该方法对这种设置的定义特征的不平衡和非IID数据分布是鲁棒的。沟通成本是主要限制，与同步随机梯度下降相比，我们显示所需的通信回合减少了10-100倍。_

## 💡创新点

### (): 从分散数据中对深网的沟通有效学习；2016

#### 🌏研究背景：

### ✅ 一、理想的联邦学习问题应具备以下特征：

1. **在移动设备上训练比在数据中心用代理数据训练更有优势**  
    👉 因为真实用户的数据比“公共数据集”更贴合实际任务，比如你手机上的输入行为，比Wikipedia的语料更适合训练输入法。
2. **数据具有隐私敏感性或体量太大，不适合上传到数据中心**  
    👉 比如用户照片、聊天记录、密码等隐私数据，或者用户手机中数量庞大的图像，不应为了训练就全部上传服务器。
3. **监督学习任务中，标签可以自然地从用户交互中获得**  
    👉 例如：
    
    - 用户输入的文字天然就是语言模型的训练标签；
    - 哪些照片被多次查看或分享，可以作为图像的“重要性标签”。

### ✅ 二、两个非常典型且适合联邦学习的任务：

#### 🖼 图像分类

- 例子：预测哪些照片会被多次查看或分享。
- 用户数据特点：
    
    - 包含大量个人照片；
    - 与Flickr这类开源数据集差异大；
    - 训练标签可从用户行为（删除/查看/分享）中获得。

#### 💬 语言建模（如语音识别、输入法）

- 用例：
    
    - 提升键盘输入的下一个词预测、整句回复等智能功能；
- 数据来源：用户平时输入的文字、语音，包括信息、网址、密码等敏感内容；
- 数据特性：
    
    - 隐私性极强；
    - 表达方式与公开语料库（如Wikipedia）差别大；
    - 输入内容本身就是语言模型的“标签”。

#### 💧新名词：

### 📌 为什么一个样本可以写成 (x_i, y_i)？

在机器学习中：

- $x_i$：表示**输入特征**（feature vector）；
- $y_i$：表示**输出标签**（label）；
- $(x_i, y_i)$：表示第 $i$ 个训练样本，由一个输入和一个对应的输出组成。

尽管我们用一个符号 $x_i$ 来表示，但 **它本质上是一个向量**，可以包含多个特征。

### 二、符号回顾

![[image-16.png]]
- **函数图像是“碗”形的，局部最小值就是全局最小值**。
- 神经网络损失（Neural Network） – 非凸函数
    
    - 图像上下起伏，有多个极小值和极大值；
    - 特点：容易陷入局部最小值，训练结果不稳定

异步SGD:

- 异步 SGD（如 Google 的 Parameter Server 架构，Dean et al., [12]）在大规模训练神经网络中非常流行；
- 但在联邦学习中，它要求客户端**频繁通信**（每步都同步参数或梯度）；
- 对于通信非常昂贵的联邦学习环境，这种方法**代价过高，不现实**。
    

#### 重点：

**一、同步**
1. 通过增加每个客户端的计算，让每个客户端在每个通信回合之间执行更复杂的计算，而不是执行简单的计算。可以减少通信成本和时间。

在 **FedSGD（FederatedSGD）** 中，“同步（synchronous）”的含义就是：
> 服务器必须等待所有本轮选中的客户端都完成训练并上传结果，才会进行全局更新并进入下一轮。

**二、FedAvg 成功的一个关键原因** —— **客户端模型拥有共同的初始参数**。
 为什么非凸网络中“参数平均”有时失败、有时有效？
❌ 情况一：不同初始化 ⇒ 平均失败
- 两个模型 w 和 w'：
  - 从不同随机点出发；
  - 在不同数据子集上训练；
- 平均后可能掉入高损失区域，效果非常差。
---
✅ 情况二：相同初始化 ⇒ 平均有效
- 两个模型 w 和 w'：
  - 从相同随机点出发；
  - 训练后走向相近区域；
- 参数平均结果仍处于低损失区域，效果良好。
---
 ✅ 对 FedAvg 的启示：
- FedAvg 每轮所有客户端都从相同的 w_t 出发；
- 所以即使训练数据不同，本地更新模型也不会相差太远；
- 最终参数聚合后的模型往往处于合理区域，训练效果好。

#### 🔬实验方法：

#### 📜 总结：