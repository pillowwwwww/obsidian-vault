# 🔤 从分散数据中对深网的沟通有效学习 (2016, McMahan) ()

**原名：**Communication-Efficient Learning of Deep Networks from Decentralized Data

**译名：** 从分散数据中对深网的沟通有效学习

**作者：**McMahan et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/ARXIV.1602.05629](https://doi.org/10.48550/ARXIV.1602.05629)

**发表时间：**2016

**本地链接:** [@Communication-Efficient Learning of Deep Networks from Decentralized Data.md](zotero://open-pdf/0_X7LVARNS)

**摘要翻译：** _现代移动设备可以访问适合学习模型的大量数据，进而可以大大改善设备上的用户体验。例如，语言模型可以改善语音识别和文本输入，并且图像模型可以自动选择好照片。但是，这些丰富的数据通常对隐私敏感，数量较大或两者兼而有之，这可能排除记录到数据中心并使用常规方法训练那里。我们主张一种将培训数据留在移动设备上的替代方案，并通过汇总本地计算的更新来学习共享模型。我们认为这种分散的方法联合学习。我们提出了一种基于迭代模型平均的联邦学习深度网络学习的实用方法，并考虑了五个不同的模型架构和四个数据集，并进行了广泛的经验评估。这些实验证明了该方法对这种设置的定义特征的不平衡和非IID数据分布是鲁棒的。沟通成本是主要限制，与同步随机梯度下降相比，我们显示所需的通信回合减少了10-100倍。_

## 💡创新点

### (): 从分散数据中对深网的沟通有效学习；2016

#### 🌏研究背景：

### ✅ 一、理想的联邦学习问题应具备以下特征：

1. **在移动设备上训练比在数据中心用代理数据训练更有优势**  
    👉 因为真实用户的数据比“公共数据集”更贴合实际任务，比如你手机上的输入行为，比Wikipedia的语料更适合训练输入法。
2. **数据具有隐私敏感性或体量太大，不适合上传到数据中心**  
    👉 比如用户照片、聊天记录、密码等隐私数据，或者用户手机中数量庞大的图像，不应为了训练就全部上传服务器。
3. **监督学习任务中，标签可以自然地从用户交互中获得**  
    👉 例如：
    
    - 用户输入的文字天然就是语言模型的训练标签；
    - 哪些照片被多次查看或分享，可以作为图像的“重要性标签”。

### ✅ 二、两个非常典型且适合联邦学习的任务：

#### 🖼 图像分类

- 例子：预测哪些照片会被多次查看或分享。
- 用户数据特点：
    
    - 包含大量个人照片；
    - 与Flickr这类开源数据集差异大；
    - 训练标签可从用户行为（删除/查看/分享）中获得。

#### 💬 语言建模（如语音识别、输入法）

- 用例：
    
    - 提升键盘输入的下一个词预测、整句回复等智能功能；
- 数据来源：用户平时输入的文字、语音，包括信息、网址、密码等敏感内容；
- 数据特性：
    
    - 隐私性极强；
    - 表达方式与公开语料库（如Wikipedia）差别大；
    - 输入内容本身就是语言模型的“标签”。

#### 💧新名词：

#### 重点：

#### 🔬实验方法：

#### 📜 总结：