# 🔤 KDD2023: FedMultimodal：多模态联邦学习基准 (2023, Feng) ()

**原名：**FedMultimodal: A benchmark for multimodal federated learning

**译名：** KDD2023: FedMultimodal：多模态联邦学习基准

**作者：**Feng et al.

**期刊：**

**IFQ：**

**DOI：** [10.1145/3580305.3599825](https://doi.org/10.1145/3580305.3599825)

**发表时间：2023-08-06

**本地链接:** [KDD '23 The 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023-FedMultimodal A benchmark for multimodal federated learning.pdf](zotero://open-pdf/0_FQSM88VV)

**摘要：**

这是一个benchmark论文，规定了**数据、划分、特征、模型、融合、优化器、噪声仿真、指标协议，**整体打包成“标准化可复现”的统一出题与阅卷规则。

## 做了什么（“同一张试卷”）

- **选题面广**：给出覆盖 5 类应用（情感识别、动作识别、人体行为、医疗、社媒）的**10 个数据集**，统一打包。
- **统一协议 + 可复现代码**：开源**端到端的 FL 仿真框架**，包含三件基础设施：**数据划分、特征处理、联邦训练**；内置多种优化器（FedAvg/FedProx/FedRS/SCAFFOLD/FedOpt），并允许你**自定义训练器**。
- **真实客户端划分**：能用“自然 ID”就用（说话人/受试者/临床中心），没法自然划分的任务用**Dirichlet α** 控制非独立同分布强度（α=0.1/5.0）。这让**non-IID 的来源透明可控**。
- **端侧友好的特征管线**：统一选用**轻量骨干**（视觉 MobileNetV2/MobileViT，文本 MobileBERT/DistilBERT，音频 MFCC，其它模态用 Raw），明确这是因为**边缘设备装不下/跑不动大模型**。
- **鲁棒性模块（这篇的独特点）**：系统地模拟**三类现实噪声**——**模态缺失**、**标签缺失**、**错标签**——并给出统一实验与结论。作者强调这是相对既有工作**独有的补位**。

> 一句话：他们提供的是**可复制的赛道**，而不是“新车”。赛道上包括统一路况（划分/特征/训练/指标）+ 真实坏天气（噪声模块）。

## 🌏研究背景：

### 问题 & 挑战（Challenge）

这篇文章聚焦于**“为什么多模态 FL 难以形成可比共识”**与**“真实部署中的噪声因素如何系统评测”**两类挑战：

1. **缺统一基准 → 结论不可比**  
    既有多模态 FL 工作（如 MMFed、FedMSplit、CreamFL 等）各自定义实验设置，难以在相同数据与协议下公平比较。因此社区缺少**“一张共同的试卷”**来衡量算法优劣。KDD '23 The 29th ACM SIGKDD Con…
2. **真实世界噪声广泛存在 → 模型鲁棒性不明**  
    端侧传感器与标注都会出问题：

- **缺失模态（Missing modalities）**：摄像头/麦克风/电极断连或损坏；
- **缺失标签（Missing labels）**：端侧很难保证每条数据都有标注；
- **错误标签（Erroneous labels）**：主观性强的任务易产生错标。  
    但过去多模态 FL 的鲁棒性评测非常有限，缺少**系统化模拟与量化**。

3. **端侧算力受限 → 设计可部署的多模态模型更难**  
    边缘设备在算力/存储/电量/通信上都受限，庞大模型（如大规模 Transformer）难以在联邦端侧反向传播，迫使研究者在**精度-效率**之间权衡。

> 因果链条（Challenge 的“因为…所以…”）：**因为**没有统一数据/协议、且真实噪声缺少可复现实验设计、端侧约束又严，**所以**多模态 FL 很难形成可比的 SOTA 结论，也难知道算法在真实部署下是否可靠。

Table1对比了目前比较火的FL研究，研究显示，**模态缺失（missing modalities）**、**标签缺失（missing labels）**、**标签错误/噪声（erroneous labels）**。这三类，没有任何一个已经存在的FL研究把他们都概括到。而这篇文章可以！

## 🌟重点：

一个训练的例子：
  
下面用一个**完整的小故事**把这些概念串起来：**MobileViT（骨干）→ embedding（特征向量）→ 时序建模（RNN/注意力）→ 客户端训练 → 服务器聚合 → “Features”这一栏到底指什么**。

场景：做 MiT10 视频动作识别（MAR，视频+音频）

0) 数据如何切“同一张卷子”

MiT10 是从 Moments in Time 裁出的 10 类子集。为适配端侧算力，**视频按“每 10 帧取 1 帧”下采样**；UCF101 则按 **1Hz 抽帧**（同一原则）。这就是论文给的统一“出题范围”。

1) “预处理 / 骨干（backbone）”：把原始帧变成**embedding**

- **在每个客户端设备**上，原始视频帧逐帧送入**MobileViT**（一个**轻量视觉骨干**），输出一串帧级**embedding**（潜表示）。MobileViT 只有 **2.7M 参数**（MobileNetV2 4.3M），是论文专门强调的**端侧友好**特征提取器。
- 同时，音频流提**MFCC**（Mel 倒谱系数）作为声学特征，同样在**客户端**算出来。

> 这一步就是**feature processing**：先用**预训练的轻量骨干**把“原始模态 → 特征向量（embedding 序列）”，而不是端到端用大模型“生算”。这是 FedMultimodal 的明确设计（考虑端侧算力/存储/电量）。

2) “时序/序列建模”：把**一串**帧/音频特征卷起来理解

- **视频路**：骨干已经产出**逐帧 embedding 序列**，接下来进入**RNN-only 编码器**（因为不是传感器那类信号）。RNN 读入“t=1…T”的帧级表示，获得**视频的时序表示**。
- **音频路**：对 MFCC 序列用**Conv+RNN 编码器**（音频/加速度/陀螺/ECG 这一类，论文用 Conv+RNN）。
- **融合（两条模态合到一起）**：
    
    - **拼接式**：先对 GRU 的时间输出做**平均池化**，再拼接成**多模态 embedding**；
    - **注意力式**：**不做池化**，直接把两路**时间输出拼接**，用**轻量注意力**学权重，得到最终**多模态 embedding**（还能天然掩蔽缺失模态）。

> 这里“**embedding**”在两层含义上出现：  
> ① **骨干输出的帧级/片段级 embedding**（MobileViT/MFCC 产物）；  
> ② **融合前后的多模态 embedding**（RNN/注意力聚合的结果）。两者不是一回事，但都是“表示”。

3) 客户端（Client）如何训练

- 客户端本地拿自己的视频+音频，先**提特征（backbone）**，再**过编码器与融合模块**，计算分类损失。
- 按论文的统一设定训练：例如 **本地 epoch=1、batch=16、总轮数=200（MiT10/51 为 300）**，学习率等见统一表（Table 3）。

> **重要**：客户端**只上传模型参数更新**或梯度近似，**不上传原始数据和特征**（embedding 也不发）。这就是 FL 的隐私优势。

---

4) 服务器端（Server）如何“阅卷”

- 服务器收集若干被抽样客户端的**本地更新**，用 **FedAvg / FedProx / SCAFFOLD / FedOpt** 等进行聚合迭代，形成新的全局模型，再下发给下一批客户端。
- 在“同一张卷子”（同数据、同特征管线、同训练协议、同指标）的前提下，表 4 会给出**Concat vs Attention × 不同优化器**的横向可比分数。

5) “Features” 一栏到底指什么（与骨干/预处理的关系）

- 表 2 里的 **Features**，就是**每个模态用什么“特征管线/骨干”**：
    
    - 视觉：**MobileNetV2 或 MobileViT**；
    - 文本：**MobileBERT / DistilBERT**；
    - 音频：**MFCC**；
    - 传感器/ECG：**Raw**（无骨干，直接进编码器）。  
        这正是上面第 (1) 步“预处理/骨干”的选择清单。

---

### 一句话把关系捋顺

- **MobileViT（骨干）**：在**客户端**把**原始帧 → 帧级 embedding 序列**（“Features=MobileViT”就指用了它）。
- **embedding**：既包括**骨干产出的帧/片段级表示**，也包括**时序与融合后得到的多模态表示**。
- **时序建模**：RNN/Conv+RNN/注意力对**时间维的 embedding 序列**进行编码与聚合。
- **预处理/Features**：论文规定的**轻量特征管线**（MobileViT/MobileNetV2、MobileBERT、MFCC、Raw），全部**在客户端**执行。
- **客户端 ↔ 服务器**：客户端本地训练→只发**参数更新**；服务器用 **FedAvg/FedOpt…** 聚合回全局模型。

如果你愿意，我可以把这个 MiT10 的**最小可复现实验脚本/清单**列成“照抄即跑”的步骤（数据下载→抽帧→MobileViT 抽特征→RNN/注意力设置→FedOpt/FedAvg 选择→评测）。

## 🔬实验方法：

1. **统一实验设置**：给出端侧友好的模型（Conv+RNN 或 RNN-only → 融合 → 2层全连接）、固定 batch=16、本地 epoch=1、训练轮数（MiT10/51 为 300，其余 200）与关键超参表（Table 3）。目的是让不同数据集/方法在同一协议下可比。  

2. **两类融合基线对比**：

  * 拼接式（先对各模态 GRU 输出做平均池化再拼接）；

  * 轻量注意力池化$$(u=\tanh(Wh+b)), (a=\text{softmax}(u^\top c)), (v=\sum a_i h_i)$$在 **Table 4** 上与四种 FL 优化器（FedAvg/FedProx/FedRS/**FedOpt**）全面横评；总体上注意力融合在**高异质性**更优，低异质性（(\alpha=5.0)）有时拼接更稳；FedOpt 多数更强但**对服务器学习率敏感**。  

3. **单模态 vs 多模态**：在自然划分或高异质性划分上比较（Fig.3）。结论：**单模态很有竞争力**，多数数据集多模态领先幅度**≤5%**，但总体仍略胜。  

4. **三类“真实噪声/缺陷”的鲁棒性评测**：

  * **缺模态**（每模态独立伯努利缺失，(q\in{0.1,…,0.5})；用注意力融合+mask 训练，Fig.4）：<30% 影响不大，50% 明显下降；**约一半模型在 50% 时跌幅仍 <10%**。

  * **缺标签**（伯努利缺标，Fig.5）：总体影响小于缺模态。

  * **错标签**（基于转移矩阵 (Q) 生成，Fig.6）：危害最大，30% 时不少数据集跌幅>10%。

5. **任务难度画像**：HAR 最容易；社媒类（Hateful Memes/CrisisMMD）在 FL 下最难（CrisisMMD 最优 <30%）；MiT 难，但**降类到 MiT10**显著简化任务。

所以，第4章不是“只写两种融合+模态缺失”。它是：**统一协议 → 基线融合×优化器横评 → 单/多模态对照 → 三种现实噪声鲁棒性 → 任务难度解读**，共同构成这张“同卷同尺”的联邦多模态基准。

## 📜 总结：
**FedOpt 很强，但“强在可调”**：它在服务器端引入了自适应更新，而$$\eta_{\text{server}}​$$是决定成败的关键旋钮。**不调=经常不稳或不优**；**调好了=普遍优于 FedAvg**（这正是论文想提醒你的）