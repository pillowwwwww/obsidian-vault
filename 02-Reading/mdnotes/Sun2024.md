# 🔤 FFA-LoRA: 改进 LoRA 在隐私保护联邦学习中的应用 (2024, Sun) ()

**原名：**Improving LoRA in privacy-preserving federated learning

**译名：** FFA-LoRA: 改进 LoRA 在隐私保护联邦学习中的应用

**作者：**Sun et al.

**期刊：**

**IFQ：**

**DOI：** [10.48550/arXiv.2403.12313](https://doi.org/10.48550/arXiv.2403.12313)

**发表时间：**2024-03-18

**本地链接:** [2024-Improving LoRA in privacy-preserving federated learning.pdf](zotero://open-pdf/0_8I96IB6S)

---

## 0) 摘要翻译（意译，保留技术点）

作者指出：LoRA 在集中式微调里很成功，但一放到 **PPFL**（通常要用 **DP-SGD** 做更强隐私保证）就会出现明显性能问题。根因不是“LoRA不行”，而是 **LoRA 的参数分解形式**与**联邦聚合**、**DP噪声**、以及 **LoRA缩放因子 α（scaling factor）**之间存在结构性冲突。为此作者提出 **FFA-LoRA**：把 LoRA 的一个低秩矩阵（A）固定不训练，只训练另一个矩阵（B），从而同时改善：联邦聚合误差、DP 噪声放大、以及 α 的敏感性；并且训练/通信量还能减半。

---

## 1) 方法动机

### a) 为什么要提出？

目标很明确：**让 LoRA 在“本地DP + 联邦聚合”的训练范式里别崩**。作者关心的不是“更高的集中式精度”，而是 **PPFL 下 LoRA 的不稳定与掉点**。

### b) 现有方法痛点在哪里？

作者强调了 LoRA 在 PPFL 里至少三类“结构性不适配”（discordance）：

1. **聚合不一致误差（aggregation mismatch）**  
    两客户端本地学到的是 (B_1A_1, B_2A_2)，理想全局应是平均：(\frac12(B_1A_1+B_2A_2))。  
    但服务器常见做法是分别平均 (A,B) 再相乘，得到的会多出一个 **mismatch项** (\frac14(B_1-B_2)(A_1-A_2))，异质性越强越糟。
2. **DP噪声会被 LoRA 的“乘法结构”放大**  
    LoRA 同时训练 A、B 时，DP-SGD 往梯度里加的噪声进入到 A、B 两边，最终 (\Delta W=BA) 会出现**乘出来的噪声/交叉项**，方差更难控。作者用合成实验展示：当噪声项的 Frobenius norm 足够大（文中举例阈值 0.5），更新会明显偏离无噪声轨迹。
3. **缩放因子 α 很敏感**  
    LoRA 里常用 (\alpha/r) 缩放，但在 PPFL 中 α 会显著影响收敛与“平滑性”（smoothness），调参成本高、且和学习率强耦合。

### c) 核心直觉/假设

**把 LoRA 从“两个矩阵一起动的乘法系统”改成“只动一个矩阵的线性系统”**：  
冻结 A 后，联邦聚合变成对 B 的线性平均；DP 噪声也更像“加法噪声”，不再被 A×B 交叉放大；α 的麻烦基本被绕开。

---

## 2) 方法设计（重点：流程细节）

### 2a) 逐步 pipeline（输入→处理→输出，按你复现会写的顺序）

**输入：** 预训练模型权重 (W_0)（论文主要用 RoBERTa-Large），以及要插 LoRA 的线性层集合。

**Step 1：把每个目标线性层参数化为 LoRA 形式**  
标准 LoRA：  
[  
W = W_0 + \Delta W,\quad \Delta W = (\alpha/r), B A  
]  
初始化：A 高斯随机，B 全零（这是 LoRA 常见做法）。

**Step 2：把 LoRA 改成 FFA-LoRA（Fixed-A Fine-tuning B）**

- **冻结 A：** 设 (A \leftarrow A_0)，训练全过程不更新 A
- **只训练 B：** 只有 B 参与反向传播与优化  
    作者强调这样做后：**训练/通信参数量直接减半**。

**Step 3：客户端本地训练（带 DP-SGD）**  
每轮联邦训练：客户端在本地数据上对 **B 做 DP-SGD**：

- per-sample 梯度裁剪（clipping threshold，裁剪阈值）
- 加高斯噪声（noise multiplier/noise scale，噪声尺度 σ）
- 只更新 B（A 不动）  
    DP 预算 ((\epsilon,\delta)) 用 moments accountant / Opacus 计算。

**Step 4：上传与服务器聚合（FedAvg）**  
客户端上传 **B（或ΔB）**，服务器做平均：  
[  
\bar B \leftarrow \frac{1}{n}\sum_i B_i  
]  
然后全局层权重等价于  
[  
W = W_0 + (\alpha/r), \bar B A_0  
]  
关键点：这里不再出现 ((B_1-B_2)(A_1-A_2)) 的 mismatch，因为 **A 是公共常量**。这就是它“联邦友好”的根。

**输出：** 一个能在 PPFL+DP 下稳定训练的 LoRA 适配器（实际可视为全局的 B，配固定 A0）。

---

### 2b) 结构/模块协同（你读论文时该盯的模块）

- **LoRA模块**：给 Transformer 的线性层加“低秩旁路”
- **冻结策略（Fixed-A）**：把 LoRA 的乘法结构从“双变量”降成“单变量”
- **DP-SGD机制**：噪声只进入 B 的更新，避免在 A、B 两边同时注入再相乘带来的噪声放大风险（作者的主要论点之一）

### 2c) 公式/算法的直白解释

- **为什么 mismatch 项致命？**  
    因为联邦平均本质是“对参数做线性组合”。而 LoRA 的有效更新是 (BA)（乘法），把乘法拆开分别平均会额外产生交叉项；这在异质数据下必然变大。
- **为什么冻结 A 能修？**  
    因为 (A_0) 固定后，(\Delta W) 对 B 变成线性：(\Delta W \propto B)。线性对象最适合 FedAvg。
- **α 怎么看？**  
    作者给了定理：当 LoRA 的 α→∞ 时，LoRA 轨迹会逼近 FFA-LoRA 的轨迹（极限意义下等价）。  
    并且指出：**FFA-LoRA不依赖 α**，但 vanilla LoRA 依赖 α（和初始化/学习率耦合）。

---

## 3) 与其他方法对比（本质差异/创新点/适用场景）

### a) 本质不同

- LoRA：训练 A、B 两套参数，更新是“乘法系统”
- FFA-LoRA：固定 A，只训练 B，把系统降成“线性系统”，让 FedAvg/DP-SGD 更匹配

### b) 创新点（我直接按贡献度排序）

1. **指出并形式化 LoRA×FedAvg 的 mismatch 误差项**（这点很关键，不然你很容易把掉点归咎于“DP太强/数据太非IID”）
2. **FFA-LoRA：冻结 A 改结构，直接消掉 mismatch，并缓解 DP 噪声放大**
3. **α 敏感性分析 + 定理连接 α 与轨迹极限**（告诉你：别再迷信“α要精调”，至少对 FFA-LoRA 没必要）

### c) 更适用的场景

- **必须上 DP（本地DP-SGD）** 的联邦训练
- **强异质（non-IID）**、以及你不能做大规模超参搜索的场景
- **低通信/低可训练参数预算**（FFA 直接少一半参数）

### d) 方法对比表（优缺点/改进点）

|方法|训练参数|FedAvg聚合一致性|DP噪声行为|超参敏感性|主要缺点|
|---|---|---|---|---|---|
|全量微调 FFT (full fine-tuning)|最大|线性聚合OK|DP噪声不乘法放大，但成本极高|相对好调|训练/通信/隐私代价最大|
|LoRA|中|**会产生 mismatch 项**|A、B 两边噪声再相乘，可能放大|**对 α/LR 很敏感**|PPFL 下易不稳/掉点|
|**FFA-LoRA**|**LoRA的一半**|**线性聚合（对B平均）**|噪声更像加法进入 B|**α 不关键/更稳**|表达能力可能受限（但实验显示多数任务不吃亏）|

---

## 4) 实验设计与数据（重点拆解）

### 4a) 作者怎么验证？

实验分成四层递进：

1. **语言分类（GLUE子任务）**：对比 LoRA vs FFA-LoRA，在不同隐私预算 ε 下的精度变化（核心主线）。
2. **异质性强弱（iid / mild / severe）**：验证 mismatch 随异质性变强而恶化。
3. **参数预算（rank r）**：同一任务下比较不同 r；并强调要按“参数预算”公平对齐（FFA r=16 ≈ LoRA r=8 的可训练参数量）。
4. **扩展到 LLaMA-7B 生成 & ViT 视觉分类**：证明不是“只对小文本分类有效”。

实验配置也写得很具体：RoBERTa-Large（355M），3个客户端 cross-silo；每轮 10 local steps，1000 rounds，batch size 200；非IID通过标签比例控制（mild: [0.3,0.7]，severe: [0.1,0.9]）。

---

### 4b) 关键结果（挑最能说明机制的数字）

#### (1) 隐私预算越强，LoRA 越容易崩；FFA 更稳（Table 1）

例子：在 **ε=1（强隐私）** 时，RTE 上 LoRA 45.6，而 FFA-LoRA 52.0；MRPC 上 LoRA 73.7，FFA-LoRA 79.2。

#### (2) 异质性越强，mismatch 越明显；FFA 提升更大（Table 2）

MNLI(matched)：

- iid：LoRA 83.62 vs FFA 83.76（几乎一样）
- severe：LoRA 75.61 vs FFA 83.07（差 7.46 个点）  
    这组结果基本在“验你前面读的理论”：mismatch 随 non-IID 放大。

#### (3) 参数预算（rank r）下，FFA 多数任务更占优，但不是“全线碾压”（Table 3）

SST-2：LoRA r=16 为 93.98，而 FFA r=16 为 95.30。  
但 MNLI 上 FFA r=16 反而低于 LoRA r=16（85.82 vs 87.43）。  
这点你要记住：**FFA 的收益不是来自“更强表达力”，而是来自“更匹配PPFL训练动力学”**。

#### (4) “隐私×小rank”是 LoRA 的死亡组合；FFA 直接救回来（Table 4）

QNLI，ε=1：

- LoRA r=4 / r=2 直接掉到 **58.30 / 58.15**
- FFA-LoRA r=4 / r=2 仍有 **82.06 / 82.64**  
    这组是全文最有杀伤力的数据：它说明 LoRA 在强DP下可能出现“训练基本失效”的相变，而 FFA 避开了。

#### (5) α 敏感性（Table 7）

当你按“学习率随 α 缩放”的规则走：

- FFA-LoRA 在 α=2~256 基本稳定（91.31~92.46）
- LoRA 会在 α=64/256 崩到 50 左右（50.96/49.46）  
    但作者也很诚实：LoRA 不是“做不到”，而是**要重新网格搜最优学习率**，成本很高。

---

### 4c) 哪些场景优势最明显？

一句话：**强隐私（小 ε）+ 强异质（severe non-IID）+ 小参数预算（小 r）**。证据就是 Table 2（severe 非IID差距最大）与 Table 4（ε=1 小r直接救活）。

### 4d) 局限性（你别自我安慰，我直接挑刺）

1. **非隐私场景提升并不稳定**：Table 4 的 non-private 下，LoRA/FFA 很接近，甚至 FFA 在 r=2 会明显掉点（88.56）。
2. **客户端规模小（3 clients）**：异质性是做出来了，但“跨设备大规模FL”的统计与系统问题没覆盖。
3. **冻结 A 的表达力上限**：论文结果显示多数任务不吃亏，但这不等于你换到 CLIP/多模态/更大域偏移也成立——你要准备好“FFA 稳但上限被卡”的可能性。

---

## 5) 学习与应用（复现/实现建议）

### 5a) 是否开源？

我在文中没搜到明确的 code/GitHub 发布声明（至少在PDF正文/附录里未出现）。

### 5b) 复现关键步骤（最容易踩坑的点）

- **A 必须在所有客户端一致且固定**：最省通信的做法是共享随机种子生成 A0；或者服务器下发一次 A0。
- **B 初始化为 0**（否则你会把“适配器初始扰动”混进 DP 噪声里，解释不清）。
- **DP-SGD 只作用在 B 的梯度上**：在实现里务必 `requires_grad=False` 给 A。
- **α 别乱调**：FFA 里 α 不是关键，但如果你还按 LoRA 的习惯把 α 当“主要旋钮”，就是在浪费时间。
- **学习率要跟 α 有规则地缩放**：作者的经验是 FFA 用 LR scaling 稳，LoRA 则经常需要重新搜最优 LR。

### 5c) 能否迁移到其他任务？怎么迁移？

可以迁移，但别想当然：

- **迁移原则**：只要你的 PEFT 结构在联邦端需要“线性可平均”，就应该优先考虑“冻结一部分、只训练另一部分”。这是 FFA 的本质。
- 作者自己也做了跨任务验证：LLaMA-7B 的 GSM-8K 上 FFA 17.12% > LoRA 15.68%。
- ViT/ Food-101：集中式差不多，但联邦 iid 下 FFA 82.10% > LoRA 81.33%。

---

## 6) 总结

### 6a) 20字内核心思想

**固定A，只训练B做联邦聚合**

### 6b) 速记版 pipeline（不写论文术语，直接说人话）

1. 给关键层加一条“低秩旁路”
2. 随机生成一块固定矩阵并锁死
3. 只训练另一块小矩阵（本地可加DP噪声）
4. 每轮只上传小矩阵，服务器做平均再下发
5. 推理时把两块矩阵相乘，作为原权重的增量

---

如果你接下来要把它迁移到你关心的 **CLIP/多模态联邦**：我会先质疑你一个点——**你是不是默认“冻结文本侧/视觉侧的哪一边都一样”？** 在多模态里这很可能不成立（模态稳定性不同），你最好把“冻结哪一侧的LoRA矩阵”也当成可控变量做消融，而不是照搬固定 A。