---
date: 2025-11-17 10:50:50
tags: dailynote
rating: ⭐
excerpt: 
---

## TODO

- [ ] 调研merging
- [ ] 多喝水，避免久坐不动
- [ ] 阅读一篇文献

## Tracking

把modal merging调研
初步想法是针对不同的任务设计adapter，进行MOA。
modal merging：不同的任务给不同的adapter

出发之前想好讲什么问题，有什么问题，讲故事解决问
题。

调研一下modal merging（centralized中比较火

通过merging的方式。
师兄目前正在做一个align：

看一篇论文好不好：图好不好，故事，方法，

lora微调：很火的 能做

抨击现在的pre-train模型到处爬，爬出来的数据并不准确（都是一些搜索式数据）也没有针对性，依靠后期微调耗费精力，那我为什么不直接从数据端就给更高质量/贴合任务的数据？也就是联邦学习

---
问了一下gpt：
### 核心结论
- 你关注的「FL中为不同任务/模态配不同adapter，再用MoA/MoE做路由/融合」，在centralized场景是近两年的热点方向，相关工作集中在Mixture-of-Adapters、adapter fusion、model/modal merging领域。
- 该方向的联邦场景延伸仍属小众，核心价值在于明确FL特有的约束（隐私、通信、非IID）对centralized MoA/merging方法的适配与创新。

---

## 0. 先统一一下几个术语（方便你跟师兄沟通）
- **Adapter 混合 / Mixture-of-Adapters（MoA）**：把多个adapter（可以是不同任务、不同领域、不同模态）当成experts，用一个router（门控网络）按输入动态加权组合。对应中文可以叫：**适配器混合** / **混合适配器**。
- **Adapter Fusion / LoRA Fusion**：已经在不同数据集 / 任务上训好一堆LoRA / adapter，再用「融合层」或直接权重合并来组合多个adapter的知识。
- **Model / Modal Merging（模型/模态合并）**：更泛的概念，可以是直接merge多个模型/LoRA的权重（weight-space merging），也可以是多模态场景里，把不同模态分支学到的东西「合并」进一个统一表示（你师兄说的*modal merging*很可能就是这层意思：跨模态/跨任务的表示合并，不一定是严格的weight-space merge）。

你的idea = 在 **联邦** 场景里，做「任务/模态专属adapter + MoA/MoE +（可能）跨client/跨任务的merging」。这跟现在centralized那条 **Mixture-of-Adapters / Adapter Fusion / Model Merging** 主线是同一个家族，只是加了联邦的约束。

---

## 1. 近两年（2024–2025）centralized 下的 Mixture-of-Adapters 方向
### 1.1 核心：用 MoA 做多任务 / 多域 / 多模态的 PEFT
**(1) Domain Generalization using Large Pretrained Models with Mixture-of-Adapters**
- **ICLR 2024 + WACV 2025**（同一工作）
- 设定：多域图像分类 / Domain Generalization，用大模型（ViT）+ PEFT。
- 做法：每个domain学一个adapter，用 **Mixture-of-Adapters (MoA)** 路由，把不同domain adapter组合起来。
- 结论：MoA + 大backbone在DG上能跑到SOTA。
- 启发你：这是标准centralized MoA：*单服务器 + 多domain + MoA router*。你在FL里可以类比：「每个client/domain一套adapter，服务器学一个跨client的MoA / router」。

**(2) Generalizability of Mixture of Domain-Specific Adapters**（ACL 2024）
- 不是新方法，而是**系统地分析MoA的可泛化性**（robustness、对抗攻击、剪枝等）。
- 观点：混多个domain adapter看起来很香，但混在一起之后是不是还在in-domain上靠谱？会不会在对抗样本上更脆弱？
- 启发你：以后你在FL里搞跨任务MoA，可以引用这篇说：「已有工作发现mixture-of-adapters的generalization本身就有很多open question，而在非IID的FL里更复杂。」

**(3) DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person Retrieval**
- **AAAI 2025**
- 任务：基于CLIP的 **text-based person retrieval**（文字搜人）。
- 做法：在CLIP上插入一组 **vision-language Sparse Mixture-of-Adapters (SMA)**，用Top-K router选择少量adapter参与前向，目标是同时抓住不同领域/摄像头条件下的细粒度特征。
- 本质：**稀疏MoA + vision-language + domain-aware**。
- 对你的启发：这是「CLIP + MoA」的centralized版本，和Pilot、你想做的FL-MoA-CLIP非常接近。

**(4) Sparser Mixture-of-Adapters with Cross-Layer Generalization**
- **NAACL 2025 long paper**
- LLM上的MoA：用 **更稀疏的MoA**，并提出cross-layer generalization：少量adapter在多层重用，减少激活数量。
- 目标：在保证性能的前提下，极大减少推理时的active adapters数量。
- 启发你：如果你在FL里担心通信/推理开销，可以借鉴这类「sparse MoA」设计。

**(5) MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of LLMs**（2025, arXiv）
- 关键点：**异构MoA**：expert不一定都是同构的LoRA，可以是结构不同的adapter组合。
- 动机：纯LoRA-MoE容易expert collapse、负载不平衡；换成结构多样的adapter（LoRA、prefix、bottleneck等混着），让router专门学「什么时候该用哪种结构」。
- 对你：如果你将来想在FL里做「多种PEFT方式的专家池」（任务/模态/层次不同），这篇就是一个很强的centralized先行者。

---

### 1.2 多模态 & 视觉里的 MoA / modal merging
**(6) Task-Customized Mixture of Adapters for General Image Fusion (TC-MoA)**
- **CVPR 2024**
- 设定：**通用图像融合**（multi-modal / multi-exposure / multi-focus等多种fusion任务）。
- 做法：把预训练的视觉backbone固定，每个融合任务对应一组adapter，用MoA路由来「task-customized」选adapter，实现一个模型统一支持多种融合任务。
- 特别点：强调「任务之间互补但机制不同」——这和FedMIT里「Caption/VQA/Grounding盯图方式不同」很像。
- 对你：这是非常典型的「多模态任务 + MoA + centralized」。你可以说：Pilot在某种意义上就是把TC-MoA这种思路搬到联邦 + 指令微调 + MLLM里。

**(7) CLIP-MoA: Visual-Language Models with Mixture of Adapters for Multi-task Remote Sensing**（2025, remote sensing场景）
- CLIP视觉分支里插Mixture-of-Adapters，为多个遥感下游任务共享backbone。
- 模型结构：CLIP + visual MoA + router。
- 这和你/ Pilot在做的事情几乎是同一类，但数据场景不同（遥感vs通用图文）。

---

## 2. Adapter Fusion / LoRA Fusion：更偏「模型/权重合并」的一条线
这条线更接近你师兄说的*model merging / modal merging*里的「合并多个已经训好的LoRA/adapter」那种感觉。

**(8) LLM instruction fine-tuning with multiple LoRA-adapter fusion**
- **Knowledge-Based Systems 2024**
- 设定：同一个LLM在多个指令数据集上分别用LoRA微调。
- 做法：不把数据混在一起训练，而是**训练多个LoRA**，然后用多种策略 **fuse LoRA**：加权求和、层级融合等。
- 结论：「LoRA-fusion」比「直接把所有数据混一起训一个LoRA」更强，尤其在多源多任务时。
- 对你：这篇可以当成centralized版本的「多任务LoRA合并」。你未来可以在FL里做「跨client的LoRA-fusion或adapter-fusion」，对标它。

**(9) AdapterFusion / 多adapter融合在新场景里的用法**
- 典型例子：
  - **Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters**（KALLM 2024）：用ConceptNet adapter + Wikipedia adapter + **AdapterFusion** 集成不同知识源。
- NeurIPS 2024的 **Sparse High-Rank Adapters** 里专门有一节做「Multi-Adapter Fusion on LLMs」，分析多adapter场景的效率与性能。
- 还有一些把fused-adapter用在知识注入、事件推理等任务里：比如2025年的 **Injecting LLM Knowledge for Event Plausibility** 用adapter fusion来注入外部知识。

这些都说明：**“先训一堆adapter/LoRA，再在表示/权重层面做融合”** 这件事，在centralized里已经形成一小条线。

---

## 3. 「Modal / Model Merging」更宽泛的多模态合并方向
这条线不一定都是adapter，但概念上和你说的*modal merging*很像：**多模型/多模态的权重或表示合并**。

**(10) UQ-Merge: Uncertainty Guided Multimodal Large Model Merging**
- **Findings of ACL 2025**
- 核心：合并多模态大模型时，用不确定性（uncertainty）指导哪些模型/模态贡献更大。
- 结果：比纯单模态merging更好，特别是在OOD上。
- 对你：可以引用来说明「modal merging」的另外一种实现：不通过adapter，而是通过不确定性加权的模型权重合并。

**(11) Multi-modal merging in speech+text LMs**
- **NAVER Labs 在 IWSLT 2025 的系统**：在多模态自适应时有一个 *multimodal merging step*，把text-only LoRA和speech projector的LoRA权重合并，以提升语音翻译性能。

**(12) Selective Attention Merging (SA-Merge)**（2025, child ASR case study）
- 不是adapter，而是合并attention task vectors，经常会引用 *cross-modal merging techniques* 这类工作。
- 思路类似：通过对attention/表示层的「选择性merging」，在低资源任务里复用高资源模型的知识。

**(13) MM-Prompt: Cross-Modal Prompt Tuning for Continual VQA**（2025）
- 在VQA里做跨模态prompt tuning，强调「prompt表示在text/image两个模态之间的merging效果」，用t-SNE展示不同模态prompt在表示空间变得更加混合。
- 虽然是prompt而不是adapter，但精神内核还是「跨模态的可学习merging结构」。

---

## 4. 回到你的 idea：跟 centralized 热点的关系
你现在的初步想法可以概括成：
> **在联邦学习里**，
> * 针对不同任务/模态设计不同adapter（任务adapter / 模态adapter / 客户端adapter），
> * 再设计MoA或MoE来做跨任务/跨模态（甚至跨client）的 **知识融合**，
> * 目标是：既能个性化（client-specific），又能共用跨任务/跨模态的通用知识。

这个和上面这些centralized工作的关系，大致可以这样定位：
1. **和MoA/MoE系列的关系**
   - 你的MoA / CT-MoA思路，本质上是把 **Domain Generalization MoA、DM-Adapter、TC-MoA、CLIP-MoA** 这条线：「一个backbone + 一堆adapter + router/MoE」从centralized搬到了FL + MLLM + instruction tuning里（Pilot已经是一个起点）。
   - 差异点可以强调：数据在client侧，**任务和模态是「分布到各客户端」的**，路由/adapter不能直接看到所有数据；aggregation需要设计（比如Pilot的ATA、DeMA、FedMoE/FedHyMoE那一类）；privacy + 通信约束使得MoA结构本身要考虑参数/梯度传输成本。

2. **和adapter / LoRA fusion & model merging的关系**
   - centralized这条线已经证明：多adapter/LoRA分别训再融合，**比一次性训一个大LoRA更稳**（KBS 2024 LoRA-fusion）；在多模态/多知识源场景（Knowledge Graph + Wikipedia, 多模态LMs）里，**融合多个adapter是有效的**（AdapterFusion, UQ-Merge等）。
   - 你可以把FL设定看成是「**天然的多source / 多domain / 多模态**」，每个client = centralized里的一个domain / 数据源，你的工作就是：在 **不集中数据** 的前提下，设计adapter & merging机制，让这些source的知识被汇聚。

3. **FL方向其实刚刚开始接这个话题**
   - 除了你现在看的Pilot之外，已有一些FL + adapter/MoE的工作：**FedDPA**（NeurIPS 2024）：global adapter + local adapter的「双适配器」架构，做test-time personalization；**HeteroTune / DeMA（Dense Mixture of Adapters）**：为大模型在FL场景下设计Dense MoA来适应heterogeneous edge clients；**MoAFCL**：在CLIP上做mixture-of-adapters的federated continual learning。
   - 但：绝大多数还是单模态 / 单任务（或简单多任务），真正像Pilot这样做「联邦 + 多模态指令 + cross-task MoA」的还非常少。

---

## 5. 给你一点「怎么讲自己 idea」的建议
如果你后面要跟导师/师兄解释你这个方向，或者写related work，可以这样组织：
1. **先讲centralized的MoA / merging线**
   - 「Adapter混合」：ICLR/ACL/WACV/AAAI/NAACL上的MoA/DM-Adapter/SMoA等；
   - 「Adapter / LoRA融合」：LoRA-adapter fusion, AdapterFusion, Sparse High-Rank Adapters等；
   - 「多模态merging」：TC-MoA, CLIP-MoA, UQ-Merge, MM-Prompt等。

2. **再讲FL已经开始摸到这条线，但还不完善**
   - FedDPA / HeteroTune(DeMA) / MoAFCL / Pilot等，只解决了部分问题（单模态、单任务or特定场景）。

3. **最后落到你的idea：**
   - 你的贡献点可以朝几个方向强化：
     - **更细粒度的结构设计**：例如在Pilot的CT-MoA上，借鉴SMoA / DM-Adapter的 **Sparse MoA、跨层共享、Top-K路由** 之类，把centralized的设计移到FL多模态上；
     - **更强的任务/模态显式建模**：比如把任务类型、模态类型、client域信息都编码进router的输入；
     - **更严肃的理论/实验比较**：和centralized MoA / merging的对比：在非IID + 隐私约束下，MoA还能不能像centralized那样工作？和简单FedAvg / FedDPA / HeteroTune等FL方法对比：在cross-task, cross-modal setting下谁更稳。

----


先记一下与这个idea相关的一些关键词：
任务无关的对立面：任务相关/任务感知优化 task-specific
MoA
在Pilot里面，router穿的只有图像特征，我们现在吧图+文拼到一起（可以是OCR也可也是图像外面拼一圈文本+任务符号编码，VQA是1，分类是2。。。。。） 然后在服务器端出最终决定。

### 问题：
![[image-87.png|563x621]]
1. 现在的router只看图像，但任务是图文组合的呀，同一张图，问法不同 → 需要的能力完全不同，仅凭图像 token 很难区分「我现在要用哪个任务/模态的 adapter」。（VQA,分类，图文定位）
2. 在 Pilot 里，CT-MoA 的 router ϕ\phiϕ 只根据 CLIP 图像 encoder 输出的视觉特征 HvH_vHv​ 做路由，**纯视觉驱动**，没有显式用文本或任务标签；
3. 在“图像 + 文本 + OCR”等多模态任务里，**决定用哪个专家的最好线索，恰恰不在纯视觉特征里**，  但现有 router 完全忽略了这些信号。
4. 在 FL 里就算是同一个模态，同一个任务，他们的分布也是非常IID的：
- 各 client 上的 **问题分布 / 模态分布高度非 IID**
  有的机构主要是票据 OCR 问答，有的是日常 VQA；    
- 服务器看不到原始数据，只能通过参数更新、adapter 和 router 的行为“侧面感知”这些差异。

对应解决方法：
在Pilot里面，router传的只有图像特征，我们现在吧图+文拼到一起（可以是OCR也可也是图像外面拼一圈文本+任务符号编码，VQA是1，分类是2。。。。。） 然后在服务器端出最终决定。

### 问题：联邦客户端中处理的量太少，可以有选择性的加入OCR。
- 只有有 OCR 的 client 才更新 OCR expert + OCR 相关 router 分支；
- 没有 OCR 的 client：  
    - 或不加载 OCR 分支（显式屏蔽）；    
    - 或只保留一个 frozen OCR expert（来自其他 client distillation，不更新）。
- 服务器端聚合的时候：
	- 对 Vision adapter：按任务聚合（类似 Pilot）；
	- 对 OCR adapter：只在有 OCR 的 client 上做 FedAvg；
	- 对 router：可以做参数级 FedAvg，或加上一些模态-aware 的约束（比如不同模态的 gating head 分开聚合）。


---
总结一下现有的几个：
FedMix：2021年
	在每一个客户端上训练一组experts，通过Gate去选专家
pFedMoE：
	 每个客户端上部署一个 local expert 和一个 global expert 和一个gating 和一个分类头，gating是为了学到local和global的参数比例。