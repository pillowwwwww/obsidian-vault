---
date: 2025-10-23 16:43:22
tags: dailynote
rating: ⭐
excerpt: 
---

## TODO

- [ ] 学习lora后，回去看adapt综述论文和b站的两个视频[大模型时代下做科研的四个思路【论文精读·52】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1oX4y1d7X6?spm_id_from=333.788.videopod.sections&vd_source=ca057beeef58161f396ba6bc8f855c34) [多模态经典论文讲解 4：VLMO_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1MAK3zkEhT?spm_id_from=333.788.videopod.sections&vd_source=ca057beeef58161f396ba6bc8f855c34&p=4)
- [ ] zotero里面放了几篇论文，待看，比如今年联邦大模型论文等。Federated intelligence: When large AI models meet federated fine-tuning and collaborative reasoning at the network edge/  FedDAT
- [ ] 了解一下[https://huggingface.co/docs/peft/index]前几年PEFT很火
## Tracking

联邦大模型是主流创新（结合lora）
lora是adapt的一个实现方式。
今年最火的是视频生成，能结合吗

了解了一下lora 这个视频讲的不错。
## LoRA 速记笔记（用 ΔW = A·B 表示）

**核心概念**：把 **W′拆分为 = W + ΔW，去训练ΔW**
- 在微调时**不改**原权重 (W)，只学习一个**低秩更新**：  
    **ΔW = A·B**（A、B是小矩阵，秩 r 很小）。
    
- 推理用更新后的权重：**W′ = W + ΔW**。
![[image-62.png]]
![[image-60.png]]

**为什么好用（要点版）**

- **参数少**：只训练 A、B，参数量远小于全参微调。
    
- **显存省**：优化器状态也只在 A、B 上，显存需求大幅下降。
    
- **可合并部署**：训练后可把 **ΔW** 合并回 (W)，**推理零额外开销**。
    
- **模块化/可个性化**：不同任务/客户端各自一套 A、B 即可（adapter）。
    
#### 参数调整过程
![[image-63.png]]

#### LoRA在哪里能用？
![[image-64.png]]

#### LoRA改进版本

![[image-65.png]]


#### 常用超参

![[image-66.png]]- 
- **最重要的rank**，缩放系数，目标模块 
    

> 训练时**只更新 A、B**，其余参数全部**冻结**。

**最小实践流程**

1. 选基座模型（如 7B LLM）。
    
2. 指定插入位置与超参（r、α、dropout）。
    
3. **冻结**基座，仅训练 **A、B**（得到 **ΔW = A·B**）。
    
4. 导出适配器（A、B）。推理时：
    
    - 需要低延迟 → **合并** ΔW 到 (W)；
        
    - 需要多域切换 → **热插拔** A、B（不合并）。
        

**联邦学习中的用法（简述）**

- 各客户端本地训练自己的 **A、B**，只上传 **A、B**（通信量小）。
    
- 服务器做聚合（可加安全聚合/差分隐私），再下发新的 **A、B**。
    
- 客户端可保留一份**个性化** A、B 与全局版搭配使用。
    

**何时需要加强或不用 LoRA**

- **跨域跨度很大/要求极致性能**：增大 r，或**部分解冻**少量层；必要时再考虑全参。
    
- **数据量特别多且算力充足**：全参或混合策略可能更优。
    

**常见坑**

- 忘记**冻结**基座 → 显存暴涨、不稳定。
    
- r 与 α 设得极端 → 训练抖动或收敛慢。
    
- 一上来插入**太多层位** → 先小范围验证再扩展。
    

> 一句话记忆：**LoRA = 用小矩阵 A、B 近似出 ΔW，替代改大权重 W；省参、省显存、可合并、好部署。**