---
date: 2025-11-16 17:57:36
tags: dailynote
rating: ⭐
excerpt: 
---

## TODO

- [ ] stage1笔记，stage2还没看
- [ ] stage1和2跟文本/视觉模块的关系
- [x] 数理统计作业

## Tracking
很好，这两个问题都很关键，我分两块讲：  
1）client-specific adapter 什么时候学、什么时候冻；  
2）LoRA（text-adapter）在这个框架里到底是怎么干活 + 怎么做联邦聚合的。

---
你这个疑惑本质上是两个概念搅在一起了：

- 「**Stage1/Stage2**」= 本地训练时用的 **两种不同的模型结构 + loss 设计**
    
- 「**联邦一轮 (round)**」= **下发 → 本地训练若干步 → 上传 → 聚合** 这样一个通信周期
    

我一步一步帮你拆开，看清楚时间线。

---

## 1. 先说结论，纠正一下直觉

> ❌ 「Stage1 学全局任务 adapter = 一轮联邦；Stage2 再学一次 = client 上传一次参数，然后 backward 两次。」

不是这样的。

**正确的理解：**

1. **Stage1 和 Stage2 都是在本地“跑很多个 batch 的 forward/backward”的一段训练过程**，不是“只 backward 一次”。
    
2. 在 Pilot 里，**第一次通信周期**大致是：
    
    - 先本地跑一段 Stage1（很多 step 的 backward）→ 上传参数 → 服务器聚合；
        
    - 服务器下发聚合后的结果，客户端再本地跑一段 Stage2（很多 step 的 backward）→ 再上传参数 → 再聚合。
        
3. **之后的通信轮 (round 2,3,…)**，论文的实现基本是只在 **Stage2** 的结构下继续做本地训练 + 聚合（Stage1 更像一个“只有第一轮用的 warm-up / 预训练”）。
    

所以：

- **不是**「上传一次然后只 backward 两次」；
    
- 而是「**在 Stage1 和 Stage2 都各自做很多次 backward**，并且在第一次大 cycle 里面上传了两次（S1 结束一次，S2 结束一次），之后的轮次只在 Stage2 上继续优化」。
    

---

## 2. 按论文原文顺一遍时间线（关键段）

看论文里的 “Federated Optimization” 段落（我翻成白话）：

> We train the Pilot with two alternate steps of local update and global aggregation for R rounds.  
> In the first round, the server sends the base MLLM…  
> Each client performs local instruction tuning of **stage 1**, then send the task-specific adapter and text-adapter parameters to the server.  
> After global aggregation, the server sends T task-specific adapters and text-adapter parameters back to the client.  
> Each client updates the model architecture and performs local instruction tuning of **stage 2**, and finally send the local task-specific adapter and text-adapter parameters to the server.

结合后面的实现细节（$R=3, E=1$）：

可以这样理解：

### Round 1（第一次通信）
1. **下发**：
    - 服务器把 base LLaVA（CLIP + LLM + 原 connector）下发给所有 client。       
2. **本地 Stage1（很多 backward，不是一次）** 
    - 结构：有 $\psi_t$（task adapter）和 $\psi_s$（client adapter），没有 CT-MoA；        
    - 训练：      
        - 每个 client 在自己的数据上跑 $E$ 个 epoch（实现里 $E=1$），每个 epoch 里面有很多 step：          
            - 对每个 batch 算 $L = L_{\text{ce}} + \lambda_0 L_d$；              
            - backward，把梯度传到 $\psi_t,\ \psi_s,\ \Theta^l$（LoRA）。              
    - 这段结束以后，**只是结束了 Stage1 这段训练**，不是只 backward 一次。     
3. **上传 ①**：
    - 每个 client 把：     
        - 任务 adapter 参数 $\Theta^a$（来自 $\psi_t$）        
        - 文本 LoRA 参数 $\Theta^l$  
            上传给服务器。        
4. **聚合 ①**：
    - 服务器做：    
        - Task-aware aggregation：同一任务的 $\Theta^a$ 加权平均 → 得到每个任务的全局 task adapter；        
        - Adaptive Text-adapter Aggregation：对 LoRA $\Theta^l$ 做「邻居加权聚合」。            
5. **下发（第二次）**：
    - 服务器把：  
        - $T$ 个任务的全局 task adapter          
        - 每个 client 的聚合后 LoRA  
            再次下发给各个 client。          
6. **本地 Stage2（又是很多 backward）**   
    - 客户端更新结构：     
        - 把本任务的 adapter 设为 index 1：$\psi^t_1$            
        - 把其他任务的 adapter 作为 $\psi^t_2,\dots,\psi^t_T$；        
        - 在 $\psi^t_i$（$i\ge2$）上加 cross-task adapter $\psi^c_i$；        
        - 加 router $\phi$ 组成 CT-MoA。        
    - 本地训练：        
        - 对每个 batch 用 Stage2 的 loss：          $$L = L_{\text{ce}} + \lambda_1 L_b + \lambda_2 L_z$$        
        - backward，把梯度传到：本地任务的 $\psi^t_1$、所有 $\psi^c_i$、LoRA $\Theta^l$、router $\phi$  
        - client-specific adapter $\psi_s$ 在这一阶段是冻结的。    
7. **上传 ②**：
    - Stage2 结束后，每个客户端 **再次上传**：    
        - 当前任务的 task adapter 参数 $\Theta^a$（来自更新后的 $\psi^t_1$）；        
        - LoRA 参数 $\Theta^l$            
8. **聚合 ②**：
    - 同样再做一次 task-aware aggregation + ATA，
    - 得到这一轮的全局参数，供下一轮 round 使用。 

> 所以：
- 第 1 轮：做一遍 Stage1（warm-up + 初始化本任务 adapter）、然后 Stage2；
- 第 2~R 轮：只在 Stage2 架构下继续本地训练 + 聚合（因为 task adapter / CT-MoA 结构已经有了，再回头 Stage1 意义不大，也浪费通信）。

---

## 3. 回到你原来的两个点，逐条回答

### 3.1 「Stage1 学全局任务 adapter，这不就是一轮联邦下发吗？」

更精确地说：

- **Stage1 本身只是“本地模型结构 + loss 设计”的一块**；
    
- 把「所有 client 用 Stage1 在各自数据上训练一段 → 上传 → 服务器聚合」这整个流程，  
    你当然可以**抽象地**叫做「一轮联邦」，  
    但在 Pilot 里他们又在同一个大 cycle 里紧接着加了一个 Stage2 + 再一次聚合。
    

所以：

- 你可以把 **Stage1+聚合+Stage2+聚合** 看成「Pilot 的一整轮大迭代」；
    
- 也可以把每个「本地训练 + 聚合」当成一个小 round，这样 Round1 会包含 Stage1-round 和 Stage2-round 两个小步。
    

论文只是把它们统称为“两阶段本地更新 + 全局聚合”，然后整体重复 $R$ 次。

### 3.2 「Stage2 再学一次？client 上传一次数据，然后 backward 两次吗？」

- **是的，task adapter 会在 Stage1 和 Stage2 都被优化**，但这是「先用 Stage1 的结构学一遍基础，再在 CT-MoA 的结构下 fine-tune」，不是完全从头来两遍。
    
- client **不是只上传一次**，而是：
    
    - 第一次在 Stage1 结束时上传；
        
    - 第二次在 Stage2 结束时上传；
        
    - 后续轮次还会有更多上传。
        

而每一段 Stage 都包含很多个 backward step，而不是“一次 backward 对应一个 Stage”。

你可以把整个过程想成：

> - **Stage1**：在本地、单任务视角下，把 $\psi_t$ 先学成“这个 task 的好表征”，同时用 $\psi_s$ 拿走 client 独有残差。
>     
> - **聚合 ①**：把同一任务的 $\psi_t$ 汇总成全局任务 adapter。
>     
> - **Stage2**：在 CT-MoA 结构下，拿着“全局任务 adapter + 其他任务 adapter + cross-task adapter”，对 $\psi^t_1$ 再精修，让它会跨任务协同。
>     
> - **聚合 ②**：再把更新后的 task adapter + LoRA 聚一次，作为最终全局模型或下轮初始点。
>     

如果你愿意，我可以画一个「时间轴小表」给你，把：

- 每个阶段：谁在训练（哪些参数有梯度）、
    
- 谁在上传、
    
- 服务器做什么聚合
    

列成一张表，你以后再看这篇论文就不用重新在脑子里 simulate 一遍整个流程了。