---
date: 2025-11-16 17:57:36
tags: dailynote
rating: ⭐
excerpt: 
---

## TODO

- [ ] stage1笔记，stage2还没看
- [ ] stage1和2跟文本/视觉模块的关系
- [x] 数理统计作业

## Tracking
很好，这两个问题都很关键，我分两块讲：  
1）client-specific adapter 什么时候学、什么时候冻；  
2）LoRA（text-adapter）在这个框架里到底是怎么干活 + 怎么做联邦聚合的。

---

## 一、client-specific adapter 什么时候学？冻结之后还会不会 backward？

先把时间线说清楚，否则容易误解成“它从来不学”。

### 1.1 生命周期：Stage 1 学完 → Stage 2 冻住

对每个 client 来说，大致是这样一条时间线（不管通信轮数，先看一个完整流程）：

1. **Stage 1 本地训练（学习阶段）**
    
    - 可训练模块包括：
        
        - 视觉侧：task adapter $\psi_t$、client adapter $\psi_s$
            
        - 文本侧：LoRA（text-adapter）
            
    - 损失：
        
        - $L_{\text{ce}}$（语言建模）
            
        - $L_d$（difference loss，拉开 $x_t, x_s$ 的子空间）
            
    - 这时候：
        
        - forward：$H_v \to \psi_t(H_v), \psi_s(H_v)$，相加得到 $x_{\text{img}}$，送进 LLM；
            
        - backward：梯度从 $L_{\text{ce}}$ + $L_d$ 一路传回 $\psi_t$ 和 $\psi_s$，它俩都在**学**。
            
    - 结果：
        
        - $\psi_t$ 学到了“本任务、跨 client 的共性视觉信息”；
            
        - $\psi_s$ 学到了“本 client 独有的偏好 / 残差”（比如本地数据风格、标签习惯等）。
            
2. **Stage 1 结束 → 上传参数**
    
    - 上传给服务器的只有：
        
        - task adapter 的参数（记成 $\Theta^a$）
            
        - 文本侧 LoRA 参数（$\Theta^l$）
            
    - **client-specific adapter $\psi_s$ 严格不上传，参数留在本地磁盘上**。
        
3. **Stage 2 本地训练（跨任务借力阶段）**
    
    - 现在从服务器拿到一堆任务 adapter：$\psi^t_1, \dots, \psi^t_T$（都是聚合后的版本）
        
    - 在本地搭起 CT-MoA：
        
        - 本任务适配器：$\psi^t_1$
            
        - 其他任务适配器：$\psi^t_i + \psi^c_i$（$i \ge 2$）
            
        - Router $\phi$ 产生每个任务的权重 $P[i]$
            
    - **这一步明确规定：$\psi_s$ 冻结，不再更新，只做 forward**。
        

所以答案是：

> client-specific adapter 只在 **Stage 1 时参与 backward 和更新**，  
> Stage 2 故意把它 freeze，当成“已经学好的本地 personalization 分支”，不再动。

你刚才的“永远不会 backward 吗？”其实只对 Stage 2 生效，**对 Stage 1 不成立**——Stage 1 它是参与训练的。

### 1.2 冻结之后还有没有梯度？

从计算图角度说：

- 如果在实现里对 $\psi_s$ 的参数 `requires_grad=False`，那它**不会再更新参数**；
    
- 有的实现甚至会对它的输出 `x_s` 用 `.detach()`，直接把梯度截断；
    
- 即便你不 `detach`，CLIP encoder 本身也是冻结的，所以从 $\psi_s$ 往前也没有可训练参数。
    

所以你可以直接当成：

> Stage 2 里，$\psi_s$ 就是一个固定的函数：  
> $x_s = \psi_s(H_v)$，只在 forward 中贡献一份“本地偏移”，  
> 不再根据 Stage 2 的 loss 改变自己。

这也符合直觉：Stage 2 的重点是“跨任务借力”（CT-MoA + router），  
client-specific 那条线已经在 Stage 1 把本地个性学完了，不希望它被跨任务训练再冲掉。

---

## 二、LoRA（text-adapter）在这套框架里具体怎么干？

你问的“关于 LoRA 的这部分如何做的”，可以分三个层次：

1. LoRA 在 LLM 里干什么（单机视角）
    
2. 在 Stage 1 / Stage 2 怎么训练它
    
3. 在联邦优化里，怎么对 LoRA 做“自适应聚合”（Adaptive Text-adapter Aggregation, ATA）
    

### 2.1 LoRA 在 LLM 里干什么？

论文的设计是：

- 冻结整个 base LLM 的大权重（所有原始 $W_q, W_k, W_v, W_o, W_{\text{ffn}}$ 等）；
    
- 在这些矩阵上挂 LoRA 模块（低秩增量），统称为 **text-adapter**。
    

标准 LoRA 回顾一下：

- 对某个原始矩阵 $W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$，  
    不直接训练 $W$，而是引入低秩分解：
    
    - $W' = W + \Delta W$
        
    - $\Delta W = B A$，其中 $A \in \mathbb{R}^{r \times d_{\text{in}}}$、$B \in \mathbb{R}^{d_{\text{out}} \times r}$，$r \ll d$。
        
- 训练时只更新 $A, B$（也就是 LoRA 参数），$W$ 保持不动。
    

在 Pilot 里：

- 所有这些 LoRA 参数的集合，就记为一整个 $\Theta^l$，叫做 text-adapter；
    
- 它是**唯一会随着 client 本地文本数据变化而更新的“语言侧模块”**。
    

### 2.2 Stage 1 / Stage 2 中，LoRA 怎么训练？

在两个 Stage 里，LoRA 的训练方式本质上是一样的，只是视觉侧前面接的东西不同。

**前向：**

- 输入序列是 `[image tokens, text tokens]`：
    
    - image tokens 来自 $x_{\text{img}} = x_t + x_s$（Stage 1）或 $x_{\text{img}} = x_{\text{CT-MoA}} + x_s$（Stage 2）；
        
    - text tokens 是指令 + 答案前缀；
        
- 这些 token 进入 LLM，每一层的 self-attention / FFN 里都插着 LoRA 分支；
    
- 输出一个 logits 序列，对应预测下一个 token 的分布。
    

**损失：**

- 一律用语言建模交叉熵 $L_{\text{ce}}$：  
    “给定图像 + 指令 + 已生成的答案前缀，预测下一个答案 token”。
    

**反向：**

- loss $\to$ logits $\to$ LLM 各层（base 权重冻结）$\to$ LoRA 参数：
    
    - base 矩阵 $W$ 不更新；
        
    - 只有 LoRA 的 $A, B$ 得到梯度并更新。
        
- 同时，梯度也会回传到视觉侧的 adapter（Stage 1 时是 $\psi_t,\psi_s$；Stage 2 时是 $\psi^t_1,\psi^c_i$）。
    

所以你可以把 LoRA 当成：

> “挂在 LLM 里的一条可学习旁路，  
> 全程用 $L_{\text{ce}}$ 的梯度更新，  
> 不需要额外的特殊 loss。”

### 2.3 联邦里的 LoRA 聚合：Adaptive Text-adapter Aggregation（ATA）

关键在于：  
不同 client 上的文本分布、任务可能差别很大，如果简单做 FedAvg 把所有 LoRA 平均，很容易：

- 把某些 client 独有的“语言风格 / 答案模式”抹平；
    
- 不同任务之间互相污染（比如 caption 和 VQA 的文本格式差很多）。
    

所以论文设计了一个 **“自适应邻居聚合”**（ATA）：

大致步骤（概念版，不抄原公式）：

1. **收集所有 client 的 LoRA 参数集**：
    
    - 每个 client $k$ 有一套 text-adapter 参数 $\Theta^l_k$（所有 LoRA matrix 的集合）。
        
2. **计算 client 之间的距离**：
    
    - 把 LoRA 参数 flatten 后，当作一个大向量（或用某种投影），
        
    - 定义任意两 client $k,j$ 的距离 $d_{kj}$（论文用欧氏距离）。
        
3. **为每个 client 找邻居 + 算权重**：
    
    - 对 client $k$，选几 个距离最近的客户端构成邻居集合 $\mathcal{N}(k)$；
        
    - 为这些邻居分配权重，例如：
        
        - $w_{kj} \propto \exp(-d_{kj})$，在邻居集合内做 softmax 归一化。
            
4. **聚合 LoRA 参数（但“个性化”地聚）**：
    
    - 对于 client $k$，新的 LoRA 参数为：  
        $$\bar{\Theta}^l_k = \sum_{j \in \mathcal{N}(k)} w_{kj} \Theta^l_j$$
        
    - 也就是说，每个 client 的 LoRA 是从“和自己相似的几个 client”那儿**加权平均**出来的，而不是所有人一起平均。
        
5. **下发**：
    
    - 服务端把 $\bar{\Theta}^l_k$ 下发给 client $k$，
        
    - 作为下一轮 Stage 1 / Stage 2 的 text-adapter 初始值。
        

直觉上：

> - 如果两个 client 的 LoRA 参数很相似，说明他们的任务 / 文本风格也相似，互相聚合是有意义的；
>     
> - 如果很远，就不给太多权重，避免“截然不同任务的语言模式”被硬塞过来。
>     

这个策略同时用在 Stage 1 和 Stage 2 之后的联邦聚合里。

---

## 总结一句给你记：

- **client-specific adapter $\psi_s$：**
    
    - 在 Stage 1 完整地参与 forward + backward，从 $L_{\text{ce}} + L_d$ 学到“本地个性”；
        
    - 在 Stage 2 冻结，只作为一个固定的 personalization 分支参与 forward，不再更新。
        
- **LoRA / text-adapter $\Theta^l$：**
    
    - 嵌在 LLM 各层，靠 $L_{\text{ce}}$ 的梯度更新，是“语言侧的 PEFT”；
        
    - 每轮结束时，上传到服务器；
        
    - 服务器不是简单 FedAvg，而是根据 LoRA 参数之间的距离，做“邻居加权聚合”（ATA），  
        然后个性化地下发给每个 client。
        

如果你愿意，下一步我可以单独画一个 **“参数视角表”** 给你：  
把所有参数按「是否本地更新」「是否上传」「Stage 1/2 是否冻结」列成一个表，这样你之后复习这篇论文时，只要看一页就能立刻想起来哪个模块什么时候动、什么时候传。