---
date: 2025-12-15 10:08:03
tags: dailynote
rating: ⭐
excerpt: 
---

## TODO

- [ ] 任何事情都能在五小时之内做完！
- [ ] 多喝水，避免久坐不动
- [ ] 阅读一篇文献

## Tracking

要问师兄的问题：
1. 动机/研究问题的地方要不要做图呢：
	1. 现在的论文说的是”可能“，但是没有具体数据？？
		1. This indiscriminate parameter aggregation can result in the global model overfitting to specific client distributions while losing its general capabilities。
	2. 怎么看待联邦学习和增量/持续学习的关系？我感觉很类似。还有就是OOD能力和客户端漂移/未见任务未见domain上的能力，感觉也很类似。
我应该在写论文的时候：
	写清楚one-shot 客户端冷启动的发展？


2. C-CLIP这里它是先训练了lora，然后再lora+clip去测试ImageNet、CIFAR-100、StanfordCars、Flowers、DTD、Food101这些数据集上的零样本能力吗？它们的任务是什么分类吗？这里我有点混淆，分类和VQA,图像检索，目标跟踪属于不同的任务，那么不同的数据集检索也属于不同任务吗？

3. C-CLIP中提到的研究问题“能不能在只见到当前新领域数据的情况下，让 CLIP 持续学新域，”是什么意思？

4. C-CLIP选择的这些baselines都是哪些领域的方法？为什么要和他们比？C-CLIP是更重视zero-shot（未见任务/未见域？）吗？也就是OOD能力？

5. 你认为持续学习和zero-shot以及one shot联邦学习的关系是什么？多域检索是什么？

6. table4的意思是C-CLIP在未见task上zero-shot的准确度为61.58%吗？那CIFAR100是什么意思？为什么有不同的数据集和task？为什么此处没有对比CLIP的zero-shot能力？

---
我的结论是不是持续学习，它们不是一个东西
---

下面按“真的在 LoRA+FL 里明确讨论/触及 zero-shot/OOD 掉点”的强相关程度给你整理（2023–2025 顶会顶刊/强 venue 为主），并点出它们认为掉点来自哪里、怎么修。

---

## 一、LoRA + FL 里最直接碰到“zero-shot/OOD/未见客户端”问题的论文

### 1) FlexLoRA（NeurIPS 2024）

核心点：**同一个 LoRA rank 套所有客户端**会导致显著性能问题，尤其是**未见客户端数据上的 zero-shot 泛化下降**；他们把它归因到“bucket effect”（为了照顾弱设备，全球只能用很小 rank，容量不够学到更通用的全局知识），并用 SVD 做“全尺寸 LoRA 聚合→再分发到不同 rank”。在文中明确提到 zero-shot 指标提升（如 zero-shot Rouge-L 等）。

你做“未见域 zero-shot 掉点”的 story，这篇是最顺手的起点：它**直接把 zero-shot 泛化掉点当作现象**来讲。

### 2) SLoRA（NeurIPS 2023 / NeurIPS-FM workshop 线）

核心点：他们观察到**异质性越强（用户数据越 diverse），PEFT（含 LoRA）和全量微调（FFT, full fine-tuning）的性能差距会变大**，也就是“LoRA 在联邦异质环境下更容易学不到全局泛化”。因此提出 data-driven initialization 来缓解。 ([arXiv](https://arxiv.org/abs/2308.06522?utm_source=chatgpt.com "SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"))

它不一定用“zero-shot”这个词，但对你要解释“为什么未见域掉点”非常关键：**低秩容量 + 异质性 ⇒ 泛化差距扩大**。

### 3) FRLoRA（Federated Residual Low-Rank Adaptation）（ICLR 2025）

核心点：直说“FedAvg + LoRA 在数据异质时效果不好”，原因拆成两类：**内在限制（低秩参数空间受限）+ 外在限制（client drift）**，导致学不到有效 global knowledge；方法上用 residual 的方式让全局更新在更高 rank 空间里表达、并用 SVD 思路做校准。

这篇适合用来给你的论文动机加“机制解释”：掉点不是玄学，是**低秩可表达性 + 联邦漂移**的结构性矛盾。

### 4) FFA-LoRA（Improving LoRA in Privacy-Preserving FL）（ICLR 2024）

核心点：指出 LoRA 在（含 DP 噪声的）联邦里会不稳定，关键因素之一是：客户端本地**联合优化 A/B**，但服务器端却**分别聚合 A 和 B**，会出现“discordance”，性能对超参也敏感；他们用“冻结随机初始化的非零矩阵、只训练零初始化矩阵”等策略稳定训练。

你如果看到“未见域 zero-shot 掉点”同时伴随训练不稳/收敛差，这类工作常是根因之一：**LoRA 的 A/B 聚合数学结构本身就容易引入偏差**。

### 5) FLoRA（NeurIPS 2024，注意不是上面 FlexLoRA）

核心点：把问题说得更“硬”：很多 LoRA 联邦方法（如把 FedAvg 直接套在 A/B 上）会产生**数学上不正确的聚合噪声（aggregation noise）**，导致 fine-tuning effectiveness 下降；提出 stacking 聚合来做到 noise-free，并支持 rank heterogeneity。

它不主打“zero-shot”，但你要解释“为什么全局 LoRA 在未见域更差”，这类“聚合噪声导致全局更新偏离正确方向”的论证很有杀伤力。

### 6) LoRA-A2（ACL 2025）

核心点：也在打“discordance + heterogeneity 下低 rank 性能退化”的点，提出交替冻结与自适应 rank 选择来提升鲁棒性。 ([ACL文集](https://aclanthology.org/2025.acl-long.19.pdf?utm_source=chatgpt.com "Towards Robust and Efficient Federated Low-Rank ..."))

---

## 二、和你问题强相关、但不一定是 LoRA 的：VLM/CLIP 联邦里的 OOD/未见域（你做 CLIP zero-shot 很该看）

你如果研究对象是 CLIP/多模态，现实是：**很多顶会工作用 Prompt/Adapter 而不是 LoRA**来保零样本能力；但它们对“为什么会在未见域掉点”给了非常直接的证据链。

### 7) FedMVP（ICCV 2025，CLIP 联邦视觉 prompt tuning）

核心点：直接指出**（文本/视觉）prompt tuning 会对已知概念过拟合，从而限制对未见概念/未见域的泛化**；他们提出 multimodal visual prompt，让 unseen classes/domains 的泛化更好。

这篇对你“zero-shot 掉点”非常有用：它明确把“过拟合已知概念 ⇒ unseen 泛化差”写出来了。

### 8) FedAG（ICML 2025，CLIP + 多域）

核心点：在“FedFM + PEFT”框架下，明确把 **out-of-domain generalization（域外泛化）**作为核心挑战，做多 domain-specific adapters 并进行跨域学习来提升 OOD。 ([OpenReview](https://openreview.net/forum?id=6SIVFmjIm4&utm_source=chatgpt.com "Enhancing Foundation Models with Federated Domain ..."))

### 9) FedOA（ICLR 2025，foundation model + adapters）

核心点：把问题表述得和你几乎同构：**未见任务/未见客户端会带来分布偏移，导致性能变差**；用 adapter + feature distance regularization 做分布对齐，并给出 OOD 泛化视角。 ([OpenReview](https://openreview.net/forum?id=LcpdPCkZwI&utm_source=chatgpt.com "Federated Adapter on Foundation Models: An Out-Of- ..."))

---

## 三、你现在的假设里最危险的坑（我直接挑刺）

1. 你说的“zero-shot 掉点”，到底是
- “全局模型对未见域掉点”（global OOD），还是
- “个性化模型对未见域掉点”（personalized OOD），还是
- “CLIP 预训练零样本能力被破坏”（pretrained zero-shot erosion）？  
    这三者机制不同、对策也不同。很多论文其实只解决其中一个。

1. 很多“掉点”不是 LoRA 本身，而是 **LoRA 的联邦聚合方式**：A/B 分开平均会引入结构性误差（FLoRA/FFA-LoRA 都在打这个），你如果不把这个作为 baseline 对照，你会把锅甩错对象。
2. 你如果目标是 CLIP 的零样本（zero-shot）保真，那“在联邦里微调任何会破坏对齐的部分”都可能让 zero-shot 崩。很多 VLM 联邦工作因此转向 prompt/adapter（FedMVP/FedAG/FedOA 的路线），这不是保守，是经验驱动。

---

## 四、给你一个“可直接写进 related work”的归纳框架

你可以把 2023–2025 的 LoRA+FL “未见域/zero-shot 掉点”归为三类机制（每类都能对应上面论文）：

1. **容量/子空间不足（Capacity / Subspace limit）**：统一小 rank、异质性越强越学不到通用知识 → zero-shot/OOD 更差（FlexLoRA, SLoRA, FRLoRA）。
    
2. **聚合结构错误（Aggregation mismatch / noise）**：A/B 分开聚合引入结构性噪声，异质性下更严重（FLoRA, FFA-LoRA）。
    
3. **对已见概念/域过拟合（Overfitting to seen concepts/domains）**：轻量参数也会过拟合已见域，导致 unseen 退化（FedMVP；adapter 路线的 FedAG/FedOA 把它系统化）。
    

---
如果你愿意我继续往下做“更狠的调研”：我可以把每篇论文里**关于未见域/OOD/zero-shot 掉点的原句、实验设置（怎么构造 unseen domain/clients）、以及它们用了什么 backbone（LLaMA/ViT/CLIP）**逐条抽出来，帮你直接拼成 related work + problem statement。你现在只要告诉我：你的主场景是 **LLM 联邦**还是 **CLIP/VLM 联邦**。