---
date: 2025-07-25 23:25:03
tags: dailynote
rating: ⭐
excerpt: 
---

## TODO

- [ ] 任何事情都能在五小时之内做完！
- [ ] 多喝水，避免久坐不动
- [ ] 阅读一篇文献

## Tracking

cursor调大字体： ctrl shift+三个键

需要记录的两个问题：

## 1. 在深度学习中，一个模型的的训练通常要很久，而我不知道我的代码是否能够顺利跑起来，这应该怎么办呢？

这是一个在深度学习实践中非常普遍且重要的问题。花費数小时甚至数天的时间等待一个模型训练，结果却发现代码有bug或者方向错误，这会极大地浪费时间和计算资源。

为了解决这个问题，核心思想是：**先进行“演习”，再进行“实战”。** 通过一系列的“演习”来快速验证代码的每个环节是否都能正常工作，然后再投入到完整的、耗时的训练中。

以下是一套被广泛采用的、从易到难的“演习”流程和调试策略：

### 核心原则：先在小问题上验证，再扩展到大问题

开始时，你的目标不是训练出一个好模型，而是**证明你的代码能够顺利运行**。

---

### 第一阶段：代码“冒烟测试” (Dry Run)

这一阶段的目标是确保代码的各个组件能够被正确调用，数据能够顺利流动，不涉及真正的训练。

1. **用少量数据进行测试**：
    
    - 创建一个非常小的数据集。例如，从你的训练集中只抽取10-20个样本。
        
    - **目的**：确保你的 `DataLoader` 或数据预处理管道没有问题。这能快速暴露文件路径错误、数据格式问题、标签读取错误等。
        
2. **检查数据加载器 (`DataLoader`)**：
    
    - 从 `DataLoader` 中取出一个批次（batch）的数据。
        
    - 打印出这个批次的数据 (`X`) 和标签 (`y`) 的形状（shape）和类型（dtype）。
        
    - **检查点**：
        
        - 形状是否符合预期？例如，图像数据是不是 `(batch_size, channels, height, width)`？
            
        - 数据类型是否正确？例如，图像是不是 `torch.float32`，标签是不是 `torch.long`？
            
        - 数据范围是否正常？例如，归一化后的图像像素值是否在 `[0, 1]` 或 `[-1, 1]` 之间？
            
3. **单次前向传播 (Forward Pass)**：
    
    - 将刚刚取出的单个批次数据喂给你的模型：`output = model(X)`。
        
    - **检查点**：
        
        - 代码是否能顺利运行，没有报维度不匹配之类的错误？
            
        - 模型输出 (`output`) 的形状是否正确？例如，对于一个10分类问题，输出形状应该是 `(batch_size, 10)`。
            
4. **单次损失计算和反向传播 (Loss & Backward Pass)**：
    
    - 计算损失：`loss = loss_function(output, y)`。
        
    - 进行反向传播：`loss.backward()`。
        
    - 执行优化器步骤：`optimizer.step()`。
        
    - **检查点**：
        
        - 损失值是否是一个合理的标量（scalar）？例如，对于一个10分类问题，初始的交叉熵损失理论上应该在 −ln(1/10)≈2.3 左右。如果损失是一个巨大的数或者 `NaN`，说明可能有数值不稳定问题。
            
        - `loss.backward()` 是否能顺利执行？如果报错，通常是计算图中断或者有不支持求导的操作。
            
        - 检查模型参数的梯度是否存在：`model.parameters().__next__().grad` 是否不为 `None`？如果为 `None`，说明计算图可能被意外切断了。
            

**如果以上步骤都能顺利跑通，恭喜你，你的代码至少在“机械层面”是正确的。整个过程可能只需要几十秒。**

---

### 第二阶段：过拟合测试 (Overfitting Test)

这是最重要的一步，被称为“深度学习的黄金标准调试工具”。

1. **目标**：证明你的模型有足够的学习能力，并且你的训练循环逻辑是正确的。
    
2. **操作**：
    
    - 使用一个非常小的数据集（例如，1-2个batch的数据）。
        
    - 关闭所有数据增强（augmentation）、正则化（如 Dropout, Weight Decay）。
        
    - 让模型在这个极小的数据集上反复训练几百个迭代（iterations）。
        
3. **预期结果**：
    
    - **训练损失（Training Loss）应该能迅速下降到接近0**。
        
    - **训练准确率（Training Accuracy）应该能达到100%（或接近100%）**。
        

为什么这很重要？

一个足够强大的模型，必须有能力“记住”一小部分数据。如果它连这么简单的任务都完不成，说明你的代码里存在更深层次的bug，例如：

- **学习率设置不当**：太大导致梯度爆炸，太小导致不收敛。
    
- **损失函数选择错误**：例如，回归问题用了分类的损失函数。
    
- **模型架构问题**：网络层之间的连接有误。
    
- **优化器更新逻辑错误**。
    

**如果过拟合测试失败，就没必要在完整数据集上浪费时间了。** 你需要回头仔细检查你的训练逻辑。

---

### 第三阶段：系统化调试与良好实践

在通过了以上测试后，你可以更有信心地开始正式训练。同时，养成一些好习惯能让过程更顺利。

1. **日志记录 (Logging)**：
    
    - 不要只用 `print()`。使用成熟的日志库（如 `logging`）或者实验管理工具（如 **TensorBoard**, **Weights & Biases**）。
        
    - **关键记录项**：训练损失、验证损失、各项评估指标（准确率、F1分数等）、学习率。实时可视化这些指标可以让你在训练早期就发现问题（例如，损失不下降、验证集指标恶化等）。
        
2. **代码模块化**：
    
    - 将数据处理、模型定义、训练循环、评估逻辑分装到不同的文件或类中。这样更容易独立测试和调试每个部分。
        
3. **使用断言 (Assertions)**：
    
    - 在代码的关键位置使用 `assert` 来检查张量的形状、范围等，这是一种“防御性编程”，能提前捕捉到很多因形状变化引起的bug。
        
    - 例如：`assert images.shape == (BATCH_SIZE, 3, 224, 224)`
        
4. **利用调试工具**：
    
    - **IDE的调试器**：在PyCharm或VS Code中设置断点，可以让你在代码的任意位置停下来，检查所有变量的值、形状和类型，这是最强大的调试手段。
        
    - **PyTorch/TensorFlow的内置工具**：例如，PyTorch 提供了 `torch.autograd.set_detect_anomaly(True)`，可以在反向传播出错时提供更详细的堆栈跟踪信息。
        

### 总结流程

当你拿到一个新任务，准备编写和训练一个深度学习模型时，遵循以下步骤：

1. **[几秒钟]** 编写骨架代码，确保能从 `DataLoader` 取出一个batch。
    
2. **[几十秒]** 运行一次完整的前向和反向传播，确保没有机械性错误。
    
3. **[几分钟]** 在一个极小的数据集上进行过拟合测试，确保模型能学习。
    
4. **[~1小时]** 在一个中等规模的数据子集上（例如10%的训练数据）进行训练，并开启验证集。目的是初步调整超参数，确保验证损失会如期下降。
    
5. **[数小时/数天]** 在通过以上所有检查后，才启动完整的、耗时的训练任务，并使用 TensorBoard 等工具全程监控。
    

通过这种由小到大、由简到繁的验证策略，你可以极大地提高研发效率，避免在错误的道路上浪费宝贵的时间和计算资源。


## 2. MNIST是否是一个适合调试的数据集？它属于哪一步骤呢？



这是一个非常好的问题，它触及了调试策略的核心。

**总的来说，MNIST 是一个绝佳的“沙盒”或“单元测试”数据集，但它最适合用于我所描述的流程之外的一个更早的、独立的验证步骤。**

我们可以把它看作是 **“第0步：模型/算法的健全性检查 (Sanity Check)”**。

下面我来详细解释一下它属于哪一步，以及为什么这样划分：

---

### MNIST的角色：模型和算法的“标准跑道”

MNIST数据集（以及像CIFAR-10这样的类似数据集）具有以下特点：

- **简单且已解决**：它是一个非常简单的图像分类问题，几乎任何合理的模型都能在几分钟内达到很高的准确率（>95%）。
    
- **加载方便**：主流深度学习框架（PyTorch, TensorFlow）都内置了对它的支持，一行代码就能下载和加载。
    
- **运行快速**：数据量小，图片尺寸小，不需要GPU也能快速完成训练。
    

因此，它的最佳用途是：**当你实现了一个新的、不确定的模型架构，或者一个自定义的算法（比如新的损失函数、优化器）时，用来快速验证这个新组件本身是否能工作。**

**可以这样理解：** 如果你的新模型连MNIST这么简单的问题都学不会，那几乎可以肯定，它也无法解决你真正要面对的那个更复杂的问题。这说明问题出在你的**模型设计或算法实现**上，而不是你的数据处理流程上。

**所以，MNIST不完全属于我之前提到的任何一个步骤，因为它不使用你项目的“真实数据”。** 我之前描述的步骤（特别是“冒烟测试”和“过拟合测试”）核心思想是 **用你项目本身的、真实的数据** 来进行测试，哪怕只用一小部分。这是为了确保你的 **数据加载和预处理管道** 是正确的。

---

### 在整个工作流中，MNIST应该放在哪里？

一个更完整的、考虑了MNIST的调试工作流应该是这样的：

- **第0步：模型/算法健全性检查 (可选，但强烈推荐)**
    
    - **任务**：验证你新设计的模型或算法的基本学习能力。
        
    - **数据集**：**使用MNIST或CIFAR-10。**
        
    - **目的**：如果在这个“标准跑道”上都跑不起来，说明模型/算法本身有bug。这能帮你把问题**隔离**出来，让你专注于修复模型，而不是同时怀疑数据管道。
        
- **第1步：代码冒烟测试 (Dry Run)**
    
    - **任务**：验证整个代码流程能跑通。
        
    - **数据集**：**使用你真实项目的一小撮数据（例如10-20个样本）。**
        
    - **目的**：确保你的`DataLoader`、数据预处理、模型输入输出维度匹配等环节没有机械性错误。
        
- **第2步：过拟合测试 (Overfitting Test)**
    
    - **任务**：验证模型有足够的能力学习，并且训练循环是正确的。
        
    - **数据集**：**使用你真实项目的一个或两个批次（batch）的数据。**
        
    - **目的**：确保模型可以在这个微型数据集上达到接近0的损失和100%的准确率。
        
- **后续步骤...**
    
    - 在更大的数据子集上训练，最后在完整数据集上训练。
        

### 总结

- **用MNIST**：来测试你的**“引擎”（模型/算法）**本身是否能转动。
    
- **用你自己的小数据集（冒烟测试/过拟合测试）**：来测试你的**“整车”（模型 + 数据管道 + 训练逻辑）**是否能顺利上路。
    

所以，当你对你新写的模型代码没有信心时，先让它在MNIST上跑一下，这是一个非常高效且明智的习惯。如果通过了，你就可以更有信心地进入到使用你真实数据进行测试的下一个阶段。