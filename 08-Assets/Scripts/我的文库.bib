@inproceedings{10.5555/3692070.3692369,
  title = {{{FedMBridge}}: {{Bridgeable}} Multimodal Federated Learning},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Chen, Jiayi and Zhang, Aidong},
  date = {2024},
  series = {{{ICML}}'24},
  publisher = {JMLR.org},
  location = {Vienna, Austria},
  abstract = {Multimodal Federated Learning (MFL) addresses the setup of multiple clients with diversified modality types (e.g. image, text, video, and audio) working together to improve their local personal models in a data-privacy manner. Prior MFL works rely on restrictive compositional neural architecture designs to ensure inter-client information sharing via blockwise model aggregation, limiting their applicability in the real-world Architecture-personalized MFL (AMFL) scenarios, where clients may have distinguished multimodal interaction strategies and there is no restriction on local architecture design. The key challenge in AMFL is how to automatically and efficiently tackle the two heterogeneity patterns-statistical and architecture heterogeneity-while maximizing the beneficial information sharing among clients. To solve this challenge, we propose FedMBridge, which leverages a topology-aware hypernetwork to act as a bridge that can automatically balance and digest the two heterogeneity patterns in a communication-efficient manner. Our experiments on four AMFL simulations demonstrate the efficiency and effectiveness of our proposed approach.},
  articleno = {299},
  pagetotal = {20},
  file = {E:\Obsidian\obsidian_vault_template_for_graduate_student\08-Assets\pdfs\2024-FedMBridge\Chen和Zhang2024 - .pdf}
}

@article{Huang2024,
  title = {Multimodal Federated Learning: {{Concept}}, Methods, Applications and Future Directions},
  shorttitle = {Multimodal Federated Learning},
  author = {Huang, Wei and Wang, Dexian and Ouyang, Xiaocao and Wan, Jihong and Liu, Jia and Li, Tianrui},
  date = {2024-12},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {112},
  pages = {102576},
  issn = {15662535},
  doi = {10.1016/j.inffus.2024.102576},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253524003543},
  urldate = {2025-05-13},
  langid = {english},
  keywords = {⭐},
  file = {E\:\\Obsidian\\obsidian_vault_template_for_graduate_student\\02-Reading\\mdnotes\\@Multimodal federated learning Concept, methods, applications and future directions.md;E\:\\Obsidian\\obsidian_vault_template_for_graduate_student\\08-Assets\\pdfs\\Information Fusion2024\\Huang et al_2024_Multimodal federated learning.pdf}
}

@article{McMahan2016,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and family=Arcas, given=Blaise Agüera, prefix=y, useprefix=false},
  date = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1602.05629},
  url = {https://arxiv.org/abs/1602.05629},
  urldate = {2025-05-10},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  version = {4},
  keywords = {FedAvg},
  file = {E\:\\Obsidian\\obsidian_vault_template_for_graduate_student\\02-Reading\\mdnotes\\@Communication-Efficient Learning of Deep Networks from Decentralized Data.md;E\:\\Obsidian\\obsidian_vault_template_for_graduate_student\\08-Assets\\pdfs\\2016\\McMahan et al_2016_Communication-Efficient Learning of Deep Networks from Decentralized Data.pdf}
}

@software{zotero-4,
  title = {Addon {{Item}}}
}
