标题：Pruning as Alignment: Enhancing One-Shot Federated Generalization via Gradient Sensitivity
---
讲个故事：我们需要把 **One-Shot FL (骨架)**、**Domain Generalization (灵魂)** 和 **Pruning (手段)** 完美融合。

- 背景：我们处于多模态大模型时代，数据分散在各地（FL），且通信极其昂贵。因此，**One-Shot FL（单次通信）** 是刚需。
- 冲突：在 One-Shot 场景下直接 Merge，因为没有后续的微调修正，**异构的视觉参数（Visual Parameters）会发生剧烈冲突**（引用 LoRM 的“不定方程”理论，加实验一配图）。
- 假设：我们认为，这种冲突的本质是 **视觉模态的过拟合** 
		- Client A 的参数里混入了“油画笔触特征”。
		- Client B 的参数里混入了“素描线条特征”。
		- **这些特征对于“语义理解（识别是一只狗）”来说，都是噪音。**
- Insight：- 文本模态（Text Encoder）是高度稳定的（语义锚点）。
	- **剪枝不仅仅是压缩，更是一种“对齐”手段。** 通过剪掉那些对“图文匹配”没有贡献的视觉参数，我们实际上是在**强行对齐**视觉和文本，剔除风格噪音。
- **结论 (Conclusion):** 剪完之后的模型，虽然参数变少了，但**更纯粹**了。因此，它在未见过的domain上，泛化能力反而吊打所有未剪枝的方法。
---
需要定义为未见域，而不是未见任务，Pilot是未见任务，但是太难了。