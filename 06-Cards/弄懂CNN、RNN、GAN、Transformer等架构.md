---
Title: 弄懂CNN/RNN/GAN/Transformer等架构
tags:
  - CNN
  - RNN
  - GAN
  - Transformer
原文地址：: https://mp.weixin.qq.com/s/kON_TAwqg1gY0iEAk_1T-Q
---

# CNN
1. 模型整体结构说明：
模型采用结构：两个卷积模块（block_1 + block_2） + 一个全连接分类器（classifier）

--------------------------------------------------
2. block_1 模块（特征提取阶段）
- Conv2d: 提取局部空间特征（边缘/线条/纹理等）
- ReLU: 增加非线性表达能力（使模型可以拟合复杂模式）
- Conv2d: 深化特征提取，学习更复杂图像结构
- ReLU: 同上
- MaxPool2d: 下采样，减小特征图尺寸，保留主要特征（例如从 28x28 -> 14x14）

3. block_2 模块（进一步特征提取+压缩）
- 与 block_1 类似，但在更小的空间上学习更抽象的特征（例如轮廓、形状组合）
- 最终再次池化，通常变为 7x7 的特征图（具体取决于输入尺寸和池化设置）

--------------------------------------------------
4. classifier 模块（分类阶段）
- Flatten: 将多维特征图（如 [batch, channels, 7, 7]）展平为一维向量 [batch, channels*7*7]
- Linear: 输入 flattened 向量，输出为 num_classes 个分数（logits）

为什么要拉平成一维？→ 因为全连接层的输入必须是一维向量（每个特征都连接到输出的每个神经元）

--------------------------------------------------
5. 全连接层（Linear）为什么可以分类？

y = Wx + b

- x: 展平后的特征向量（例如 10×7×7 = 490维）
- W: 学习到的权重矩阵（形状 [10类, 490特征]）
- b: 偏置
- y: 输出 logits，每个维度对应一个类别的得分

预测类别：torch.argmax(y) 取最大值对应的类

本质上就是“对所有输入特征加权求和”，找到哪一类得分最高。

训练过程中，通过损失函数（CrossEntropyLoss） + 反向传播自动优化 W 和 b，使得预测越来越准确。

--------------------------------------------------
6. 总结：
- 卷积模块作用：提取空间局部特征，逐层学习图像结构
- ReLU 激活函数：引入非线性，便于学习复杂模式
- 池化层：降低空间维度、增强模型对位移鲁棒性
- Flatten + Linear：将空间特征映射为分类器可用的一维输入，最终做分类

## 卷积层
- 类似其他神经网络层，卷积层的目标是：**学习出有用的权重模式（特征检测器）**
- 在训练过程中，卷积核的参数会通过 **反向传播** 自动调整，以最小化损失函数
- **注意：不同参数组合将影响输出特征图的大小，公式如下：**
	`输出尺寸 = [(输入尺寸 - 卷积核尺寸 + 2 * padding) / stride] + 1`
### 1 为什么 kernel_size 可以是 `(5, 5)`？
- 什么是 kernel？在 CNN 中，“卷积核”或“过滤器”本质上是一个小的、可学习的权重矩阵，通常是 2D 或 3D 的张量。它会在输入图像（或前一层的特征图）上滑动，执行**加权求和**来提取局部特征。
（5，5）这是因为：
- 卷积核本质是一个二维窗口（对图像进行局部扫描），例如：
    - `(3, 3)` 表示 3 高 × 3 宽
    - `(5, 5)` 表示 5 高 × 5 宽
        
- 你可以用整数：`kernel_size=3`（表示 3x3）
- 也可以用元组：`kernel_size=(5, 5)`（明确表示高度和宽度都是 5）
- 表示：每个卷积核会覆盖输入图像的 5 行 5 列区域，并在图像上滑动，逐块进行加权卷积。
### 2 超参数
`in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` 都是**超参数**  
- 它们不是模型通过训练自动学习的，而是我们手动设定的
- 这些值并没有标准答案，需要通过：
    - 实验尝试
        
    - 借鉴已有的模型架构（如 TinyVGG）
        
    - 或通过自动调参工具（如 GridSearch、Optuna）


## 池化层 
### `nn.MaxPool2d()`
池化的核心作用是**降低特征图的尺寸**，从而：

1. **减少参数量与计算量**；
    
2. **增强模型的空间不变性（translation invariance）**，即模型对图像的轻微平移、旋转不敏感；
    
3. **防止过拟合**，提高泛化能力。

###  为什么常用 2x2 池化窗口？
创建最大池化层（窗口为 2x2）
max_pool_layer = nn.MaxPool2d(kernel_size=2)

- 2x2 池化可以将特征图尺寸缩小一半（例如 64x64 → 32x32）；
    
- 能有效压缩图像信息，同时保留局部空间特征；
    
- 与 stride=2 一起使用时，效率和效果都较好；
    
- 是经验中效果较平衡的设置。

当然也可以用 3x3、4x4，但会更激进地压缩图像，可能导致信息损失更多。

![[image-10.png|700x241]]

### `n.AvgPool2d()` 的作用是什么？

顾名思义，`nn.AvgPool2d()` 表示**平均池化层**，它的功能是：

> 在滑动窗口区域内，**取平均值**作为特征图的输出。

这与 `nn.MaxPool2d()` 不同，后者是取最大值。
### 对比总结
- 最大池化（Max Pooling）：提取窗口中的最大值，强调**强特征**，适用于分类、目标检测；
- 平均池化（Avg Pooling）：提取窗口中的平均值，强调**整体信息**，适用于图像风格迁移、图像平滑等任务；
- 两者都是降维工具，但保留信息的方式不同；
- 实际使用时可以根据任务需求和数据特点选择。

### ✅ PyTorch 使用图像数据的两个关键要求：

| 要求                       | 原因            |
| ------------------------ | ------------- |
| 转为 Tensor                | 便于模型处理        |
| 封装为 Dataset 和 DataLoader | 支持批处理、打乱、并行加载 |
