---
Title: 弄懂CNN/RNN/GAN/Transformer等架构
tags:
  - CNN
  - RNN
  - GAN
  - Transformer
原文地址：: https://mp.weixin.qq.com/s/kON_TAwqg1gY0iEAk_1T-Q
---

# CNN


1. 模型整体结构说明：
模型采用结构：两个卷积模块（block_1 + block_2） + 一个全连接分类器（classifier）

--------------------------------------------------
2. block_1 模块（特征提取阶段）
- Conv2d: 提取局部空间特征（边缘/线条/纹理等）
- ReLU: 增加非线性表达能力（使模型可以拟合复杂模式）
- Conv2d: 深化特征提取，学习更复杂图像结构
- ReLU: 同上
- MaxPool2d: 下采样，减小特征图尺寸，保留主要特征（例如从 28x28 -> 14x14）

3. block_2 模块（进一步特征提取+压缩）
- 与 block_1 类似，但在更小的空间上学习更抽象的特征（例如轮廓、形状组合）
- 最终再次池化，通常变为 7x7 的特征图（具体取决于输入尺寸和池化设置）

--------------------------------------------------
4. classifier 模块（分类阶段）
- Flatten: 将多维特征图（如 [batch, channels, 7, 7]）展平为一维向量 [batch, channels*7*7]
- Linear: 输入 flattened 向量，输出为 num_classes 个分数（logits）

为什么要拉平成一维？→ 因为全连接层的输入必须是一维向量（每个特征都连接到输出的每个神经元）

--------------------------------------------------
5. 全连接层（Linear）为什么可以分类？

y = Wx + b

- x: 展平后的特征向量（例如 10×7×7 = 490维）
- W: 学习到的权重矩阵（形状 [10类, 490特征]）
- b: 偏置
- y: 输出 logits，每个维度对应一个类别的得分

预测类别：torch.argmax(y) 取最大值对应的类

本质上就是“对所有输入特征加权求和”，找到哪一类得分最高。

训练过程中，通过损失函数（CrossEntropyLoss） + 反向传播自动优化 W 和 b，使得预测越来越准确。

--------------------------------------------------
6. 总结：
- 卷积模块作用：提取空间局部特征，逐层学习图像结构
- ReLU 激活函数：引入非线性，便于学习复杂模式
- 池化层：降低空间维度、增强模型对位移鲁棒性
- Flatten + Linear：将空间特征映射为分类器可用的一维输入，最终做分类
