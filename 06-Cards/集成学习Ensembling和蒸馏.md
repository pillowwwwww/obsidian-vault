---
Title: 集成学习、蒸馏
tags: 
原始链接:
---

## 一 、什么是集成学习？ 
集成学习，即分类器集成，通过构建并结合多个学习器来完成学习任务。一般结构是：先产生一组“个体学习器”，再用某种策略将它们结合起来。结合策略主要有平均法、投票法和学习法等。
集成学习是这样一个过程，按照某种算法生成多个模型，**如分类器或者称为专家**，再将这些模型按照某种方法组合在一起来解决某个智能计算问题。集成学习主要用来提高模型（分类，预测，函数估计等）的性能，或者用来降低模型选择不当的可能性。
### 🎯 核心思想

> “多个弱模型的组合可以胜过一个强模型。”
### 📌 常见的集成方法：

| 方法                   | 说明                                  |
| -------------------- | ----------------------------------- |
| **Voting**           | 适用于分类，多个模型投票决定最终类别（如少数服从多数）         |
| **Averaging**        | 对多个模型的预测概率进行平均                      |
| **Stacking**         | 用另一个模型（元学习器）来学习如何组合这些模型的输出          |
| **Boosting/Bagging** | 特殊的集成训练流程，如 Adaboost、Random Forest。 |
**Adaboost（Adaptive Boosting）** 是一种经典的**Boosting集成学习方法**，通过多个“弱分类器”组合成一个“强分类器”。
🎯 核心思想：
> 让后来的分类器重点关注之前被错分的样本。

什么是 Random Forest（随机森林）？
**Random Forest（随机森林）** 是一种 Bagging（装袋）集成方法，由许多决策树组成。
 ### 🎯 核心思想：

> 多棵树+多数投票，减少单棵树的过拟合。

## 二、什么是 Distillation（知识蒸馏）？

**知识蒸馏（Knowledge Distillation）** 是一种模型压缩技术，把一个复杂模型（或模型集合）的知识“压缩”进一个更小的模型里。
最初由 Hinton 等人在 2015 年提出，目的是训练一个**学生模型**模仿**教师模型**的行为。

### 🔍 关键步骤：
![[image-26.png]]
1. 教师模型对样本预测 logits（未归一化的 softmax 输出）
    
2. 学生模型对相同样本进行训练，目标是**模仿教师模型的输出**
    
3. 通常通过 **Kullback–Leibler (KL) 散度** 来最小化二者输出的差异
    
📌 换句话说，学生模型不是直接学“标签”，而是学“老师怎么看问题”。

### 📈 举例图（教师→学生）：

输入图像 → 教师模型 → soft logits
              ↓
         学生模型 → 输出预测
              ↓
         模拟 KL Loss，调整学生模型参数

### KD的损失函数/KD是如何进行模仿教师模型行为的呢？
通过KL散度。
什么是 KL 散度（Kullback–Leibler Divergence）？
KL散度是衡量**两个概率分布之间的差异**的指标，常用于机器学习中知识蒸馏和信息论中。
![[image-22.png]]
#### 📌 解读：

- 当 P=Q 时，KL散度为0，表示“学生模型完全模仿了教师模型”。
    
- KL散度并不是对称的： DKL(P∣∣Q)≠DKL(Q∣∣P)D_{KL}(P || Q) \neq D_{KL}(Q || P)DKL​(P∣∣Q)=DKL​(Q∣∣P)
    
- KL散度越大，表示两个分布差距越大。

### 知识蒸馏（KD）中使用**联合损失**
![[image-24.png|665x249]]
一、软标签（Soft Label）与硬标签（Hard Label）是什么？

| 类型                  | 来源               | 形式         | 举例（10类分类）                           |
| ------------------- | ---------------- | ---------- | ----------------------------------- |
| **硬标签（Hard Label）** | 数据集的真实标签         | one-hot 向量 | `[0, 0, 1, 0, ..., 0]` 表示第3类        |
| **软标签（Soft Label）** | 教师模型的 softmax 输出 | 实值分布       | `[0.6, 0.3, 0.05, 0.03, ..., 0.02]` |
 🎯 二、为什么要结合 Soft + Hard 标签？
📌只有软标签（KD）- 模仿老师，但容易偏离 ground-truth（真实标签）
📌 只有硬标签（CE）：- 忽视了类间相似性 → 泛化能力弱
    
 ✅ 二者结合：
> 让学生“既能学到知识结构（soft），也不丢任务目标（hard）”。
### 知识蒸馏（Knowledge Distillation）可以按照“知识迁移的层次”分为两个主要方向：

一、基于目标蒸馏（Soft-target / Logits-based Distillation）：上面提到的
二、基于特征蒸馏（Feature-based Distillation）：

 ✅ 核心思想

> 不仅模仿教师模型的输出，还要模仿**中间层的特征表示**。

也就是说，学生模型在每一层都尽量学习教师模型的“思考过程”。
![[image-25.png|650x497]]
## ✅ 总结对比

| 特征    | 目标蒸馏（Logits-based）       | 特征蒸馏（Feature-based）          |
| ----- | ------------------------ | ---------------------------- |
| 训练信号  | 输出 logits / soft targets | 中间层的特征向量或激活                  |
| 模型要求  | 可异构、黑盒                   | 通常需要结构相似或可对齐的特征维度            |
| 实施复杂度 | 较低                       | 较高（需要中间层访问）                  |
| 表现提升  | 中等偏稳                     | 更大但更依赖细节设计                   |
| 应用案例  | DistilBERT、TinyBERT      | FitNets, AT, RKD, CRD, NST 等 |
