---
Title: 常见深度学习术语解析笔记
tags: 
原始链接:
---

## 常见深度学习术语解析笔记

我们在阅读文章时经常看到 backbone、head、neck 等术语，但可能不清楚它们的确切含义，本文对这些术语进行简要解释：

### 1. Backbone（主干网络）
backbone 直译为“主干网络”，用于提取输入图像的深层特征，是整个模型的基础部分。
- 常见的 backbone 有 ResNet、VGG 等。
- 一般直接使用预训练模型作为 backbone，然后在其后连接自己的网络结构。
- 训练时进行微调，使其更适应当前任务。

### 2. Head
head 指的是用于**输出结果**的部分。
- 输入为前面 backbone 提取的特征。
- 输出为最终的分类结果、回归结果等预测内容。

### 3. Neck
neck 是连接 backbone 与 head 的中间结构。
- 它的作用是进一步加工、融合、变换 backbone 提取的特征，以便 head 更好地使用。
- 如 FPN（特征金字塔网络）就是一种典型的 neck。

### 4. Bottleneck（瓶颈结构）
bottleneck 指网络中**维度变小**的一段，类似瓶子的细脖子。
- 输入维度高，输出维度低（例如从 1024 降到 256）。
- 常见设置为 `bottle_num=256` 表示 bottleneck 输出是 256 维向量。

### 5. GAP（Global Average Pooling，全局平均池化）
GAP 层将每个通道的所有像素点**取平均值**，压缩空间维度。
- PyTorch 中用 `nn.AdaptiveAvgPool2d(1)` 实现。
- 说人话就是将某个通道的特征取平均值。
- 代码示例：
  ```python
  self.gap = nn.AdaptiveAvgPool2d(1)
```

### 6. Embedding（嵌入）

embedding 指将原始数据转化为向量形式（通常是稠密向量），用于表达抽象特征。

- 通常用于处理离散数据（如词语、ID）。
    
- 是深度学习中自动特征学习的重要手段。
    

### 7. Pretext Task 与 Downstream Task

- **Pretext Task**（前置任务、代理任务）：用于预训练模型的任务，如自监督对比任务。
    
- **Downstream Task**（下游任务）：用于模型微调和评估的真实任务，如分类、检测等。
    

### 8. Temperature Parameter（温度参数）

temperature parameters 在论文中经常能看到这个温度参数的身影，那么他都有什么用处呢？比如经常看到下面这样的式子：
![[866b95dc127796d0c8e09f802af195f9.png]]
示例代码：

```import torch
x = torch.tensor([1.0,2.0,3.0])
y = torch.softmax(x,0)
print(y)
 
x1 = x / 2  # beta 为2
y = torch.softmax(x1,0)
print(y)
 
x2 = x/0.5  # beta 为0.5
y = torch.softmax(x2,0)
print(y)
```

输出结果：


`tensor([0.0900, 0.2447, 0.6652])
`tensor([0.1863, 0.3072, 0.5065])`
`tensor([0.0159, 0.1173, 0.8668])`

- β 越大 → softmax 趋于平均，分布更平滑；
    
- β 越小 → softmax 趋于尖锐，更关注极大值；
    
- 类似学习率，可以根据训练阶段动态调整。
    

### 9. Warm-up（热身训练）

- 指的是在训练初期使用较小的学习率，等模型稳定后再恢复正常学习率。
    
- 防止初始参数不稳定导致训练发散。
    

### 10. End-to-End（端到端训练）

- 表示输入到输出整个过程由一个统一模型完成，**中间过程无需人工设计或手动处理**。
    
- 示例：图像分类任务，输入一张图，输出类别标签。
    
