---
Title: 
tags: 
原始链接:
---
 
## 1. self-attention

#### 目标任务 ：把”我爱你“翻译为英文
#### 步骤：
![[image-70.png|501x537]]

1. 输入我爱你，转为词向量（再把这几个向量**按顺序叠在一起**，形成**一张矩阵**（行数=token 个数，列数=D））。
    **X** = ([x_我, x_爱, x_你])
2. 然后用（X），**分别乘上** (W_Q)、(W_K)、(W_V)**三张矩阵**，就得到一整列 **Q/K/V**。
![[image-72.png|500x295]]
得到Q,K,V
其中三个矩阵WQ,WK,WV正是我们要训练的。
3. 用Q和每一个K点乘，计算相似度, softmax，再相加
![[image-73.png|912x497]]

### 是不是先“我”，再“爱”，再“你”？

**不是。** 自注意力（Encoder 里）是**并行**的：

- 同一层里会**同时**算出 `out_我`、`out_爱`、`out_你` 三个新向量。
    
- 你可以把“更新爱”理解成**只看整张并行运算结果的其中一行**；并不表示它是按顺序依次执行的。

### 3) 都在同一层里完成吗？

**是的。** 在**同一层的“注意力子层”** 里，三步（算分→softmax→对 V 加权求和）一次性并行完成，给出每个位置的 `out_t`。  
随后还会通过**残差 + 层归一化 + 前馈网络（FFN）**，得到这一层的最终输出，再交给**下一层**。

## 2. 残差