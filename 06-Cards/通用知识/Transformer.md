---
Title: 
tags: 
原始链接:
---
 
## 1. self-attention

#### 目标任务 ：把”我爱你“翻译为英文
前置步骤：
- **先**把 token 变成词向量（embedding）
- **再**把“位置信息”与它**结合**（加法或在注意力里注入）
	    也就是把每个词贴上座位号：
	    ![[image-74.png|468x464]]
- 把结果送进第一层 Transformer
#### 步骤：
![[image-70.png|501x537]]

1. 输入我爱你，转为词向量（再把这几个向量**按顺序叠在一起**，形成**一张矩阵**（行数=token 个数，列数=D））。
    **X** = ([x_我, x_爱, x_你])
2. 然后用（X），**分别乘上** (W_Q)、(W_K)、(W_V)**三张矩阵**，就得到一整列 **Q/K/V**。
![[image-72.png|500x295]]
得到Q,K,V
其中三个矩阵WQ,WK,WV正是我们要训练的。
3. 用Q和每一个K点乘，计算相似度, softmax，再相加
![[image-73.png|912x497]]

#### 是不是先“我”，再“爱”，再“你”？

**不是。** 自注意力（Encoder 里）是**并行**的：

- 同一层里会**同时**算出 `out_我`、`out_爱`、`out_你` 三个新向量。
    
- 你可以把“更新爱”理解成**只看整张并行运算结果的其中一行**；并不表示它是按顺序依次执行的。

#### 3) 都在同一层里完成吗？

**是的。** 在**同一层的“注意力子层”** 里，三步（算分→softmax→对 V 加权求和）一次性并行完成，给出每个位置的 `out_t`。  
随后还会通过**残差 + 层归一化 + 前馈网络（FFN）**，得到这一层的最终输出，再交给**下一层**。

## 2. 残差相加和LayNorm
自主注意力算完后，`out_我`、`out_爱`、`out_你` 三个新向量会进入下一步骤：残差相加。

#### 残差相加
**公式**：`y = x + F(x)`

- 把子层学到的变化 `F(x)` **加回**原输入 `x`。
**直观作用**
- **学“改动”更容易**：模型只需学“在原有基础上微调多少”，而不是重造一个全新表示。
- **给梯度修高速公路**：反传时梯度可直接沿着 `x → y` 的**快捷通道**流动，**减轻梯度消失/爆炸**，让**很深**的网络也能训起来。
- **信息不易丢**：原始信息 `x` 始终**保底**地传到下一层；注意力/FFN学到的只是“增量”。
**一句话**：Residual = “**保底通道 + 学增量**”。
#### 归一化
分为很多种归一化，BN,LN之类的。具体可以看onenote上面的笔记。

**总结** ：把这一层 Encoder 想成“前半段=注意力，后半段=FFN”，每一半都配一套 **残差（Add）+ 归一化（LayerNorm）**。

接下来是后半段FFN：

## 3. FFN：升维（Linear） → 激活(ReLU/GELU) → 降维（Linear）



**一层里就包含“Self-Attention → FFN”这两个子层**（各自配残差+LayerNorm）。然后把这一层的输出送到**下一层**，再次做“Self-Attention → FFN”。6 层就是把这个两步循环做 6 次