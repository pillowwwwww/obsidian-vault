---
Title: 
tags: 
原始链接:
---
 
## 1. self-attention

#### 目标任务 ：把”我爱你“翻译为英文
前置步骤：
- **先**把 token 变成词向量（embedding）
- **再**把“位置信息”与它**结合**（加法或在注意力里注入）
	    也就是把每个词贴上座位号：
	    ![[image-74.png|626x621]]
- 把结果送进第一层 Transformer
#### 步骤：
![[image-70.png|501x537]]

1. 输入我爱你，转为词向量（再把这几个向量**按顺序叠在一起**，形成**一张矩阵**（行数=token 个数，列数=D））。
    **X** = ([x_我, x_爱, x_你])
2. 然后用（X），**分别乘上** (W_Q)、(W_K)、(W_V)**三张矩阵**，就得到一整列 **Q/K/V**。
![[image-72.png|500x295]]
得到Q,K,V
其中三个矩阵WQ,WK,WV正是我们要训练的。
3. 用Q和每一个K点乘，计算相似度, softmax，再相加
![[image-73.png|912x497]]

#### 是不是先“我”，再“爱”，再“你”？

**不是。** 自注意力（Encoder 里）是**并行**的：

- 同一层里会**同时**算出 `out_我`、`out_爱`、`out_你` 三个新向量。
    
- 你可以把“更新爱”理解成**只看整张并行运算结果的其中一行**；并不表示它是按顺序依次执行的。

#### 3) 都在同一层里完成吗？

**是的。** 在**同一层的“注意力子层”** 里，三步（算分→softmax→对 V 加权求和）一次性并行完成，给出每个位置的 `out_t`。  
随后还会通过**残差 + 层归一化 + 前馈网络（FFN）**，得到这一层的最终输出，再交给**下一层**。

## 2. 残差相加和LayNorm
自主注意力算完后，`out_我`、`out_爱`、`out_你` 三个新向量会进入下一步骤：残差相加。

#### 残差相加
**公式**：`y = x + F(x)`

- 把子层学到的变化 `F(x)` **加回**原输入 `x`。
**直观作用**
- **学“改动”更容易**：模型只需学“在原有基础上微调多少”，而不是重造一个全新表示。
- **给梯度修高速公路**：反传时梯度可直接沿着 `x → y` 的**快捷通道**流动，**减轻梯度消失/爆炸**，让**很深**的网络也能训起来。
- **信息不易丢**：原始信息 `x` 始终**保底**地传到下一层；注意力/FFN学到的只是“增量”。
**一句话**：Residual = “**保底通道 + 学增量**”。
#### 归一化
分为很多种归一化，BN,LN之类的。具体可以看onenote上面的笔记。

**总结** ：把这一层 Encoder 想成“前半段=注意力，后半段=FFN”，每一半都配一套 **残差（Add）+ 归一化（LayerNorm）**。

接下来是后半段FFN：

## 3. FFN：升维（Linear） → 激活(ReLU/GELU) → 降维（Linear）

**一层里就包含“Self-Attention → FFN”这两个子层**（各自配残差+LayerNorm）。然后把这一层的输出送到**下一层**，再次做“Self-Attention → FFN”。6 层就是把这个两步循环做 6 次。


---
# Decoder
当一些生成式任务的时候，需要使用到decoder
![[image-77.png|426x372]]
注意上图：encoder中的输出H_enc输入到cross-attention中，作为KV的来源，而decoder的第一步的输出作为Q的来源。
## 1. Mask Multi-Head Attention & 残差归一
![[image-76.png|613x577]]

## 2. Cross-attention

**交叉注意力**= **用“我现在要写什么”的线索（Q，来自解码器当前隐藏态）去“源句的记忆库”（K/V，来自 Encoder 输出 $H_{\text{enc}}$ ) 里按相关性**取回一份内容摘要（上下文向量）**，帮助预测下一个词。
* (Q = Y_t W_Q)（来自解码侧），
* $(K=H_{\mathrm{enc}} W_K,; V=H_{\text{enc}} W_V)$ $(K=H_{\mathrm{enc}} W_K,; V=H_{\text{enc}} W_V)$（来自编码侧），
* 输出一份对中文里“**爱**”最相关的加权内容，帮你写出 *love*。
#### 为什么 Q 来自解码器，而 K/V 来自编码器？

因为这**吻合生成的本质**：

> 生成第 (t) 个词要基于“**我已经写到哪儿（前缀）**”和“**原文/图片里有什么（证据）**”。

* **Q（当前需求）**必须由**解码器此刻的状态**给出（它包含了前缀、语法、语境）。

* **K/V（可检索的证据）**自然由**Encoder 的输出**提供（对源输入的上下文化表示，稳定可缓存）

之后的流程：Cross-Attention 之后 →（残差+归一化）→ FFN（两层 MLP）→（残差+归一化）→ 进入下一层 Decoder；最后一层之后再接词表线性层得到 logits，训练算交叉熵，推理选下一个 token。


#### 自注意力和注意力有什么区别？
* **注意力（Attention）**：一种通用机制——用 **Q（Query）** 和 **K（Key）** 算相关性，得到权重，再对 **V（Value）** 加权求和：
$$
[

\text{Attn}(Q,K,V)=\mathrm{softmax}!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V

]
$$
——这是“**按相关性取料**”的通用公式。

* **自注意力（Self-Attention）**：注意力的一个**特例**，**Q=K=V 都来自同一批输入**（同一序列/同一张图的patch）：
$$
[

Q = XW_Q,; K = XW_K,; V = XW_V
]
$$
——让**同一序列内部**的各元素彼此“看”对方、交换信息（sequence mixing）。

关键区别一览:

| 维度       | 注意力（泛指）                                     | 自注意力（特例）                                        |
| -------- | ------------------------------------------- | ----------------------------------------------- |
| Q/K/V 来源 | 可来自**不同**张量（任意两端）                           | **同一**张量/同一序列                                   |
| 作用对象     | 广义“从A去读B”                                   | **在自己队伍内**互相取信息                                 |
| 典型用法     | **交叉注意力（Cross-Attn）**：Q 来自解码器，K/V 来自编码器     | **Encoder/Decoder 的 Self-Attn**：Q=K=V 都是本侧隐藏态   |
| 是否需要因果掩码 | 视任务而定；Cross-Attn 一般**不需要**（只做 padding mask） | Encoder 自注意力：**不遮**；Decoder 自注意力：**要遮右边**（因果掩码） |
| 解决的问题    | 条件检索、对齐外部记忆/他模态                             | 同序列上下文建模、长依赖捕获                                  |

所以说，**跨注意力（cross-attention）不属于自注意力**。它们都用同一套“注意力”机制，但**Q/K/V 的来源不同**

#### 为什么Transformer中都是**多头注意力**？
简短结论：**标准的 Transformer 层里，注意力模块几乎总是“多头注意力（Multi-Head Attention, MHA）”。**
- **Encoder**：用的是 **多头自注意力（MHSA）**。
- **Decoder**：先 **多头 masked 自注意力**，再 **多头交叉注意力（MHCA）**。
- “单头”只是 **MHA 的退化特例（head 数 = 1）**，主流实现默认都是多头。
