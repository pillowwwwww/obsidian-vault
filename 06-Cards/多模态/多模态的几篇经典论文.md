---
Title: 
tags: 
原始链接:
---
## CLIP 
论文笔记：[[CLIP  Radford2021]]

Image encoder, text encoder和embedding是什么关系？
encoder 的输出 = 特征；
最终的 embedding =（encoder 特征）再经过投影 + 归一化

## ABEF


## CoOp和CoCoOp
[(99+ 封私信) 【CLIP系列Paper解读】CoOp: Learning to Prompt for Vision-Language Models - 知乎](https://zhuanlan.zhihu.com/p/492546332)
[(99+ 封私信) 【CLIP系列Paper解读】CoCoOp: Conditional Prompt Learning for Vision-Language Models - 知乎](https://zhuanlan.zhihu.com/p/493354342)
COop有一些局限性：**现象**（CoOp 在 _unseen/new classes_ 上掉得厉害）和**机理**（为什么会掉——因为训练时学到的 _learned context_ 实际上“贴合/记住”了 _base classes_）

### CoOp 的局限（为什么会“好用但不够稳健”）

1. **静态提示（static prompts）**：同一数据集里**每张图都用同一套上下文**，上下文会对 base classes 记忆化（overfit），对**未见类别（unseen / novel classes）**的泛化变差——即一旦发生 **class shift**，性能会明显掉。
    
2. **类内多样性 / 实例属性（instance attributes）**：静态上下文不能针对“这张图的风格/属性/姿态”等做细调，**实例级异质性**无法被反映。
    
3. **跨域泛化（cross-dataset / domain generalization）**：当分布迁移时，静态 prompt 的鲁棒性不足。

#### 什么是上下文：
在 CLIP 里，我们通常用一个**手写模板**（hand-crafted template）去包住类别名，比如
- 英：`"a photo of a {class_name}."`
- 中：`“一张关于{类名}的照片。”`
**CoOp**把这个模板里的文字换成了**可学习的向量前缀**，也叫**上下文 token**：
$$
Prompt(c)  =  [ v1,  v2,  …,  vM,  className ]$$

- 这里 v1…vM都是**模型参数**，在少量标注（few-shot）的 base classes 上训练得到；
    
-
    
- 文本/图像编码器一般**冻结**（frozen），只优化这些 v；
    
- 同一数据集里，每张图、每次前向传播都用完全相同的一组 [v1,…,vM]（**image-agnostic / static**）。
    
> 小结：**“上下文”不是一句自然语言，而是一段固定长度、可学习的“前缀向量串”。它不随图片改变。**
（注：CoOp有**class-agnostic**版本——一套上下文配所有类；也有**class-specific**版本——每个类有自己的上下文。不管哪种，**都不随具体图片改变**。）

#### 再来解释一下静态提示：
coop中包含两种上下文：两种实现：**unified context（统一上下文）**——所有类别共用同一组向量；**class-specific context（类专属上下文）**——每个类别各自一组向量。无论哪种，**它们都不随单张图片而改变**（image-agnostic/static）。

#### 为什么说“每张图都用同一套上下文（static prompt）**为什么会把 CoOp 推向**对 base classes 的记忆化（overfit to base classes）”