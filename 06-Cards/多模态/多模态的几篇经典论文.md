---
Title: 
tags: 
原始链接:
---
## CLIP 
论文笔记：[[CLIP  Radford2021]]

Image encoder, text encoder和embedding是什么关系？
encoder 的输出 = 特征；
最终的 embedding =（encoder 特征）再经过投影 + 归一化

## ALBEF


# CLIP → CoOp → CoCoOp → MaPLe
## CoOp和CoCoOp
[(99+ 封私信) 【CLIP系列Paper解读】CoOp: Learning to Prompt for Vision-Language Models - 知乎](https://zhuanlan.zhihu.com/p/492546332)
[(99+ 封私信) 【CLIP系列Paper解读】CoCoOp: Conditional Prompt Learning for Vision-Language Models - 知乎](https://zhuanlan.zhihu.com/p/493354342)
COop有一些局限性：**现象**（CoOp 在 _unseen/new classes_ 上掉得厉害）和**机理**（为什么会掉——因为训练时学到的 _learned context_ 实际上“贴合/记住”了 _base classes_）

### CoOp 的局限（为什么会“好用但不够稳健”）

1. **静态提示（static prompts）**：同一数据集里**每张图都用同一套上下文**，上下文会对 base classes 记忆化（overfit），对**未见类别（unseen / novel classes）**的泛化变差——即一旦发生 **class shift**，性能会明显掉。
    
2. **类内多样性 / 实例属性（instance attributes）**：静态上下文不能针对“这张图的风格/属性/姿态”等做细调，**实例级异质性**无法被反映。
    
3. **跨域泛化（cross-dataset / domain generalization）**：当分布迁移时，静态 prompt 的鲁棒性不足。

#### 什么是上下文：
在 CLIP 里，我们通常用一个**手写模板**（hand-crafted template）去包住类别名，比如
- 英：`"a photo of a {class_name}."`
- 中：`“一张关于{类名}的照片。”`
**CoOp**把这个模板里的文字换成了**可学习的向量前缀**，也叫**上下文 token**：
$$
Prompt(c)  =  [ v1,  v2,  …,  vM,  className ]$$

- 这里 v1…vM都是**模型参数**，在少量标注（few-shot）的 base classes 上训练得到；
    
-
    
- 文本/图像编码器一般**冻结**（frozen），只优化这些 v；
    
- 同一数据集里，每张图、每次前向传播都用完全相同的一组 [v1,…,vM]（**image-agnostic / static**）。
    
> 小结：**“上下文”不是一句自然语言，而是一段固定长度、可学习的“前缀向量串”。它不随图片改变。**
（注：CoOp有**class-agnostic**版本——一套上下文配所有类；也有**class-specific**版本——每个类有自己的上下文。不管哪种，**都不随具体图片改变**。）

#### 再来解释一下静态提示：
coop中包含两种上下文：两种实现：**unified context（统一上下文）**——所有类别共用同一组向量；**class-specific context（类专属上下文）**——每个类别各自一组向量。无论哪种，**它们都不随单张图片而改变**（image-agnostic/static）。

#### 为什么说“每张图都用同一套上下文（static prompt）会把 CoOp 推向对 base classes 的记忆化（overfit to base classes）”
1) 直观小例子（cats vs. dogs，测试遇到 fox）
设训练只含两类：**cat** 与 **dog**（base classes），测试会出现**fox**（unseen class）。
* CoOp 的做法：学一段**可训练但静态**的上下文向量 (v)（learned context），所有图都用同一套 ([v_1,\dots,v_M])。
* 训练目标：让 $$(\text{Text}([v,\text{cat}]))$$ 更接近猫图特征，让 $$(\text{Text}([v,\text{dog}]))$$ 远离猫、接近狗图特征，以此在**cat vs. dog**上分得更清楚。

**会发生什么？**

优化过程会把整段 (v) 调到一个“最利于**猫-狗**区分”的方向。比如，假设训练集里：

* 猫图多是**室内、柔光、近景**；

* 狗图多是**户外、草地、远景**。

那么最后学到的 (v) 可能**无意识地放大这些“风格/背景”线索**，从而让
$$\text{Text}([v,\text{cat}])$$更贴近“室内柔光近景”的图像簇
$$\text{Text}([v,\text{dog}]) $$更贴近“户外草地远景”的图像簇。


当测试来了一张 **fox**（赤狐，常在草地/树林），**虽然它不是 dog**，但它的视觉风格更像狗那一边，于是

$$\cos\big(\text{Img}(\text{fox}),\text{Text}([v,\text{dog}])\big)

>

\cos\big(\text{Img}(\text{fox}),\text{Text}([v,\text{cat}])\big)

]
$$
就被错分成了 **dog**。

**关键点**：(v) 是“**一刀切**”的全局偏置（image-agnostic），它被**仅来自 base 类**的梯度“调味”过，一旦出现**新类**或**风格/域迁移**，这种“为猫狗而调的味”就可能**系统性地误导**决策——这就是“**记忆化 base classes**”。

2) 用一个“数字玩具”看扭曲（representation distortion）

把文本原型当作向量（CLIP/文本编码器输出）：

* 零样本时：
$$(\langle \text{Img}(\text{fox}),\text{Text}(\text{dog})\rangle=0.30)，

(\langle \text{Img}(\text{fox}),\text{Text}(\text{cat})\rangle=0.28)$$。差不多；若有“fox”类别可能还能分开。

* CoOp 学到的 (v) 把 **dog** 的文本原型整体**往“户外/草地”方向**推了 0.05，**cat** 往“室内/柔光”推了 0.03：

$$(\langle \text{Img}(\text{fox}),\text{Text}([v,\text{dog}])\rangle=0.37)（↑）$$
$$
(\langle \text{Img}(\text{fox}),\text{Text}([v,\text{cat}])\rangle=0.25)（↓）$$

对**猫狗**来说，这个扭曲是**好事**（分得更开）；对**未见类狐狸**而言，这个扭曲就是**偏见**（bias），把它“吸”向了 dog。

> 这就是 **prototype distortion（原型扭曲）**：learned context 会把各类文本原型朝“最利于 base 类区分”的方向整体平移/旋转；对新类，它可能是错的方向。

---
现在你知道了CoOp的局限性：learned context的泛化性不够好，很难泛化到同一个数据集内的unseen classes，这就意味着CoOp在训练时overfit到了base classes上。
那么这个团队进一步提出了CoCoOp：
![[image-84.png|672x952]]
最后说下个人看法，CoCoOp其实介于CoOp和CLIP之间的一个方法。CoOp的设计初衷用于将CLIP先验快速adapt到下游任务上，其提升了accuracy的同时也牺牲了generalization，所以CoOp“专”而“精”；而CLIP从头到尾都是在追求generalization，所以CLIP在所有数据集上的效果都还不错，但不够“精”，即“广”而“糙”。而CoCoOp则介于它们之间，那么问题拿来：我们什么时候才需要CoCoOp呢？或者说CoCoOp的应用场景真的广吗？举个例子，如果说我想完成某个下游任务我就直接用CoOp，如果我想**同时完成**很多下游任务那么我就选CLIP。这样看，CoCoOp的motivation并不是很强。

---
CoCoOp和Coop，后续工作：MaPle(在文本和图像端都进行了上下文学习)
## Clip的缺点
尽管 CLIP 在泛化到新概念上很有效，但它规模太大，而下游任务又常常是 few-shot（小样本） 场景，这使得对整个模型做全参数微调（full fine-tuning）在实践中并不现实。而且，如果你真的硬微调全模型，很容易：

- 把在大规模预训练阶段学到的有用知识“洗掉”（灾难性遗忘），

- 在下游小数据集上严重过拟合。 

为什么？
### 1. 为什么“下游任务常常是小样本（few-shot）”？

这里说的“下游任务”指的不是 **所有**现实中的任务，而是这一条论文线（CLIP → CoOp → CoCoOp → MaPLe）里**刻意设计的评测场景**，它们都有几个共同点：

1. **研究兴趣本身就是“少标注/少样本”的适配能力**
    
    - CLIP 这种叫 _foundation model_（基础模型），大目标是：**预训练一次 → 适配很多任务**。
        
    - 真正想考察的是：在**标注极少**的情况下，能不能很好迁移？
        
    - 所以这些论文会**故意**把每类只给 1、2、4、16 张图，叫做 **1-shot / 2-shot / 4-shot / 16-shot**。
        
2. 现实里也确实经常“没有很多标注数据”：
    
    - 做医疗影像、遥感、工业缺陷检查时，**标注一张图的成本是很高的**，不可能像 ImageNet 那样给你上百万张带标签的图。
        
    - 新品类、新场景刚出现的时候，**天然就是小数据**：比如你想让模型识别一款刚上市不久的商品，最多就几十张照片。
        
3. 工程角度：
    
    - 对一个已经预训练好的大模型，你不一定愿意再标好多数据去“重训一次”；
        
    - 更现实的期望是：**我只标一点点数据，模型就能快速适配我这个小任务**。
        

所以，论文里说 “few-shot setting” 其实是：

> 在**研究设定**和**很多真实场景**里，给你的标注样本就是非常少的 —— 这是它们故意要解决的问题。

### 2. 为什么 few-shot 下不适合 full fine-tuning（遗忘 + 严重过拟合）？

（“遗忘 + 严重过拟合”具体怎么发生？举两个例子）

先强调一句：

> “不能 full fine-tuning” 不是**数学上绝对不行**，而是**非常不划算、风险极大**，所以论文称它为 _infeasible_（不太现实、不太可行）。

###### 2.1 过拟合是怎么来的？（举个夸张一点的例子）

想象一下：

- CLIP 模型大概是 **上亿个参数**（10^8 级别）；
    
- Few-shot 任务里，比如每类 16 张图，10 个类，总共也就 **160 张训练图**。
    

就好像你要用一个有 **10 万个自由变量** 的函数去拟合 **10 个点**：

- 你当然可以把这 10 个点拟合得**完美无瑕**（训练集 acc 接近 100%）；
    
- 但在新点上预测几乎是**随缘**——因为模型有太多自由度，可以“记住”训练集的噪声、背景、拍摄角度等各种无关模式。
    

更直观的例子：

> 你让一个记忆力很夸张的学生，背 10 道题的答案，然后用这 10 道题来考试。  
> 他当然考满分。但你第二天给他换一套题，他可能一分都拿不到 —— 因为他不是理解了知识，只是死记硬背了那 10 道题的答案。

full fine-tuning 在 few-shot 时，就是在鼓励 CLIP 变成这个“死记硬背型学生”：

- 所有上亿参数都可以动；
    
- 但只有几百张图给你提供梯度；
    
- 模型会疯狂地把这些样本的各种细节“刻到”权重里，而不是真正在学一般规律。
###### 2.2 “遗忘”是怎么来的？（简单的 CLIP 场景举例）
假设：
- 原始的 CLIP 已经在 4 亿对图文上预训练，会识别 **上千种概念**（猫、狗、飞机、椅子、城市、山……）；
    
- 现在你拿一个很小的数据集，只包含 **“猫 vs 狗”** 两类图，想用 full fine-tuning 再训一训。
    
训练过程中：

1. 每次反向传播的梯度，只来自你这个小数据集里的“猫”和“狗”；
    
2. 模型为了把“猫 vs 狗”的边界做得更好，会不断调整视觉和文本 encoder 的参数；
    
3. 这些参数原来是为了“平衡地处理世界上很多概念”的，现在被你强行拧成“特别擅长分猫和狗”；
    
4. 由于没有“飞机、椅子、山”等图像参与训练，**模型不会收到任何提醒“别把这部分能力搞坏了”**。
    
结果：
- 在“猫 vs 狗”任务上，模型可能表现不错（甚至在训练集上非常好）；
- 但当你再拿它去做 zero-shot：
    - “一张飞机的照片” vs “a photo of an airplane”，   
    - 它可能就不再像原始 CLIP 那样稳了，因为相关的表示已经被你后期的梯度更新“扭曲”掉了。    
这就是 **catastrophic forgetting（灾难性遗忘）**：

> 你在一个新任务上继续训练模型时，由于只看到了新数据，模型会逐渐丢掉它在旧任务/旧分布上的能力。

对 CLIP 这种“通用大模型”来说，最宝贵的是**在大规模预训练里学到的通用知识**。  
full fine-tuning 在 few-shot 上，很容易用一点小数据把这堆宝贵知识“洗坏”。

## MaPLE
针对上述CLIP的缺点，提出了MaPLE。
#### 为什么不手动去调整prompt？
 手工 prompt 调整 = 在黑箱里瞎试词
在 CLIP 里做 zero-shot 分类时，我们一般这样写文本：

> 模板：`"a photo of a {class}"`  
> 类别：`{class} = "cat", "dog", "airplane", ...`

然后把这句 prompt 丢进 text encoder，得到类别的文本向量，再和图像向量做 cosine 相似度。
问题是：**稍微改几个字，效果可能完全不一样**，比如：
- `"a photo of a {class}"`    
- `"a picture of a {class}"`    
- `"a close-up photo of a {class}"`    
- `"this is a photo of the {class}"`    
- `"a satellite photo of a {class}"`（用在遥感数据集上）
这些句子在你看来差不多，但在 CLIP 的 text encoder 里：
- 对应的 embedding 差别可能很大；  
- 最终 top-1 accuracy 也会差很多（很多论文里都有这样的对比表）。   
所以如果不用 **prompt learning**，你就得：
> 一条一条人肉写模板 → 在验证集上试 → 换一句再试 → 不同数据集又要重新试一轮……
这就是所谓的 **prompt engineering 人肉调参**： 主观、靠感觉； 非常费时间； 完全不系统。

MaPLe 这条线把它视为“**应该用梯度代替人瞎试**”的问题。

