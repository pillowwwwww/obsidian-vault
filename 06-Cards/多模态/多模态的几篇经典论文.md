---
Title: 
tags: 
原始链接:
---
## CLIP 
论文笔记：[[CLIP  Radford2021]]

Image encoder, text encoder和embedding是什么关系？
encoder 的输出 = 特征；
最终的 embedding =（encoder 特征）再经过投影 + 归一化

## ABEF


## CoOp和CoCoOp
[(99+ 封私信) 【CLIP系列Paper解读】CoOp: Learning to Prompt for Vision-Language Models - 知乎](https://zhuanlan.zhihu.com/p/492546332)
[(99+ 封私信) 【CLIP系列Paper解读】CoCoOp: Conditional Prompt Learning for Vision-Language Models - 知乎](https://zhuanlan.zhihu.com/p/493354342)
COop有一些局限性：**现象**（CoOp 在 _unseen/new classes_ 上掉得厉害）和**机理**（为什么会掉——因为训练时学到的 _learned context_ 实际上“贴合/记住”了 _base classes_）

### CoOp 的局限（为什么会“好用但不够稳健”）

1. **静态提示（static prompts）**：同一数据集里**每张图都用同一套上下文**，上下文会对 base classes 记忆化（overfit），对**未见类别（unseen / novel classes）**的泛化变差——即一旦发生 **class shift**，性能会明显掉。
    
2. **类内多样性 / 实例属性（instance attributes）**：静态上下文不能针对“这张图的风格/属性/姿态”等做细调，**实例级异质性**无法被反映。
    
3. **跨域泛化（cross-dataset / domain generalization）**：当分布迁移时，静态 prompt 的鲁棒性不足。

#### 什么是上下文：
在 CLIP 里，我们通常用一个**手写模板**（hand-crafted template）去包住类别名，比如
- 英：`"a photo of a {class_name}."`
- 中：`“一张关于{类名}的照片。”`
**CoOp**把这个模板里的文字换成了**可学习的向量前缀**，也叫**上下文 token**：
$$
Prompt(c)  =  [ v1,  v2,  …,  vM,  className ]$$

- 这里 v1…vM都是**模型参数**，在少量标注（few-shot）的 base classes 上训练得到；
    
-
    
- 文本/图像编码器一般**冻结**（frozen），只优化这些 v；
    
- 同一数据集里，每张图、每次前向传播都用完全相同的一组 [v1,…,vM]（**image-agnostic / static**）。
    
> 小结：**“上下文”不是一句自然语言，而是一段固定长度、可学习的“前缀向量串”。它不随图片改变。**
（注：CoOp有**class-agnostic**版本——一套上下文配所有类；也有**class-specific**版本——每个类有自己的上下文。不管哪种，**都不随具体图片改变**。）

#### 再来解释一下静态提示：
coop中包含两种上下文：两种实现：**unified context（统一上下文）**——所有类别共用同一组向量；**class-specific context（类专属上下文）**——每个类别各自一组向量。无论哪种，**它们都不随单张图片而改变**（image-agnostic/static）。

#### 为什么说“每张图都用同一套上下文（static prompt）会把 CoOp 推向对 base classes 的记忆化（overfit to base classes）”
1) 直观小例子（cats vs. dogs，测试遇到 fox）
设训练只含两类：**cat** 与 **dog**（base classes），测试会出现**fox**（unseen class）。
* CoOp 的做法：学一段**可训练但静态**的上下文向量 (v)（learned context），所有图都用同一套 ([v_1,\dots,v_M])。
* 训练目标：让 $$(\text{Text}([v,\text{cat}]))$$ 更接近猫图特征，让 $$(\text{Text}([v,\text{dog}]))$$ 远离猫、接近狗图特征，以此在**cat vs. dog**上分得更清楚。

**会发生什么？**

优化过程会把整段 (v) 调到一个“最利于**猫-狗**区分”的方向。比如，假设训练集里：

* 猫图多是**室内、柔光、近景**；

* 狗图多是**户外、草地、远景**。

那么最后学到的 (v) 可能**无意识地放大这些“风格/背景”线索**，从而让
$$\text{Text}([v,\text{cat}])$$更贴近“室内柔光近景”的图像簇
$$\text{Text}([v,\text{dog}]) $$更贴近“户外草地远景”的图像簇。


当测试来了一张 **fox**（赤狐，常在草地/树林），**虽然它不是 dog**，但它的视觉风格更像狗那一边，于是

$$\cos\big(\text{Img}(\text{fox}),\text{Text}([v,\text{dog}])\big)

;>;

\cos\big(\text{Img}(\text{fox}),\text{Text}([v,\text{cat}])\big)

]
$$
就被错分成了 **dog**。

**关键点**：(v) 是“**一刀切**”的全局偏置（image-agnostic），它被**仅来自 base 类**的梯度“调味”过，一旦出现**新类**或**风格/域迁移**，这种“为猫狗而调的味”就可能**系统性地误导**决策——这就是“**记忆化 base classes**”。

2) 用一个“数字玩具”看扭曲（representation distortion）

把文本原型当作向量（CLIP/文本编码器输出）：

* 零样本时：
$$(\langle \text{Img}(\text{fox}),\text{Text}(\text{dog})\rangle=0.30)，

(\langle \text{Img}(\text{fox}),\text{Text}(\text{cat})\rangle=0.28)$$。差不多；若有“fox”类别可能还能分开。

* CoOp 学到的 (v) 把 **dog** 的文本原型整体**往“户外/草地”方向**推了 0.05，**cat** 往“室内/柔光”推了 0.03：

$$(\langle \text{Img}(\text{fox}),\text{Text}([v,\text{dog}])\rangle=0.37)（↑），

(\langle \text{Img}(\text{fox}),\text{Text}([v,\text{cat}])\rangle=0.25)（↓）$$

对**猫狗**来说，这个扭曲是**好事**（分得更开）；对**未见类狐狸**而言，这个扭曲就是**偏见**（bias），把它“吸”向了 dog。

> 这就是 **prototype distortion（原型扭曲）**：learned context 会把各类文本原型朝“最利于 base 类区分”的方向整体平移/旋转；对新类，它可能是错的方向。