---
Title: 
tags: 
原始链接:
---


![](https://i0.hdslb.com/bfs/note/c2519dea29fe88a33e4fc551f87303646837126f.png@690w_!web-note.webp)

ViLT论文里的这张图展示了多模态模型的发展历程，最开始的模型特点是Visual Encoder最大，Textual Encoder第二大，Modality Interaction是对文本特征和图像特征做一个点乘，计算量最小，所以VE>TE>MI。后来CLIP把Textual Encoder换成更大的attention结构，VilBERT、UNITER把Modality Interaction换成更大的attention结构，而ViLT保留较大的Modality Interaction，转而把TE、VE换成小的，就是TE只做个tokenization，VE做个patch embeding。不过ViLT在一些任务上的性能还是比不过ViLBERT，这也正常，毕竟计算量比它小那么多，而且直觉上来说图片信息比文本信息多，理应VE应该比TE大。而且ViLT虽然推理速度比ViLBERT快，但训练时间依然非常耗时，原因之一是它的WPA loss计算比较耗时。

CLIP在实际应用时，一般会事先把数据库里的图像或文本提前用VE或TE提好特征，这样在图文匹配时只需要做个简单得点乘即可，速度较快。CLIP对图文匹配任务还是表现得不错，但对其他任务，比如视觉问答VQA、视觉推理VR、视觉蕴含VE，因为点乘无法分析特别复杂的情况。

基于以上的分析，我们想要一个比较好的图文模型，模型的结构应该像上图的(c)，即VE比TE大，MI不能太小。训练方面，我们会采样CLIP的ITC（Image Text Contrastive）、BERT的MLM（Mask Language Modeling）和ViLBERT的ITM（Image Text Matching）这三个目标函数。

我们来看一下ALBEF的模型结构图：

![](https://i0.hdslb.com/bfs/note/546d9947a6abc64b39bd9214c85ad677daec35e6.png@690w_!web-note.webp)

图像编码器跟CLIP一样是12层的transformer，但文本编码那边，他把12层的transformer劈成了两部分，前6层用来做文本编码器，后6层用来做多模态融合的编码器。这个模型结构是符合我们前面的设想的，从图中也可以看到损失函数使用的是我们前面说的ITC、ITM、MLM。

ALBEF出自Salesfore Research的论文《ALign before Fuse: Vision and Language Representation Learning with Momentum Distillation》,这个团丢后面又做了BLIP、MUST、ALPro等多模态的工作。论文摘要里说了，本论文有两大核心贡献：第一，在抽取完图像特征和文本特征之后，在多模态融合之前，作者使用ITC对齐了一下这两个特征，也就是论文题目Align Before Fuse的来源。第二个贡献是为了克服数据集里noisy的图文对，提出了momentum distillation，即用一个额外的momentum model（借鉴自Moco论文提出的momentum encoder）去生成pseudo target（其实就是softmax score，而不再是one hot label），让模型学习这个pseudo target从而做自训练。最后的结论是这三个训练用的目标函数还有momentum distillation，它们地作用都是在为同一个图像文本对生成不同的视角，也就是变相地在做一种data augmentation，从而让训练出来的模型具备semantic preserving的功能。

再回来模型，图像编码器使用的参数来自DEiT。图中黄色方块代表的是class token，是一个1×768的向量，代表了整张图片的全局信息。ITM的本质是一个FC层做二分类，判断图片和文本是不是一个对，但是我们会通过某种方式选择最难的负样本，也就是最接近于正样本的负样本，举例来说，batch size = 512，那么正样本对是512个，我们把这张图片的class token跟所有文本的class token计算一遍余弦相似度，然后选择除了正样本文本外，相似度高的那个文本作为 hard negative。MLM的本质是一个完形填空，即把原来输入的文本中的某个单词遮住，让模型输出这个单词，跟NLP那边的MLM不同的是，这里还会借助图像信息来预测这个单词。值得一提的是，在计算ITC和ITM的时候，模型的输入都是图片和完整的句子，而在计算MLM的时候，模型的输入是图片和遮住一个单词的句子，所以为了计算loss，模型其实需要做两次前向传播。

接着讲一下momentum distillation，顾名思义，它里面还有动量EMA和蒸馏学习的思想。之所以用momentum distillation是因为数据集的图文对很多是弱相关的甚至不匹配，这会导致在计算损失函数的时候有偏差，比如对于ITC，可能所谓的负样本里也很好地描述了图像里的内容，甚至比GT 文本更好，对于MLM，完形填空的那个空 是可以填很多种单词的，有时候其他单词可能比GT单词更合适，如下图所示：

![](https://i0.hdslb.com/bfs/note/0e17e6a9949340fcbdf52e580627d5a501f5e3ff.png@690w_!web-note.webp)

所以网上爬下来的那些noisy data的one hot label对ITC和MLM是不友好的，因为负样本里可能也包含很多正确的信息，你让损失函数去惩罚这些负样本可能会让模型学习变得困难。既然这个问题是由noisy data的one hot label带来的，一种解决方法就是再提供额外的信号，这个信号不是one hot，而是multi hot，是另外一个模型的输出。这两年比较火的self training 就可以应用到这样的场景来。DINO就算是一种自训练模型。作者采取的方法是，先构建一个动量模型，用这个动量模型去生成pseudo target，这个pseudo target其实就是softamx score，而这个动量模型怎么来？其实就是在已有模型上做EMA。它的目的是，在训练时，让模型的预测不仅与GT one hot label接近，还要跟动量模型的输出匹配。拿ITC和MLM举个例子，下面公式里的q是动量模型的输出，p是主模型的输出，q是由s'做了softmax得到，s'分为文本对图片的相似度和图片对文本的相似度：

![](https://i0.hdslb.com/bfs/note/b18af22fc95d14054a6fdf33a6be9627780b1d80.png@690w_!web-note.webp)

因为有2项KL损失，所以它的权重系数是α/2：

![](https://i0.hdslb.com/bfs/note/9a8ec64ef28fcdda7230e3d597c820b9d5b2c6ef.png@690w_!web-note.webp)

MLM损失函数里的q和p就是两个模型输出的关于遮住单词的概率分布：

![](https://i0.hdslb.com/bfs/note/7318260a659e854f2b6450401dddc70d8f41e3c3.png@690w_!web-note.webp)
