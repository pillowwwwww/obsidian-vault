---
Title: 
tags: 
原始链接: 
date:
---


## LoRA 速记笔记（用 ΔW = A·B 表示）

**核心概念**：把 **W′拆分为 = W + ΔW，去训练ΔW**
- 在微调时**不改**原权重 (W)，只学习一个**低秩更新**：  
    **ΔW = A·B**（A、B是小矩阵，秩 r 很小）。
    
- 推理用更新后的权重：**W′ = W + ΔW**。
![[image-62.png|762x351]]
![[image-60.png|927x521]]

**为什么好用（要点版）**

- **参数少**：只训练 A、B，参数量远小于全参微调。
    
- **显存省**：优化器状态也只在 A、B 上，显存需求大幅下降。
    
- **可合并部署**：训练后可把 **ΔW** 合并回 (W)，**推理零额外开销**。
    
- **模块化/可个性化**：不同任务/客户端各自一套 A、B 即可（adapter）。
    
#### 参数调整过程
![[image-63.png]]

#### LoRA在哪里能用？
![[image-64.png]]

#### LoRA改进版本
![[image-67.png]]
![[image-69.png]]
![[image-65.png]]


#### 常用超参

![[image-66.png]]- 
- **最重要的rank**，缩放系数，目标模块 
    
> 训练时**只更新 A、B**，其余参数全部**冻结**。


> 一句话记忆：**LoRA = 用小矩阵 A、B 近似出 ΔW，替代改大权重 W；省参、省显存、可合并、好部署。**