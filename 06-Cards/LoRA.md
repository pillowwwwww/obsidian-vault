---
Title: 
tags: 
原始链接: https://chatgpt.com/share/690871f9-3f70-800b-ab3d-cbec8a4c448b
date:
---


## LoRA 速记笔记（用 ΔW = A·B 表示）

**核心概念**：把 **W′拆分为 = W + ΔW，去训练ΔW**
- 在微调时**不改**原权重 (W)，只学习一个**低秩更新**：  
- **先定义 A 和 B 的低秩结构，通过训练 A 和 B 让其乘积逼近 ΔW**，而非先有 ΔW 再分解出 A 和 B。
    **ΔW = A·B**（A、B是小矩阵，秩 r 很小）。
- U⋅Σ⋅VT 是对 ΔW（即 B⋅A 的乘积）做 SVD
- 推理用更新后的权重：**W′ = W + ΔW**。
![[image-78.png|587x439]]
![[image-79.png|715x166]]
A点乘B的维度和原始的 ΔW 维度一样

#### 线性代数的一些基础概念：
任何一个矩阵都可以被分解为三个矩阵的乘积，比如S可以分解为U，$\sum$、V这三个矩阵的乘积。
中间的$\sum$是一个对角矩阵，存在多个奇异值，把最大的几个奇异值保留，即可获得这个矩阵的大部分信息。
![[image-62.png|762x351]]
![[image-60.png|927x521]]


对于中间这个$\sum$ 矩阵，它的信息大部分集中在7.03，3，2.15这几个维度上，那么我们就称它为低秩矩阵。
研究发现，ΔW是一个低秩矩阵，它可以被分解。并且由于ΔW的秩很低，可以直接由A和B构造，不需要完整的SVD计算。
**为什么ΔW是一个低秩矩阵呢？**
![[image-80.png]]

**为什么好用（要点版）**
- **参数少**：只训练 A、B，参数量远小于全参微调。
- **显存省**：优化器状态也只在 A、B 上，显存需求大幅下降。
- **可合并部署**：训练后可把 **ΔW** 合并回 (W)，**推理零额外开销**。
- **模块化/可个性化**：不同任务/客户端各自一套 A、B 即可（adapter）。

### 步骤:
#### 1. 第一步：冻结预训练模型主干（省资源的关键）
先拿一个现成的预训练大模型（比如LLaMA、CLIP），**把它的所有原始权重（记为W₀）冻结住——不更新、不修改**。  
比如大模型里有个负责“特征映射”的权重矩阵W₀（假设维度是1024×1024，参数量超百万），现在完全固定它，避免全量微调时“训不动、耗内存”的问题。
#### 2. 第二步：加两个“小插件”——低秩矩阵A和B
在冻结的W₀旁边，加两个**参数量极小的低秩矩阵**（这是LoRA的核心设计）：  
- 矩阵A（输入侧）：维度是“低秩r × 输入维度dᵢ”，比如r=16、dᵢ=1024，参数量仅16×1024=16384；  
  作用：把输入的高维特征（比如1024维）“压缩”到低维空间（16维），相当于提取任务需要的关键特征；  
- 矩阵B（输出侧）：维度是“输出维度dₒ × 低秩r”，比如dₒ=1024、r=16，参数量仅1024×16=16384；  
  作用：把A压缩后的低维特征（16维）“还原”回高维输出维度（1024维），适配大模型的后续计算。  
这两个矩阵加起来才3万多参数，远少于W₀的百万级参数——这就是“参数高效”的来源。
#### 3. 第三步：只训A和B，让它们“学会适配任务”
用新任务的数据（比如“文本分类”“图像问答”数据）训练模型，但**只更新A和B的参数**，W₀始终不动：  
- 初始时：A随机初始化，B先设为全0（这样一开始A×B的结果是0，模型相当于完全用预训练权重跑）；  
- 训练中：根据任务损失（比如分类错误率），反向调整A和B的数值，让A×B的结果（记为ΔW，即“权重更新量”）越来越贴合新任务需求；  
  比如做“猫狗分类”，A会慢慢学会“提取毛发、面部特征”，B会学会“把这些特征映射到‘猫’‘狗’类别”，最终ΔW就成了“分类任务专属的适配权重”。
#### 4. 第四步：推理时“拼回”权重，用新模型干活
训练完A和B后，推理（即实际用模型预测）时，把A和B先乘起来得到ΔW，再加到冻结的原始权重W₀上，得到最终可用的权重W：  $W = W_0 + A \times B$
然后用这个W去做任务预测（比如输入一张猫的图片，模型就能用W正确输出“猫”）。  

甚至可以把A×B的结果直接融入W₀，推理时不用额外加载A和B——相当于给预训练模型“悄悄更了新”，但全程没动它的主干。

![[image-90.png|734x657]]
![[image-91.png]]

#### 参数调整过程

![[image-81.png]]
这个r=8是一个参数，可以调节。

![[image-63.png]]

#### LoRA在哪里能用？
![[image-64.png]]

#### LoRA改进版本
![[image-67.png]]
![[image-69.png]]
![[image-65.png]]


#### 常用超参

![[image-66.png]]- 
- **最重要的rank**，缩放系数，目标模块 
    
> 训练时**只更新 A、B**，其余参数全部**冻结**。


> 一句话记忆：**LoRA = 用小矩阵 A、B 近似出 ΔW，替代改大权重 W；省参、省显存、可合并、好部署。**

---

## Lora训练步骤
好，我们用一个**非常具体的小例子**把 LoRA 的训练从头到尾走一遍。你只要抓住两句话：

1. **标签还是原任务的标签**（比如答案文本），**不是 ΔW**。
2. **只更新 LoRA 的 A、B**，大模型原权重 W 一直**不动**。

#### 模型与 LoRA 安装

1. **加载基座模型**（Llama-7B 之类），权重记为 **W**。
2. **冻结 W**（`requires_grad=False`）。
3. 在注意力的投影矩阵（常见打 **Wq/Wv**，也可 Wk/Wo/FFN）**挂上 LoRA**：
    
    - 新增**两小块参数**：**A（down）** 和 **B（up）**，秩 **r=8**（举例）。
    - 有效权重变成  
    - $$
        [  
        W' = W + \underbrace{\tfrac{\alpha}{r}}_{\text{一个缩放旋钮}} , B A  
        ]
        $$
    - **只有 A、B 会被训练**；W 不更新。
        
![[image-83.png|1087x939]]

**`y_base = x · W^T` 是啥？**  
    这是某个线性层的**基座输出**（用**冻结的**权重 W 算出来的那部分）。  
    LoRA 再额外算一小段 **`y_lora`**（由 A、B 产生的“补丁输出”），**总输出**是`y = y_base + y_lora`
    也就是：**基座给出一个“原始答案”**，**LoRA 给出一个“小改动”**，两者相加就是模型的最终输出。
#### **为什么选注意力投影层**？因为这些矩阵**决定“看谁、怎么写”**：

- **Wq/Wk/Wv** 决定注意力该聚焦哪些信息（读什么）。
    
- **Wo** 决定读到的信息怎么整合到输出（怎么写）。  
    在这些地方做**很小的改动**，就能对模型行为产生**很大的可控影响**，而且**参数特别省**（性价比高）。
    

> 也有人在前馈层（FFN）打 LoRA，只是注意力投影层通常最“划算”。
> **不从头训练**。你用的是**预训练好的基座模型**（比如 Llama），只是把它**冻住**（不更新参数），再给其中的某些**线性层**（最常见就是注意力里的投影矩阵 Wq/Wk/Wv/Wo）**加一个很小的可训练“外挂”**（LoRA 的 A、B）


#### 2) 前向时基座参与计算，但参数不更新，是不是“白训练”？`y_base = x · W^T` 又是什么？

- **不是白训练**。  
    “训练”包含**前向+反向**两部分。你把基座**冻结**，只是说**它不接收梯度、参数不更新**；但**前向必须要它参与**，否则哪来的基线输出？
    
- **`y_base = x · W^T` 是啥？**  
    这是某个线性层的**基座输出**（用**冻结的**权重 W 算出来的那部分）。  
    LoRA 再额外算一小段 **`y_lora`**（由 A、B 产生的“补丁输出”），**总输出**是
    
    `y = y_base + y_lora`
    
    也就是：**基座给出一个“原始答案”**，**LoRA 给出一个“小改动”**，两者相加就是模型的最终输出。
    
- **为什么不白费？**
    
    - 你算出的 **y_base** 参与了最终预测和损失，决定了**反向传播的方向**。
        
    - 反向时，**梯度只流进 A、B**（W 冻结不收梯度），于是**A、B 会学会“如何修正 y_base”**，让 `y = y_base + y_lora` 更接近标签。
        
    - 换个比喻：**雕像（W）不动**，你只用**橡皮泥（A、B）**做微小补偿，**雕像仍然是主角**，橡皮泥只是把细节修到位。
        

---

#### 超简流程（带你过一遍心里就有数了）

1. **加载预训练基座** W（不从头）。
    
2. **冻结** W（不更新）。
    
3. 在要改的层（常见 Wq/Wv，或再加 Wo/FFN）**挂 LoRA(A,B)**。
    
4. **前向**：算出 `y_base`（用 W） + `y_lora`（用 A、B），得到预测。
    
5. **算损失**（还是用你任务的**真实标签**，比如 SFT 就是答案 token 的交叉熵）。
    
6. **反向**：只给 **A、B** 梯度，更新它们；**W 不动**。
    
7. 重复直到收敛。最后 **A、B 形成的补丁** 就把模型**按你的任务“掰正”**了。