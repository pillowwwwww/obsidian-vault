---
Title: 
tags: 
原始链接: https://chatgpt.com/share/690871f9-3f70-800b-ab3d-cbec8a4c448b
date:
---


## LoRA 速记笔记（用 ΔW = A·B 表示）

**核心概念**：把 **W′拆分为 = W + ΔW，去训练ΔW**
- 在微调时**不改**原权重 (W)，只学习一个**低秩更新**：  
    **ΔW = A·B**（A、B是小矩阵，秩 r 很小）。
    
- 推理用更新后的权重：**W′ = W + ΔW**。
![[image-78.png|587x439]]
![[image-79.png|715x166]]
A点乘B的维度和原始的 ΔW 维度一样

#### 线性代数的一些基础概念：
任何一个矩阵都可以被分解为三个矩阵的乘积，比如S可以分解为U，$\sum$、V这三个矩阵的乘积。
中间的$\sum$是一个对角矩阵，存在多个奇异值，把最大的几个奇异值保留，即可获得这个矩阵的大部分信息。
![[image-62.png|762x351]]
![[image-60.png|927x521]]


对于中间这个$\sum$ 矩阵，它的信息大部分集中在7.03，3，2.15这几个维度上，那么我们就称它为低秩矩阵。
研究发现，ΔW是一个低秩矩阵，它可以被分解。并且由于ΔW的秩很低，可以直接由A和B构造，不需要完整的SVD计算。
**为什么ΔW是一个低秩矩阵呢？**
![[image-80.png]]

**为什么好用（要点版）**

- **参数少**：只训练 A、B，参数量远小于全参微调。
    
- **显存省**：优化器状态也只在 A、B 上，显存需求大幅下降。
    
- **可合并部署**：训练后可把 **ΔW** 合并回 (W)，**推理零额外开销**。
    
- **模块化/可个性化**：不同任务/客户端各自一套 A、B 即可（adapter）。
    
#### 参数调整过程

![[image-81.png]]
这个r=8是一个参数，可以调节。

![[image-63.png]]

#### LoRA在哪里能用？
![[image-64.png]]

#### LoRA改进版本
![[image-67.png]]
![[image-69.png]]
![[image-65.png]]


#### 常用超参

![[image-66.png]]- 
- **最重要的rank**，缩放系数，目标模块 
    
> 训练时**只更新 A、B**，其余参数全部**冻结**。


> 一句话记忆：**LoRA = 用小矩阵 A、B 近似出 ΔW，替代改大权重 W；省参、省显存、可合并、好部署。**

---
## Lora训练步骤
好，我们用一个**非常具体的小例子**把 LoRA 的训练从头到尾走一遍。你只要抓住两句话：

1. **标签还是原任务的标签**（比如答案文本），**不是 ΔW**。
2. **只更新 LoRA 的 A、B**，大模型原权重 W 一直**不动**。

#### 模型与 LoRA 安装

1. **加载基座模型**（Llama-7B 之类），权重记为 **W**。
2. **冻结 W**（`requires_grad=False`）。
3. 在注意力的投影矩阵（常见打 **Wq/Wv**，也可 Wk/Wo/FFN）**挂上 LoRA**：
    
    - 新增**两小块参数**：**A（down）** 和 **B（up）**，秩 **r=8**（举例）。
    - 有效权重变成  
    - $$
        [  
        W' = W + \underbrace{\tfrac{\alpha}{r}}_{\text{一个缩放旋钮}} , B A  
        ]
        $$
    - **只有 A、B 会被训练**；W 不更新。
        
![[image-83.png|1087x939]]

**`y_base = x · W^T` 是啥？**  
    这是某个线性层的**基座输出**（用**冻结的**权重 W 算出来的那部分）。  
    LoRA 再额外算一小段 **`y_lora`**（由 A、B 产生的“补丁输出”），**总输出**是`y = y_base + y_lora`
    也就是：**基座给出一个“原始答案”**，**LoRA 给出一个“小改动”**，两者相加就是模型的最终输出。
#### **为什么选注意力投影层**？因为这些矩阵**决定“看谁、怎么写”**：

- **Wq/Wk/Wv** 决定注意力该聚焦哪些信息（读什么）。
    
- **Wo** 决定读到的信息怎么整合到输出（怎么写）。  
    在这些地方做**很小的改动**，就能对模型行为产生**很大的可控影响**，而且**参数特别省**（性价比高）。
    

> 也有人在前馈层（FFN）打 LoRA，只是注意力投影层通常最“划算”。
> **不从头训练**。你用的是**预训练好的基座模型**（比如 Llama），只是把它**冻住**（不更新参数），再给其中的某些**线性层**（最常见就是注意力里的投影矩阵 Wq/Wk/Wv/Wo）**加一个很小的可训练“外挂”**（LoRA 的 A、B）


#### 2) 前向时基座参与计算，但参数不更新，是不是“白训练”？`y_base = x · W^T` 又是什么？

- **不是白训练**。  
    “训练”包含**前向+反向**两部分。你把基座**冻结**，只是说**它不接收梯度、参数不更新**；但**前向必须要它参与**，否则哪来的基线输出？
    
- **`y_base = x · W^T` 是啥？**  
    这是某个线性层的**基座输出**（用**冻结的**权重 W 算出来的那部分）。  
    LoRA 再额外算一小段 **`y_lora`**（由 A、B 产生的“补丁输出”），**总输出**是
    
    `y = y_base + y_lora`
    
    也就是：**基座给出一个“原始答案”**，**LoRA 给出一个“小改动”**，两者相加就是模型的最终输出。
    
- **为什么不白费？**
    
    - 你算出的 **y_base** 参与了最终预测和损失，决定了**反向传播的方向**。
        
    - 反向时，**梯度只流进 A、B**（W 冻结不收梯度），于是**A、B 会学会“如何修正 y_base”**，让 `y = y_base + y_lora` 更接近标签。
        
    - 换个比喻：**雕像（W）不动**，你只用**橡皮泥（A、B）**做微小补偿，**雕像仍然是主角**，橡皮泥只是把细节修到位。
        

---

#### 超简流程（带你过一遍心里就有数了）

1. **加载预训练基座** W（不从头）。
    
2. **冻结** W（不更新）。
    
3. 在要改的层（常见 Wq/Wv，或再加 Wo/FFN）**挂 LoRA(A,B)**。
    
4. **前向**：算出 `y_base`（用 W） + `y_lora`（用 A、B），得到预测。
    
5. **算损失**（还是用你任务的**真实标签**，比如 SFT 就是答案 token 的交叉熵）。
    
6. **反向**：只给 **A、B** 梯度，更新它们；**W 不动**。
    
7. 重复直到收敛。最后 **A、B 形成的补丁** 就把模型**按你的任务“掰正”**了。