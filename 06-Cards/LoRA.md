---
Title: 
tags: 
原始链接: 
date:
---


## LoRA 速记笔记（用 ΔW = A·B 表示）

**核心概念**：把 **W′拆分为 = W + ΔW，去训练ΔW**
- 在微调时**不改**原权重 (W)，只学习一个**低秩更新**：  
    **ΔW = A·B**（A、B是小矩阵，秩 r 很小）。
    
- 推理用更新后的权重：**W′ = W + ΔW**。
![[image-78.png|587x439]]
![[image-79.png|715x166]]
A点乘B的维度和原始的 ΔW 维度一样

#### 线性代数的一些基础概念：
任何一个矩阵都可以被分解为三个矩阵的乘积，比如S可以分解为U，$\sum$、V这三个矩阵的乘积。
中间的$\sum$是一个对角矩阵，存在多个奇异值，把最大的几个奇异值保留，即可获得这个矩阵的大部分信息。
![[image-62.png|762x351]]
![[image-60.png|927x521]]


对于中间这个$\sum$ 矩阵，它的信息大部分集中在7.03，3，2.15这几个维度上，那么我们就称它为低秩矩阵。
研究发现，ΔW是一个低秩矩阵，它可以被分解。并且由于ΔW的秩很低，可以直接由A和B构造，不需要完整的SVD计算。
**为什么ΔW是一个低秩矩阵呢？**
![[image-80.png]]

**为什么好用（要点版）**

- **参数少**：只训练 A、B，参数量远小于全参微调。
    
- **显存省**：优化器状态也只在 A、B 上，显存需求大幅下降。
    
- **可合并部署**：训练后可把 **ΔW** 合并回 (W)，**推理零额外开销**。
    
- **模块化/可个性化**：不同任务/客户端各自一套 A、B 即可（adapter）。
    
#### 参数调整过程

![[image-81.png]]
这个r=8是一个参数，可以调节。

![[image-63.png]]

#### LoRA在哪里能用？
![[image-64.png]]

#### LoRA改进版本
![[image-67.png]]
![[image-69.png]]
![[image-65.png]]


#### 常用超参

![[image-66.png]]- 
- **最重要的rank**，缩放系数，目标模块 
    
> 训练时**只更新 A、B**，其余参数全部**冻结**。


> 一句话记忆：**LoRA = 用小矩阵 A、B 近似出 ΔW，替代改大权重 W；省参、省显存、可合并、好部署。**