---
Title: 机器学习与深度学习中的数学知识点
tags:
---


#  1. sigmoid函数

### 1.1. 函数定义

`sigmoid`函数一类函数的统称，常见的`sigmoid`函数有：![[Pasted image 20250516175841.png]]  
准确地讲，**sigmoid function不是某一个函数，而是指某一类形如"S"的函数，都可以成为sigmoid的函数**
```python
x = np.linspace(-10, 10, 100)
y = 1 / (1 + np.exp(-x))

plt.figure(figsize=(6, 4))
plt.plot(x, y)
plt.title("S-函数")
plt.grid(True)

plt.show()
```

![image.png](https://img2024.cnblogs.com/blog/83005/202408/83005-20240806180428701-1404522868.png)  
从图形可以看出，**S函数**的输出会控制在一个有限的范围内（上面的函数是0~1之间），  
真是这个特性使得它非常适合表示概率或者用于二分类问题的输出层。

注意，`sigmoid`函数的输出并不是一定要在区间**(0,1)**中，


# 2. softmax函数（分类函数）

### 2.1. 函数定义

接下来介绍`softmax`函数，`softmax`函数是一种在机器学习和深度学习中广泛使用的函数，特别是在处理多分类问题的场景中。  
而上面介绍的`sigmoid`函数更多应用在二分类场景。

![[Pasted image 20250516180028.png]]

### 2.2 softmax层

![[Pasted image 20250516180049.png]]

### 2.2. 应用场景

`softmax`函数可以应用在：

1. 多分类问题：它是处理多分类问题时的标准输出层激活函数。能够将模型的原始输出（通常是线性层的输出）转换为概率分布，便于后续使用交叉熵损失函数进行训练。
2. 神经网络的输出层：在构建用于分类任务的神经网络时，常被用作输出层的激活函数。特别是在卷积神经网络（`CNN`）、循环神经网络（`RNN`）及其变体中用于生成最终的类别预测。
3. 强化学习：在某些强化学习场景中，可用于将Q值（即动作的价值估计）转换为选择每个动作的概率，从而实现基于概率的动作选择策略。
4. 自然语言处理：用来计算注意力权重，这些权重决定了模型在处理输入时应该给予哪些部分更多的关注。

`softmax`函数是机器学习和深度学习中处理多分类问题、生成概率分布和进行概率决策的重要工具。
根据前面的介绍，`sigmoid`函数适合二分类问题，`softmax`函数适合多分类问题。  
那么，`sigmoid`函数会不会是`softmax`函数的一个简化版本呢？
我们可以认为`softmax`函数是将`sigmoid`函数扩展到多变量之后而得到的。
# 3. Logits

#### 机器学习中的Logits
Logits 是模型预测的中间结果。它们本身并非最终的预测值，而需要通过激活函数（如 Softmax）进行归一化，转化为概率分布。

模型的优化目标（如交叉熵损失）直接基于 Logits 或其归一化结果进行计算。

- 定义：  
    Logits 是模型最后一层输出的原始分数，也就是在经过 softmax 归一化之前，每个 token 在整个词汇表上的预测“打分”。它们并不直接表示概率，而是未经过处理的数值。
- 作用：
- 这些分数反映了模型对各个可能输出 token 的倾向性。数值越高，经过 softmax 后得到的概率也越大，表示模型越“倾向”于预测该 token 作为下一个输出。
- 通常在大语言模型中，最终的隐藏状态会通过一个线性层转换成与词汇表大小相同的 logits 向量。

在分类任务中，很多[[损失函数（Loss Function）]]（如 PyTorch 中的 `nn.BCEWithLogitsLoss` 和 `nn.CrossEntropyLoss`）都要求输入是 **logits** ，而不是已经经过激活函数（如 softmax 或 sigmoid）后的结果。
原因如下：
 1. 数值稳定性更好
- 把 softmax/sigmoid 和 loss 结合在一起计算，可以避免浮点数下溢或上溢的问题
- 比如：如果你先算 `sigmoid(x)`，再带入 BCELoss，可能会因为数值太小而变成 0
2.  使用方便
- 损失函数内部自动帮你做了激活操作（如 sigmoid 或 softmax）
- 所以你不需要手动加 `torch.sigmoid()` 或 `F.softmax()`
**为什么用线性层生成 logits？**

因为在分类任务中，我们希望把网络输出的“高维抽象特征”转成 **每个类别的一个分数（logit）**，这正是线性层（全连接层）最擅长做的事！
**用通俗的类比解释**：
想象你在参加比赛，有三个评委（猫、狗、鸟）。
你之前展示了各种才艺（前面 CNN/LSTM 处理的特征），  
现在你要**交给一个打分系统**，让它输出每个评委给你的得分。

这个“打分系统”就是——**线性层**（Linear layer）！

它接收你前面的“才艺总结向量”，通过一个简单的“权重×输入 + 偏置”，  
输出每个类别的一个分数（logit），比如：
`输入特征向量：[a1, a2, ..., aN] 线性层输出 logits： [score_cat, score_dog, score_bird]`
**为什么不是再用 ReLU、卷积、LSTM 之类？**
因为这时候你只是需要一个**最后的投票或评分**，不需要再去提取特征了。  
线性层不会引入非线性，它只是简单的**分数加权组合**，恰好适合做最后一步分类。
> **线性层就像是“最终投票器”或“打分器”，根据前面学到的特征，给每个类别一个分数（logit），然后交给 softmax 决定最终分类。**
#### 数学中的Logits
![[Pasted image 20250516204617.png|356x131]]
![[Pasted image 20250516204755.png|520x310]]


### 3.1 逻辑回归Logistic Regression
![[Pasted image 20250516182052.png]]
左侧是sigmoid函数，也叫logistic函数，我们把z和sigmoid函数结合，就生成了逻辑回归模型
#### 输出
这么理解逻辑回归的输出： 他的输出的概率是给定输入x时，y等于1的概率。
![[Pasted image 20250516182657.png]]
#### 在论文中的表述
![[Pasted image 20250516182822.png]]

![[Pasted image 20250516182827.png]]















## 面对一个对此毫无了解的人，如何能让他理解？

## 应该属于什么框架？
