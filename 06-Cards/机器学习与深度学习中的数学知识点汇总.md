---
Title: 机器学习与深度学习中的数学知识点
tags:
---


#  1. sigmoid函数

### 1.1. 函数定义

`sigmoid`函数一类函数的统称，常见的`sigmoid`函数有：![[Pasted image 20250516175841.png]]  
准确地讲，**sigmoid function不是某一个函数，而是指某一类形如"S"的函数，都可以成为sigmoid的函数**
```python
x = np.linspace(-10, 10, 100)
y = 1 / (1 + np.exp(-x))

plt.figure(figsize=(6, 4))
plt.plot(x, y)
plt.title("S-函数")
plt.grid(True)

plt.show()
```

![image.png](https://img2024.cnblogs.com/blog/83005/202408/83005-20240806180428701-1404522868.png)  
从图形可以看出，**S函数**的输出会控制在一个有限的范围内（上面的函数是0~1之间），  
真是这个特性使得它非常适合表示概率或者用于二分类问题的输出层。

注意，`sigmoid`函数的输出并不是一定要在区间**(0,1)**中，


# 2. softmax函数（分类函数）

### 2.1. 函数定义

接下来介绍`softmax`函数，`softmax`函数是一种在机器学习和深度学习中广泛使用的函数，特别是在处理多分类问题的场景中。  
而上面介绍的`sigmoid`函数更多应用在二分类场景。

![[Pasted image 20250516180028.png]]

### 2.2 softmax层

![[Pasted image 20250516180049.png]]

### 2.2. 应用场景

`softmax`函数可以应用在：

1. 多分类问题：它是处理多分类问题时的标准输出层激活函数。能够将模型的原始输出（通常是线性层的输出）转换为概率分布，便于后续使用交叉熵损失函数进行训练。
2. 神经网络的输出层：在构建用于分类任务的神经网络时，常被用作输出层的激活函数。特别是在卷积神经网络（`CNN`）、循环神经网络（`RNN`）及其变体中用于生成最终的类别预测。
3. 强化学习：在某些强化学习场景中，可用于将Q值（即动作的价值估计）转换为选择每个动作的概率，从而实现基于概率的动作选择策略。
4. 自然语言处理：用来计算注意力权重，这些权重决定了模型在处理输入时应该给予哪些部分更多的关注。

`softmax`函数是机器学习和深度学习中处理多分类问题、生成概率分布和进行概率决策的重要工具。
根据前面的介绍，`sigmoid`函数适合二分类问题，`softmax`函数适合多分类问题。  
那么，`sigmoid`函数会不会是`softmax`函数的一个简化版本呢？
我们可以认为`softmax`函数是将`sigmoid`函数扩展到多变量之后而得到的。
![[image-31.png|602x226]]
# 3. Logits

#### 机器学习中的Logits
Logits 是模型预测的中间结果。它们本身并非最终的预测值，而需要通过激活函数（如 Softmax）进行归一化，转化为概率分布。

模型的优化目标（如交叉熵损失）直接基于 Logits 或其归一化结果进行计算。

- 定义：  
    Logits 是模型最后一层输出的原始分数，也就是在经过 softmax 归一化之前，每个 token 在整个词汇表上的预测“打分”。它们并不直接表示概率，而是未经过处理的数值。
- 作用：
- 这些分数反映了模型对各个可能输出 token 的倾向性。数值越高，经过 softmax 后得到的概率也越大，表示模型越“倾向”于预测该 token 作为下一个输出。
- 通常在大语言模型中，最终的隐藏状态会通过一个线性层转换成与词汇表大小相同的 logits 向量。

在分类任务中，很多[[损失函数（Loss Function）]]（如 PyTorch 中的 `nn.BCEWithLogitsLoss` 和 `nn.CrossEntropyLoss`）都要求输入是 **logits** ，而不是已经经过激活函数（如 softmax 或 sigmoid）后的结果。
原因如下：
 1. 数值稳定性更好
- 把 softmax/sigmoid 和 loss 结合在一起计算，可以避免浮点数下溢或上溢的问题
- 比如：如果你先算 `sigmoid(x)`，再带入 BCELoss，可能会因为数值太小而变成 0
2.  使用方便
- 损失函数内部自动帮你做了激活操作（如 sigmoid 或 softmax）
- 所以你不需要手动加 `torch.sigmoid()` 或 `F.softmax()`
**为什么用线性层生成 logits？**

因为在分类任务中，我们希望把网络输出的“高维抽象特征”转成 **每个类别的一个分数（logit）**，这正是线性层（全连接层）最擅长做的事！
**用通俗的类比解释**：
想象你在参加比赛，有三个评委（猫、狗、鸟）。
你之前展示了各种才艺（前面 CNN/LSTM 处理的特征），  
现在你要**交给一个打分系统**，让它输出每个评委给你的得分。

这个“打分系统”就是——**线性层**（Linear layer）！

它接收你前面的“才艺总结向量”，通过一个简单的“权重×输入 + 偏置”，  
输出每个类别的一个分数（logit），比如：
`输入特征向量：[a1, a2, ..., aN] 线性层输出 logits： [score_cat, score_dog, score_bird]`
**为什么不是再用 ReLU、卷积、LSTM 之类？**
因为这时候你只是需要一个**最后的投票或评分**，不需要再去提取特征了。  
线性层不会引入非线性，它只是简单的**分数加权组合**，恰好适合做最后一步分类。
> **线性层就像是“最终投票器”或“打分器”，根据前面学到的特征，给每个类别一个分数（logit），然后交给 softmax 决定最终分类。**
#### 数学中的Logits
![[Pasted image 20250516204617.png|356x131]]
![[Pasted image 20250516204755.png|520x310]]


### 3.1 逻辑回归Logistic Regression
![[Pasted image 20250516182052.png]]
左侧是sigmoid函数，也叫logistic函数，我们把z和sigmoid函数结合，就生成了逻辑回归模型
#### 输出
这么理解逻辑回归的输出： 他的输出的概率是给定输入x时，y等于1的概率。
![[Pasted image 20250516182657.png]]
#### 在论文中的表述
![[Pasted image 20250516182822.png]]

![[Pasted image 20250516182827.png]]


# 4. 条件分布
### 1. **数据分布 = 样本对 (x,y) 的分布**

- 在机器学习中，**一个样本由输入特征 x和 **输出标签 y** 构成；
    
- 所以我们关心的是二元对 (x,y) 出现的概率，即联合分布 P(x,y)；
    
- 这个 P(x,y)就是“**数据分布**”。

![[image-18.png]]

![[image-19.png]]

# 5. 温度参数
[[常见深度学习术语解析笔记# 8. Temperature Parameter（温度参数）]]
![[image-23.png]]
这张图展示了 softmax 输出随着温度 TTT 增大的变化趋势：
- **T=0.5**：分布最尖锐，几乎 one-hot，只有 Class A 被“相信”；
- **T=1.0**：正常 softmax，Class A 占主导；
- **T=2.0**：分布变得更平滑，Class B 也有一定概率；
- **T=5.0**：非常平滑，模型对所有类都较“宽容”，反映类间相似性。

**结论**：较大的 T 能让学生模型看到更多“老师对其他类别的看法”，从而学到更丰富的知识结构，而不是只记住答案。还可以减少过拟合。


# 6. L1,L2范数，余弦相似度
## ✅ 范数（Norm）是什么？

范数是一种**衡量向量“有多大”或“有多远”**的方式，常用于：

- 计算距离
    
- 向量归一化
    
- 正则化损失（如 L1/L2 正则）
    
## 🟧 L1 范数（曼哈顿距离 / Manhattan norm）

### 定义：


$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|
$$

即：**所有维度取绝对值后加起来**
### 直观理解：

- 表示从原点走到点 xxx 的“走街串巷”距离（只能水平或垂直走）
- 像你在城市街道上走路
### 示例：
$$
\mathbf{x} = [3, -4] \Rightarrow \|\mathbf{x}\|_1 = |3| + |-4| = 7
$$

## 🟩 L2 范数（Euclidean norm）把向量看成平面（或空间）中的一个箭头

### 数学定义：

$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$

### 示例：

$$
\mathbf{x} = [3, 4] \Rightarrow \|\mathbf{x}\|_2 = \sqrt{3^2 + 4^2} = \sqrt{25} = 5
$$
### 🔹 举个简单的二维例子：
设一个向量：`x = [3, 4]`

你可以在二维坐标系中这样表示它：

- 起点：原点 (0,0)
    
- 终点：点 (3,4)
    
- 从 (0,0) 到 (3,4) 画一条箭头，这条箭头就是这个向量。

### 🔸 三维空间中：
如果你有一个向量：`x = [1, 2, 3]`
你可以在三维坐标系中，从 (0,0,0) 指向点 (1,2,3)，画一条空间中的箭头，这条线段（有方向）就是这个三维向量。

- L2 范数 = 箭头的长度

- 余弦相似度 = 箭头间夹角的余弦，越接近 1 越相似。

### 🧩余弦相似度（Cosine Similarity）

#### ✅ 定义

余弦相似度是衡量两个向量夹角的相似程度，而不是大小。常用于衡量**两个梯度方向是否一致**。

公式如下：

$$\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_i A_i B_i}{\sqrt{\sum_i A_i^2} \sqrt{\sum_i B_i^2}}$$
- $$ A⋅B$$      表示两个向量的**点积**（也就是逐维相乘后求和）；
- ∣∣A∣∣：是向量 a 的**长度**（L2范数）；
余弦相似度是一个 **衡量两个向量方向是否一致** 的指标：

- 如果两个向量方向完全相同，余弦相似度 = 1；
    
- 如果两个向量方向完全相反，余弦相似度 = -1；
    
- 如果两个向量垂直（无关），余弦相似度 = 0。
#### 步骤
![[image-29.png|551x474]]


# 7. 最大似然
## ✅ 一、什么是最大似然估计（Maximum Likelihood Estimation, MLE）？

在机器学习中，我们常常需要从数据中**估计模型参数**。最大似然估计（MLE）就是一种**最常用的参数估计方法**。

### 👉 核心思想：

> 给定一些训练样本，我们希望找到一组参数，使得模型在这些样本下出现的“可能性最大”。

![[image-35.png|587x410]]
![[image-36.png|590x359]]