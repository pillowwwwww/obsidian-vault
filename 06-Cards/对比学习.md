---
Title: 对比学习
tags:
  - ContrastiveLearning
  - 对比学习
详细介绍链接: https://encord.com/blog/guide-to-contrastive-learning/
---

对比学习相关经典论文：
CLIP: 
## 对比学习

这种方法无需依赖人工标注的数据集，而是通过捕捉数据之间的相似性与差异性，从中提取有效的特征表征，从而实现学习任务的目标。


对比学习的**核心思想**在于，通过构建正样本对（表示相似关系）和负样本对（表示差异关系），促使模型在嵌入空间中将相似的数据靠近，而将差异较大的数据远离，以此来构建一个最优的Embedding Space。

在对比学习中，**关键操作是构建正样本对和负样本对，并利用这些对来指导模型的优化： 
- **正样本对**：由两个具有相似特征的样本组成。
    
    例如，在图像处理中，同一物体在不同角度、不同光照条件下拍摄的图像通常构成正样本对。这些样本之间的共同特性为模型提供了学习数据一致性的依据。
    
- **负样本对** ：由两个具有明显不同特征的样本组成。
    
    例如，不同物体的图像或完全不同类别的数据形成负样本对。通过学习这些样本的差异，模型能够在嵌入空间中区分不同的类别或特性。
    
在训练过程中，对比学习的目标是通过优化损失函数，将正样本对的嵌入距离尽可能缩小，同时扩大负样本对的嵌入距离。

这种方式不仅帮助模型捕捉数据的内在联系，还增强了其对特征差异的敏感性，从而**提升了数据表示的质量。**

## 步骤：
### 1. 数据增强
常见的数据增强技术包括裁剪、翻转、旋转、随机裁剪和颜色变换。通过生成多样化的实例，对比学习可确保模型能够学习捕捉相关信息，而不受输入数据变化的影响。
### 2.训练Encoder
对比学习的下一步是训练编码器网络。编码器网络将增强的实例作为输入，并将其映射到潜在表示空间，从而捕获有意义的特征和相似性。
编码器网络通常是一种深度神经网络架构，例如用于图像数据的[卷积神经网络](https://encord.com/glossary/cnn-definition/)(CNN) 或用于序列数据的[循环神经网络](https://en.wikipedia.org/wiki/Recurrent_neural_network)(RNN)。该网络学习从增强实例中提取并编码高级表征，从而有助于在后续步骤中区分相似和不相似的实例。
### 3. Projection网络
投影头的作用是将编码器提取的表示映射到一个适合进行对比损失计算的空间，从而提升对比学习效果。
这个额外的投影步骤有助于增强学习到的表征的判别力。通过将表征映射到低维空间，投影网络降低了数据的复杂性和冗余度，从而更好地区分相似和不相似的实例。
### 4. 对比学习
一旦增强实例被编码并投影到嵌入空间，就会应用对比学习目标。目标是最大化正对（来自同一样本的实例）之间的一致性，并最小化负对（来自不同样本的实例）之间的一致性。

这会促使模型拉近相似的实例，同时拉开不相似的实例。实例之间的相似性通常使用距离度量来衡量，例如[欧氏距离](https://en.wikipedia.org/wiki/Euclidean_distance)或[余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)。模型经过训练，在嵌入空间中最小化正样本对之间的距离，并最大化负样本对之间的距离。
### 5. 设定损失函数

对比学习利用各种损失函数来设定学习过程的目标。这些损失函数对于引导模型捕捉重要的表征并区分相似和不相似的实例至关重要。

选择合适的损失函数取决于具体的任务要求和数据特征。每个损失函数都旨在促进学习能够有效捕捉数据中有意义的相似性和差异性的表征。在后面的部分中，我们将详细介绍这些损失函数。
### 6.训练与优化

一旦定义了损失函数，就可以使用大量未标记的数据集来训练模型。训练过程包括迭代更新模型参数，以最小化损失函数。

诸如[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)(SGD) 或其变体之类的优化算法通常用于微调模型的超[参数](https://encord.com/glossary/hyper-parameters-definition/)。训练过程通常涉及批量更新，其中同时处理增强实例的子集。

在训练过程中，模型学习捕捉数据中的相关特征和相似性。迭代优化过程会逐步完善学习到的表征，从而更好地区分和分离相似和不相似的实例。
### 7.评估与概括
- #### 下游任务评估
对比学习成功的最终衡量标准是其在**下游任务中的表现**。学习到的表征是图像分类、物体检测、情感分析或语言翻译等任务的输入特征。
![[600e3b8c-a3e9-4d4d-a109-92d8f26cfc59_image2-1.avif]]
图像分类 vs 对象检测 vs 图像分割

模型在这些任务上的表现将使用适当的指标进行评估，包括**准确率、精确率、召回率、[F1 分数](https://encord.com/blog/f1-score-in-machine-learning/)或特定于任务的评估标准**。下游任务上的表现越高，表明所学习到的表征具有更好的泛化能力和实用性。
- #### 迁移学习
对比学习支持[迁移学习](https://encord.com/glossary/transfer-learning-definition/)，即将一个任务中学习到的[表征](https://blog.flipsnack.com/how-to-make-a-presentation/)应用于相关任务。泛化能力的评估是通过评估表征迁移到新任务或数据集的效果来体现的。如果学习到的表征能够很好地泛化到未知数据，并提升新任务的性能，则表明对比学习在捕捉有意义的特征和相似性方面是有效的
- #### 与基线的比较
为了理解对比学习的有效性，将学习到的表征与基线模型或其他最先进的方法进行比较至关重要。比较的维度包括**性能指标、鲁棒性、迁移学习能力或计算效率**。这样的比较有助于深入了解对比学习的附加价值及其相对于其他方法的潜在优势。


## 对比损失函数
（分为对比损失、Triplet Loss、N-pair Loss、InfoNCE、逻辑损失）等方法。详细介绍看链接

## 典型应用场景
对比学习是一种基于相似性和差异性训练模型的方法，近年来在多个领域展现出卓越效果，特别适用于**数据标注困难、样本复杂度高**的任务环境。

- 对比学习具有多种应用，例如半监督学习、监督学习、NLP 和数据增强，它可以提高模型性能和泛化能力。

## 常用对比学习框架
 SimCLR、MoCo、BLOY、SwAV、[Barlow Twins](https://encord.com/blog/barlow-twins-self-supervised-learning/)

### SimCLR:
[https://mp.weixin.qq.com/s/BFBeboupCpmWc-dkk3Ewdg]
该文讲SimCLR对比学习讲的非常好！请仔细阅读
![[image.png]]

#### 步骤简述：
```
原始图像 x
    ↓
两次随机增强 → x₁, x₂
    ↓
Encoder f → 表征 h₁, h₂
    ↓
Projection head g → 对比空间 z₁, z₂
    ↓
计算余弦相似度 sim(z₁, z₂)
    ↓
构造 NT-Xent 对比损失（正样本 vs 所有其他负样本）
    ↓
反向传播，更新 f 和 g
    ↓
在训练结束后：

- 投影头 g 被丢弃，因为它是为对比学习服务的辅助模块；
    
- 编码器 f 被保留，作为图像的表示提取器；
    
- 可将 f(x) 应用于下游任务（如图像分类、物体检测等）。
```
#### 2. 为什么有了编码器还要有投影头（Projection Head）？

结构如下：

`图像 → 数据增强 → 编码器 f(·) → 特征表示 h → 投影头 g(·) → 投影空间向量 z`

- `f(x)` 是主干网络（如 ResNet），提取特征表示 **h**
    
- `g(h)` 是 projection head，通常是一个2层 MLP，输出最终表示 **z**，供对比损失使用
    
❓那为什么还要加 `g(h)`？

因为：

1. **对比损失的优化方向**和**最终语义任务所需特征方向**不是完全一致。
    
2. 加 projection head 让编码器 f(·) 更自由地优化有用特征，而把对比损失的目标“推给” g(·)。
    

这在论文中也实验说明：

> Removing the projection head and performing contrastive loss directly on h, results in a significant performance drop.