---
Title: 集成学习
tags: 
原始链接:
---

## 一 、什么是集成学习？ 
集成学习，即分类器集成，通过构建并结合多个学习器来完成学习任务。一般结构是：先产生一组“个体学习器”，再用某种策略将它们结合起来。结合策略主要有平均法、投票法和学习法等。
集成学习是这样一个过程，按照某种算法生成多个模型，**如分类器或者称为专家**，再将这些模型按照某种方法组合在一起来解决某个智能计算问题。集成学习主要用来提高模型（分类，预测，函数估计等）的性能，或者用来降低模型选择不当的可能性。
### 🎯 核心思想

> “多个弱模型的组合可以胜过一个强模型。”
### 📌 常见的集成方法：

| 方法                   | 说明                                  |
| -------------------- | ----------------------------------- |
| **Voting**           | 适用于分类，多个模型投票决定最终类别（如少数服从多数）         |
| **Averaging**        | 对多个模型的预测概率进行平均                      |
| **Stacking**         | 用另一个模型（元学习器）来学习如何组合这些模型的输出          |
| **Boosting/Bagging** | 特殊的集成训练流程，如 Adaboost、Random Forest。 |
**Adaboost（Adaptive Boosting）** 是一种经典的**Boosting集成学习方法**，通过多个“弱分类器”组合成一个“强分类器”。
🎯 核心思想：
> 让后来的分类器重点关注之前被错分的样本。

什么是 Random Forest（随机森林）？
**Random Forest（随机森林）** 是一种 Bagging（装袋）集成方法，由许多决策树组成。
 ### 🎯 核心思想：

> 多棵树+多数投票，减少单棵树的过拟合。

## 二、什么是 Distillation（知识蒸馏）？

**知识蒸馏（Knowledge Distillation）** 是一种模型压缩技术，把一个复杂模型（或模型集合）的知识“压缩”进一个更小的模型里。

最初由 Hinton 等人在 2015 年提出，目的是训练一个**学生模型**模仿**教师模型**的行为。

### 🔍 关键步骤：

1. 教师模型对样本预测 logits（未归一化的 softmax 输出）
    
2. 学生模型对相同样本进行训练，目标是**模仿教师模型的输出**
    
3. 通常通过 **Kullback–Leibler (KL) 散度** 来最小化二者输出的差异
    

📌 换句话说，学生模型不是直接学“标签”，而是学“老师怎么看问题”。

### 📈 举例图（教师→学生）：

输入图像 → 教师模型 → soft logits
              ↓
         学生模型 → 输出预测
              ↓
         模拟 KL Loss，调整学生模型参数

### KD的损失函数/KD是如何进行模仿教师模型行为的呢？
通过KL散度。
什么是 KL 散度（Kullback–Leibler Divergence）？
KL散度是衡量**两个概率分布之间的差异**的指标，常用于机器学习中知识蒸馏和信息论中。
![[image-22.png]]
#### 📌 解读：

- 当 P=QP = QP=Q 时，KL散度为0，表示“学生模型完全模仿了教师模型”。
    
- KL散度并不是对称的： DKL(P∣∣Q)≠DKL(Q∣∣P)D_{KL}(P || Q) \neq D_{KL}(Q || P)DKL​(P∣∣Q)=DKL​(Q∣∣P)
    
- KL散度越大，表示两个分布差距越大。